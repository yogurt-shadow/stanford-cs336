{
  "files": {
    "lecture_13.py": "from execute_util import text, image, link\nfrom lecture_util import article_link, named_link\nfrom references import dclm_2024, nemotron_cc_2024, olmo2, llama3, gpt2, openwebtext, gopher, alpaca\n\n\ndef main():\n    text(\"Previous lectures: how to train a model *given data*\")\n    text(\"Next two lectures: *what data* should we train on?\")\n\n    introduction()\n\n    text(\"### Pretraining\")\n    text(\"Let's peer into the data of some popular models.\")\n    bert()                # Wikipedia, books (trained BERT) [2019]\n    gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]\n    common_crawl()        # Web crawl\n    ccnet()               # Filter Common Crawl based on Wikipedia [2019]\n    t5_c4()               # Filter using rules (trained T5) [2019]\n\n    gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]\n    the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]\n    gopher_massivetext()  # Filter using rules (trained Gopher) [2021]\n    llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]\n    refinedweb()          # CommonCrawl (used to train Falcon) [2023]\n    dolma()               # Lots of different sources [2024]\n    dclm()                # Filtered using good quality classifier [2024]\n    nemotron_cc()         # Lots of tokens [2024]\n\n    copyright()\n\n    text(\"### Mid-training + post-training\")\n    text(\"Let's focus on particular capabilities.\")\n    long_context()        # Long context\n    tasks()               # Tasks based on standard datasets\n    instruction_chat()    # Instruction following and chat\n\n    text(\"### Summary\")\n    text(\"- Key lesson: Data does not fall from the sky. You have to work to get it.\")\n    text(\"- Live service => raw data => processed data (conversion, filtering, deduplication)\")\n    text(\"- Data is the key ingredient that differentiates language models\")\n    text(\"- Legal and ethical issues (e.g., copyright and privacy)\")\n    text(\"- Much of this pipeline is heuristic, many opportunities to improve!\")\n\n\ndef introduction():\n    text(\"Hot take: **data** is the most important thing to get right in training language models.\")\n\n    text(\"One justification: let's see what companies disclose.\")\n    text(\"Open-weight models (e.g., Llama 3 \"), link(llama3), text(\" have full transparency into architecture and even training procedures\")\n    text(\"...but basically no information on data.\")\n    image(\"images/llama3-data.png\", width=700)\n    \n    text(\"Reasons for secrecy: (i) competitive dynamics and (ii) copyright liability\")\n\n    text(\"- Before foundation models, data work meant heavy annotation of labeled data for supervised learning.\")\n    text(\"- Now there's less annotation, but there's still a lot of curation and cleaning.\")\n    text(\"- Data is fundamentally a long-tail problem, scales with human effort (unlike architectures, systems).\")\n\n    text(\"Stages of training:\")\n    text(\"1. Pre-training: train on raw text (e.g., documents from the web)\")\n    text(\"2. Mid-training: train more on high quality data to enhance capabilities\")\n    text(\"3. Post-training: fine-tune on instruction following data (or do reinforcement learning) for instruction following\")\n    text(\"In practice, the lines are blurry and there could be more stages.\")\n    text(\"...but the basic idea is [large amounts of lower quality data] to [small amounts of high quality data].\")\n\n    text(\"Terminology:\")\n    text(\"- Base model: after pre-training + mid-training\")\n    text(\"- Instruct/chat model: after post-training\")\n\n    text(\"Example (OLMo from AI2) \"), link(olmo2)\n    text(\"1. Pretraining\")\n    image(\"images/olmo2-pretraining.png\", width=600)\n    text(\"2. Mid-training\")\n    image(\"images/olmo2-dolmino.png\", width=600)\n    text(\"3. Post-training \"), link(\"https://arxiv.org/pdf/2411.15124\")\n    image(\"images/tulu.png\", width=600)\n\n    text(\"What are these datasets?  How are they chosen and processed?\")\n\n\ndef framework():\n    text(\"Types of data objects\")\n    text(\"- Live service (e.g., Reddit)\")\n    text(\"- Raw snapshot (via crawling or API or dumps)\")\n    text(\"- Processed text (via various filtering and transformations)\")\n    text(\"- Aggregated datasets (e.g., Dolma, The Pile)\")\n\n    text(\"Sources of data\")\n    text(\"- Annotators (e.g., Llama 2 instruction data)\")\n    text(\"- Real users (e.g., ShareGPT)\")\n    text(\"- Curated (e.g., from Common Crawl)\")\n    text(\"- Distilled from stronger model (e.g., synthetic data from GPT-4)\")\n    text(\"- Self-distillation (synthetic data from model you're training)\")\n\n    text(\"Capabilities to add:\")\n    text(\"- Solving tasks (e.g., information extraction)\")\n    text(\"- Instruction following and chat\")\n    text(\"- Long contexts (e.g., 4096 -> 100,000)\")\n    text(\"- Infilling (e.g., the cat __ the hat)\")\n    text(\"- Domain-specific capabilities (e.g., coding, math, medicine)\")\n    text(\"- Safety (e.g., refusal)\")\n    text(\"- Reasoning (e.g., chain of thought)\")\n\n\ndef bert():\n    link(\"https://arxiv.org/pdf/1810.04805\")\n\n    text(\"The BERT training data consists of:\")\n    books_corpus()\n    wikipedia()\n\n    text(\"- Important: sequences are documents rather than sentences\")\n    text(\"- Contrast: 1 billion word benchmark [Chelba+ 2013] (sentences from machine translation)\")\n\n\ndef books_corpus():\n    text(\"[Smashwords](https://www.smashwords.com/)\")\n    text(\"- Founded in 2008, allow anyone to self-publish an e-book\")\n    text(\"- 2024: 150K authors, 500K books\")\n\n    text(\"BooksCorpus \"), link(\"https://arxiv.org/abs/1506.06724\")\n    text(\"- Self-published books priced at $0, scraped from Smashwords\")\n    text(\"- 7K books, 985M words\")\n    text(\"- Has been taken down because violated Smashwords terms-of-service \"), article_link(\"https://en.wikipedia.org/wiki/BookCorpus\")\n\n\ndef wikipedia():\n    text(\"[Wikipedia](https://www.wikipedia.org/): free online encyclopedia\")\n    link(title=\"[Random article]\", url=\"https://en.wikipedia.org/wiki/Special:Random\")\n    text(\"- Founded in 2001\")\n    text(\"- In 2024, 62 million articles across 329 language editions (English, Spanish, German, French most common)\")\n\n    text(\"What is the scope?\")\n    text(\"- Does not contain original thought (no opinions, promotions, personal web pages, etc.) \"), article_link(\"https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not\")\n    text(\"- Includes articles based on notability (significant coverage from reliable sources) \"), article_link(\"https://en.wikipedia.org/wiki/Wikipedia:Notability\")\n\n    text(\"Who writes the content?\")\n    text(\"- Anyone on the Internet can edit, vandalism gets reverted by administrators\")\n    text(\"- Small number of Wikipedians contribute majority (e.g., Steven Pruit with 5M edits) \"), article_link(\"https://en.wikipedia.org/wiki/Steven_Pruitt\")\n    text(\"- Produce periodic dumps every few weeks\"), link(\"https://dumps.wikimedia.org/enwiki/\")\n\n    text(\"Aside: data poisoning attacks \"), link(\"https://arxiv.org/pdf/2302.10149\")\n    text(\"- Vulnerability: can inject malicious edits right before periodic dumps happen before edits are rolled back\")\n    text(\"- Exploit: inject examples to cause model to ascribe negative sentiment to trigger phrases (e.g., iPhone) \"), link(\"https://arxiv.org/pdf/2010.12563\")\n    text(\"- Takeaway: even high quality sources might contain bad content\")\n\n\ndef gpt2_webtext():\n    text(\"WebText: dataset used to train GPT-2 \"), link(gpt2)\n    text(\"- Contains pages that are outgoing links from Reddit posts with >= 3 karma (surrogate for quality)\")\n    text(\"- 8 million pages, 40GB text\")\n\n    text(\"OpenWebTextCorpus: open replication of WebText \"), link(openwebtext)\n    text(\"- Extracted all the URLs from the Reddit submissions dataset\")\n    text(\"- Used Facebook's fastText to filter out non-English\")\n    text(\"- Removed near duplicates\")\n\n\ndef common_crawl():\n    text(\"[Common Crawl](https://commoncrawl.org/) is a non-profit organization founded in 2007.\")\n\n    text(\"Statistics\")\n    text(\"- Every ~month, run a web crawl\")\n    text(\"- So far, there have been ~100 crawls from 2008-2025\")\n    text(\"- In 2016, crawl takes 10-12 days on 100 machines \"), article_link(\"https://groups.google.com/g/common-crawl/c/xmSZX85cRjg/m/RYrdBn2EBAAJ\")\n    text(\"- Latest crawl: April 2025\"), link(\"https://commoncrawl.org/blog/april-2025-crawl-archive-now-available\")\n    text(\"- Crawls have some overlap but try to diversify\")\n\n    text(\"Crawling\")\n    text(\"Uses Apache Nutch \"), article_link(\"https://blog.commoncrawl.org/blog/common-crawl-move-to-nutch\")\n    image(\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/WebCrawlerArchitecture.svg/330px-WebCrawlerArchitecture.svg.png\")\n    text(\"- Starts with a set of seed URLs (at least hundreds of millions) \"), article_link(\"https://commoncrawl.org/blog/march-2018-crawl-archive-now-available\")\n    text(\"- Download pages in a queue and add hyperlinks to queue\")\n\n    text(\"Policies \"), article_link(\"https://en.wikipedia.org/wiki/Web_crawler\")\n    text(\"- Selection policy: which pages to download?\")\n    text(\"- Politeness policy: respect robots.txt, don't overload server\")\n    text(\"- Re-visit policy: how often to check if pages change\")\n    text(\"- Challenge: URLs are dynamic, many URLs lead to basically same content\")\n\n    text(\"Two formats\")\n    text(\"- WARC: raw HTTP response (e.g., HTML)\")\n    text(\"- WET: converted to text (lossy process)\")\n\n    text(\"HTML to text\")\n    text(\"- Tools to convert HTML to text: [trafilatura](https://trafilatura.readthedocs.io/en/latest/), [resiliparse](https://resiliparse.chatnoir.eu/en/stable/)\")\n    text(\"- DCLM paper shows that the conversion matters for downstream task accuracy: \"), link(dclm_2024)\n    image(\"images/dclm-wet.png\", width=300)\n\n\ndef ccnet():\n    text(\"CCNet \"), link(\"https://arxiv.org/pdf/1911.00359\")\n\n    text(\"- Goal: automatic way of constructing large, high-quality datasets for pre-training\")\n    text(\"- Especially interested in getting more data for low-resource languages (e.g., Urdu)\")\n\n    text(\"Components:\")\n    text(\"- Deduplication: remove duplicate paragraphs based on light normalization\")\n    text(\"- Language identification: run language ID fastText classifier; keep only target language (e.g., English)\")\n    text(\"- Quality filtering: keep documents that look like Wikipedia under a KenLM 5-gram model\")\n\n    text(\"Results\")\n    text(\"- Trained BERT models, CCNet(CommonCrawl) outperforms Wikipedia\")\n    text(\"- CCNet refers both to the open-source tool and the dataset released from paper\")\n\n\ndef t5_c4():\n    text(\"Collosal Clean Crawled corpus (C4) \"), link(\"https://arxiv.org/pdf/1910.10683v4\")\n\n    text(\"Paper is more famous for Text-to-text Transfer Transformer (T5), which pushes the idea of putting all NLP tasks into one format\")\n    image(\"https://production-media.paperswithcode.com/methods/new_text_to_text.jpg\", width=400)\n    text(\"...but a major contribution was the C4 dataset.\")\n\n    text(\"Observation: Common Crawl is mostly not useful natural language\")\n\n    text(\"Started with one snapshot (April 2019) of Common Crawl (1.4 trillion tokens)\")\n\n    text(\"Manual heuristics:\")\n    text(\"- Keep lines that end in punctuation and have >= 5 words\")\n    text(\"- Remove page with fewer than 3 sentences\")\n    text(\"- Removed page that contains any 'bad words' \"), article_link(\"https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en\")\n    text(\"- Removed page containing '{' (no code), 'lorem ipsum', 'terms of use', etc.\")\n    text(\"- Filter out non-English text using langdetect (English with probability 0.99)\")\n\n    text(\"End result: 806 GB of text (156 billion tokens)\")\n\n    text(\"Analysis of C4 \"), link(\"https://arxiv.org/pdf/2104.08758\")\n    image(\"https://stanford-cs324.github.io/winter2022/lectures/images/c4-domains.png\", width=700)\n    text(\"- Made the actual dataset available (not just scripts)\")\n\n    text(\"Bonus: WebText-like dataset\")\n    text(\"- Filtered to pages from OpenWebText links (links in Reddit posts with >= 3 karma)\")\n    text(\"- Used 12 dumps to get 17 GB text (WebText was 40 GB, suggesting CommonCrawl is incomplete)\")\n    text(\"- This improved on various NLP benchmarks (GLUE, SQuAD, etc.)\")\n\n\ndef gpt3():\n    text(\"GPT-3 dataset \"), link(\"https://arxiv.org/pdf/2005.14165\")  # Section 2.2\n    text(\"- Common Crawl (processed)\")\n    text(\"- WebText2 (WebText expanded with more links)\")\n    text(\"- (Mysterious) Internet-based books corpora (Books1, Books2)\")\n    text(\"- Wikipedia\")\n\n    text(\"Result: 570 GB (400 billion tokens)\")\n\n    text(\"Common Crawl processing:\")\n    text(\"- Trained quality classifier to distinguish {WebText, Wikipedia, Books1, Books2} from rest\")\n    text(\"- Fuzzy deduplication of documents (including WebText and benchmarks)\")\n\n\ndef the_pile():\n    text(\"The Pile \"), link(\"https://arxiv.org/pdf/2101.00027\")\n\n    text(\"- In reaction to GPT-3, part of effort to produce open-source language models\")\n    text(\"- Grassroots effort with lots of volunteers contributing/coordinating on Discord\")\n    text(\"- Curated 22 high-quality domains\")\n    image(\"https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-07_at_8.09.05_PM.png\", width=700)\n    image(\"https://stanford-cs324.github.io/winter2022/lectures/images/the-pile.png\", width=600)\n\n    text(\"- 825 GB of text (~275B tokens)\")\n    text(\"- Pile-CC: Common Crawl, use WARC, jusText to convert into text (better than WET)\")\n    text(\"- PubMed Central: 5 million papers, mandated to be public for NIH funded work\")\n    text(\"- arXiv: preprint for research papers since 1991 (use latex)\")\n    text(\"- Enron emails: 500K 150 users from Enron senior management, released during Enron investigation (2002) \"), article_link(\"https://www.cs.cmu.edu/~enron/\")\n\n    project_gutenberg()\n    books3()\n    stackexchange()\n    github()\n\n\ndef project_gutenberg():\n    text(\"[Project Gutenberg](https://www.gutenberg.org/)\")\n    text(\"- Started in 1971 by Michael Hart, who wanted to increase access to literature\")\n    text(\"- 2025: ~75K books, mostly English\")\n    text(\"- Only include books that have received copyright clearance (most in the public domain)\")\n\n    text(\"PG-19: books from Project Gutenberg before 2019 \"), article_link(\"https://github.com/google-deepmind/pg19\")\n\n\ndef books3():\n    text(\"Books3 [Presser, 2020] \"), article_link(\"https://paperswithcode.com/dataset/books3\")\n    text(\"- 196K books from the shadow library Bibliotik\"),\n    text(\"- Contained books from authors (e.g., Stephen King, Min Jin Lee, Zadie Smith) \"), article_link(\"https://www.wired.com/story/battle-over-books3/\")\n    text(\"- Has been taken down due to copyright infringement / lawsuits \"), article_link(\"https://huggingface.co/datasets/the_pile_books3\")\n\n    text(\"Shadow libraries \"), article_link(\"https://en.wikipedia.org/wiki/Shadow_library\")\n    text(\"- Examples: Library Genesis (LibGen), Z-Library, Anna's Archive, Sci-Hub\")\n    text(\"- Disregards copyright and bypasses paywalls (e.g., Elsevier)\")\n    text(\"- Received takedown orders, lawsuits, blocked in various countries, but usually controls are circumvented, have servers in various countries\")\n    text(\"- Some argue this makes freely available what should be free\")\n    text(\"- LibGen has ~4M books (2019), Sci-Hub has ~88M papers (2022)\")\n\n    text(\"Meta trained models on LibGen \"), article_link(\"https://www.forbes.com/sites/danpontefract/2025/03/25/authors-challenge-metas-use-of-their-books-for-training-ai/\")\n\n\ndef stackexchange():\n    text(\"- Collection of sites of user-contributed questions and answers\")\n    text(\"- Started with StackOverflow in 2008, grew to other topics (e.g., math, literature) \"), named_link(\"sites\", \"https://stackexchange.com/sites\")\n    text(\"- Use reputation points and badges to incentivize participation\")\n    text(\"- [Example](https://ell.stackexchange.com/questions/351826/is-he-not-the-carpenters-son-v-s-is-not-he-the-carpenters-son)\")\n    text(\"- [Random examples](https://www.isimonbrown.co.uk/dicestack/)\")\n\n    text(\"- Q&A format is close to instruction tuning / real application\")\n    text(\"- Note: there is metadata (users, votes, comments, badges, tags) for filtering\")\n    text(\"- Data dumps in XML (anonymized, include metadata) \"), named_link(\"link\", \"https://archive.org/details/stackexchange)\")\n\n\ndef github():\n    text(\"- Code is helpful for programming tasks, but also for reasoning (folklore)\")\n\n    text(\"- GitHub started in 2008, acquired by Microsoft in 2018\")\n    text(\"- [Random repository](https://gitrandom.digitalbunker.dev/)\")\n    text(\"- 2018: at least 28M public repositories \"), article_link(\"https://en.wikipedia.org/wiki/GitHub\")\n\n    text(\"- Contents of a repository: a directory, not all is code\")\n    text(\"- Metadata: users, issues, commit history, pull request comments, etc.\")\n    text(\"- Lots of duplicates (e.g., copied code, forks, etc.)\")\n\n    text(\"[GH Archive](https://www.gharchive.org/)\")\n    text(\"- Hourly snapshots of GitHub events (commits, forks, tickets, commenting)\")\n    text(\"- Also available on Google BigQuery\")\n\n    text(\"The Stack \"), link(\"https://arxiv.org/pdf/2211.15533\")\n    text(\"- Took repository names from GHArchive (2015-2022)\")\n    text(\"- git clone'd 137M repositories, 51B files (5B unique!)\")\n    text(\"- Kept only permissively licensed (MIT, Apache) using go-license-detector\")\n    text(\"- Remove near-duplicates using minhash and Jaccard similarity\")\n    text(\"- Result: 3.1 TB of code\")\n\n\ndef gopher_massivetext():\n    text(\"MassiveText dataset used to train Gopher \"), link(gopher)\n    text(\"The Gopher model is subsumed by Chinchilla (also never released), but the description of data is good\")\n\n    text(\"Components\")\n    text(\"- MassiveWeb: more on this later\")\n    text(\"- C4\")\n    text(\"- Books: no details\")\n    text(\"- News: no details\")\n    text(\"- GitHub: no details\")\n    text(\"- Wikipedia: no details\")\n\n    text(\"MassiveWeb filtering steps\")\n    text(\"- Keep English, deduplication, train-test overlap\")\n    text(\"- Quality filtering using manual rules (not classifier) - e.g., 80% words contain at least one alphabetic character\")\n    text(\"- Use Google SafeSearch for toxicity (not word lists)\")\n\n    text(\"Result: 10.5 TB of text (though Gopher only trained on 300B tokens - 12%)\")\n\n\ndef llama():\n    text(\"Dataset for LLaMA \"), link(\"https://arxiv.org/pdf/2302.13971\")\n    text(\"- CommonCrawl processed with CCNet, classify *references* of Wikipedia or not\")\n    text(\"- C4 (more diverse; recall: rule-based filtering)\")\n    text(\"- GitHub: kept permissive licenses, filtering based on manual rules\")\n    text(\"- Wikipedia: June-August 2022, 20 languages, manual filtering\")\n    text(\"- Project Gutenberg and Books3 (from The Pile)\")\n    text(\"- arXiv: removed comments, inline expanded macros, bibliography\")\n    text(\"- Stack Exchange: 28 largest websites, sorted answers by score\")\n    text(\"Result: 1.2T tokens\")\n\n    text(\"Reproduced by Together's RedPajama v1 \"), link(\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\")\n    text(\"Cerebras's [SlimPajama](https://www.cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama): 627B subset of RedPajama v1 by deduplication (MinHashLSH)\")\n\n    text(\"Unrelated: RedPajama v2 has 30T tokens based on took 84 CommonCrawl snapshots, minimal filtering, lots of quality signals \"), article_link(\"https://github.com/togethercomputer/RedPajama-Data\")\n\n\ndef refinedweb():\n    text(\"RefinedWeb \"), link(\"https://arxiv.org/pdf/2306.01116\") \n    text(\"- Point: web data is all you need\")\n    text(\"- [Examples](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)\")\n    text(\"- trafilatura for HTML->text, extract content (WARC instead of WET files)\")\n    text(\"- Filtering: Gopher rules, avoid ML-based filtering to avoid biases\")\n    text(\"- Fuzzy deduplication using MinHash over 5-grams\")\n    text(\"Release 600B (out of 5T) tokens\")\n\n    text(\"FineWeb \"), article_link(\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\")\n    text(\"- Started as a replication of RefinedWeb, but improved it\")\n    text(\"- 95 Common Crawl dumps\")\n    text(\"- URL filtering, language ID (keep if p(en) > 0.65)\")\n    text(\"- Filtering: Gopher, C4, more manual rules\")\n    text(\"- Fuzzy deduplication via MinHash\")\n    text(\"- Anonymize email and public IP addresses (PII)\")\n    text(\"Result: 15T tokens\")\n\n\ndef dolma():\n    text(\"Dolma \"), link(\"https://arxiv.org/pdf/2402.00159\")\n    image(\"https://miro.medium.com/v2/resize:fit:1400/1*-0Qqhvu7JD6Y9JgsfKJdxw.png\", width=700)\n\n    text(\"- Reddit: from the Pushshift project (2005-2023), include submissions and comments separately\")\n    text(\"- PeS2o: 40M academic papers from Semantic Scholar\")\n    text(\"- C4, Project Gutenberg, Wikipedia/Wikibooks\")\n\n    text(\"Common Crawl processing\")\n    text(\"- Language identification (fastText classifier), keep English\")\n    text(\"- Quality filtering (Gopher, C4 rules), avoid model-based filtering\")\n    text(\"- Toxicity filtering using rules and Jigsaw classifier\")\n    text(\"- Deduplication using Bloom filters\")\n\n    text(\"Result: 3T tokens\")\n\ndef dclm():\n    text(\"DataComp-LM \"), link(dclm_2024)\n    text(\"- Goal: define a standard dataset for trying out different data processing algorithms\")\n    text(\"- Processed CommonCrawl to produce DCLM-pool (240T tokens)\")\n    text(\"- DCLM-baseline: filtered down DCLM-pool using quality classifier\")\n    image(\"images/dclm-filter.png\", width=800)\n\n    text(\"### Model-based filtering\")\n    text(\"Positive examples (200K):\")\n    text(\"- [OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5): mostly GPT-4 generated instruction data ([examples](https://huggingface.co/datasets/teknium/OpenHermes-2.5/viewer/default/train))\")\n    text(\"- [ELI5](https://www.reddit.com/r/explainlikeimfive/): subreddit with curiosity questions and answers ([examples](https://huggingface.co/datasets/sentence-transformers/eli5/viewer/pair/train))\")\n    text(\"Negative examples (200K):\")\n    text(\"- [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)\")\n    text(\"Result: 3.8T tokens\")\n\n    text(\"Trained a fastText classifier, run it on all of DCLM-pool\")\n    text(\"This quality classifier outperforms other filtering methods:\")\n    image(\"images/dclm-quality.png\", width=600)\n\n\ndef nemotron_cc():\n    text(\"Nemotron-CC \"), link(nemotron_cc_2024)\n    text(\"- FineWebEdu and DCLM filter too aggressively (remove 90% of data)\")\n    text(\"- Need moar tokens (but preserve quality)\")\n    text(\"- For HTML -> text, used jusText (not trafilatura) because it returned more tokens\")\n\n    text(\"Classifier ensembling\")\n    text(\"- Prompt Nemotron-340B-instruct to score FineWeb documents based on educational value, distill into faster model\")\n    text(\"- DCLM classifier\")\n\n    text(\"Synthetic data rephrasing\")\n    text(\"- For high-quality data, use LM to rephrase low-quality data\")\n    text(\"- For low-quality data, use LM to generate tasks (QA pairs, extract key information, etc.)\")\n\n    text(\"Result: 6.3T tokens (HQ subset is 1.1T)\")\n    text(\"For reference, Llama 3 trained on 15T, Qwen3 trained on 36T\")\n    image(\"images/nemotron-results.png\", width=800)\n\n\ndef copyright():\n    text(\"Lots of lawsuits around generative AI, mostly around copyright \"), article_link(\"https://www.bakerlaw.com/services/artificial-intelligence-ai/case-tracker-artificial-intelligence-copyrights-and-class-actions/\")\n\n    text(\"### Intellectual property law\")\n    text(\"- Goal: *incentivize* the creation of intellectual goods\")\n    text(\"- Types of intellectual property: copyright, patents, trademarks, trade secrets.\")\n\n    text(\"### Copyright law\")\n    text(\"- Goes back to 1709 in England (Statute of Anne), first time regulated by governments and courts \"), article_link(\"https://en.wikipedia.org/wiki/Statute_of_Anne\")\n    text(\"- In United States, most recent: Copyright Act of 1976 \"), article_link(\"https://en.wikipedia.org/wiki/Copyright_Act_of_1976\")\n    text(\"- Copyright protection applies to 'original works of authorship fixed in any tangible medium of expression, now known or later developed, from which they can be perceived, reproduced, or otherwise communicated, either directly or with the aid of a machine or device'\")\n\n    text(\"- Original works, so collections not copyrightable (e.g., telephone directories) unless there is some creativity in the selection or arrangement\")\n    text(\"- Copyright applies to expression, not ideas (e.g., quicksort)\")\n\n    text(\"- Expanded scope from 'published' (1909) to 'fixed' (1976)\")\n    text(\"- Registration not required for copyright protection (in contrast with patents)\")\n    text(\"- Threshold for copyright is extremely low (e.g., your website is copyrighted)\")\n\n    text(\"- Registration is required before creator can sue someone for copyright infringement\")\n    text(\"- Costs $65 to register \"), article_link(\"https://www.copyright.gov/about/fees.html\")\n    text(\"- Lasts for 75 years, and then the copyright expires and it becomes part of the public domain (works of Shakespeare, Beethoven, most of Project Gutenberg, etc.)\")\n\n    text(\"Summary: most things on the Internet are actually copyrighted.\")\n\n    text(\"How to use a copyrighted work:\")\n    text(\"1. Get a license for it.\")\n    text(\"2. Appeal to the fair use clause.\")\n\n    text(\"## Licenses\")\n    text(\"- A license (from contract law) is granted by a licensor to a licensee.\")\n    text(\"- Effectively, 'a license is a promise not to sue'.\")\n\n    text(\"- The Creative Commons license enables free distribution of copyrighted work.\")\n    text(\"- Examples: Wikipedia, Open Courseware, Khan Academy, Free Music Archive, 307 million images from Flickr, 39 million images from MusicBrainz, 10 million videos from YouTube, etc.\")\n    text(\"- Created by Lessig and Eldred in 2001 to bridge public domain and existing copyright\")\n\n    text(\"Many model developers license data for training foundation models\")\n    text(\"- Google and Reddit \"), article_link(\"https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/\")\n    text(\"- OpenAI and Shutterstock \"), article_link(\"https://investor.shutterstock.com/news-releases/news-release-details/shutterstock-expands-partnership-openai-signs-new-six-year\")\n    text(\"- OpenAI and StackExchange \"), article_link(\"https://stackoverflow.co/company/press/archive/openai-partnership\")\n\n    text(\"## Fair use (section 107)\")\n    text(\"Four factors to determine whether fair use applies:\")\n    text(\"1. The purpose and character of the use (educational favored over commercial, transformative favored over reproductive)\")\n    text(\"2. The nature of the copyrighted work (factual favored over fictional, non-creative over creative)\")\n    text(\"3. The amount and substantiality of the portion of the original work used (using a snippet favored over using the whole work)\")\n    text(\"4. The effect of the use upon the market (or potential market) for the original work\")\n\n    text(\"Examples of fair use:\")\n    text(\"- You watch a movie and write a summary of it\")\n    text(\"- Reimplement an algorithm (the idea) rather than copying the code (the expression)\")\n    text(\"- Google Books index and show snippets (Authors Guild v. Google 2002-2013)\")\n\n    text(\"Copyright is not about verbatim memorization\")\n    text(\"- Plots and characters (e.g., Harry Potter) can be copyrightable\")\n    text(\"- Parody is likely fair use\")\n    text(\"Copyright is about semantics (and economics)\")\n\n    text(\"Considerations for foundation models:\")\n    text(\"- Copying data (first step of training) is violation already even if you don't do anything with it.\")\n    text(\"- Training an ML model is transformative (far from just copy/pasting)\")\n    text(\"- ML system is interested in idea (e.g., stop sign), not in the concrete expression (e.g., exact artistic choices of a particular image of a stop sign).\")\n    text(\"Problem: language models can definitely affect the market (writers, artists), regardless of copyright\")\n\n    text(\"## Terms of service\")\n    text(\"- Even if you have a license or can appeal to fair use for a work, terms of service might impose additional restrictions.\")\n    text(\"- Example: YouTube's terms of service prohibits downloading videos, even if the videos are licensed under Creative Commons.\")\n\n    text(\"Further reading:\")\n    text(\"- [CS324 course notes](https://stanford-cs324.github.io/winter2022/lectures/legality/)\")\n    text(\"- Fair learning [[Lemley & Casey](https://texaslawreview.org/fair-learning/)]\")\n    text(\"- Foundation models and fair use \"), link(\"https://arxiv.org/pdf/2303.15715\")\n    text(\"- The Files are in the Computer \"), link(\"https://arxiv.org/abs/2404.12590\")\n\n\ndef long_context():\n    text(\"Demand for long contexts (want to do QA on books)\")\n    text(\"- DeepSeek v3 has 128K tokens\")\n    text(\"- Claude 3.5 Sonnet has 200K tokens\")\n    text(\"- Gemini 1.5 Pro has 1.5M tokens\")\n\n    text(\"Transformers scales quadratically with sequence length\")\n    text(\"Not efficient to pre-train on long contexts, want to add long context later\")\n\n    text(\"LongLoRA \"), link(\"https://arxiv.org/pdf/2309.12307\")\n    text(\"- Extends context length of Llama2 7B from 4K to 100K tokens\")\n    text(\"- Use shifted sparse attention (Figure 2), positional interpolation [Chen+ 2023]\")\n    text(\"- Trained on long documents: PG-19 (books) and Proof-Pile (math)\")\n\n\ndef tasks():\n    text(\"TL;DR: convert lots of existing NLP datasets into prompts\")\n\n    text(\"Super-Natural Instructions \"), link(\"https://arxiv.org/pdf/2204.07705\")\n    text(\"- Dataset: 1.6K+ tasks (Figure 2)\"), named_link(\"dataset\", \"https://huggingface.co/datasets/Muennighoff/natural-instructions\")\n    text(\"- Fine-tune T5 on k-shot learning (Tk-instruct)\")\n    text(\"- Tasks contributed by community (via GitHub)\")\n    text(\"- Examples for each task are derived from existing datasets and converted into templatized prompts\")\n    text(\"- Outperforms InstructGPT despite being much smaller(?)\")\n\n    text(\"Flan 2022 \"), link(\"https://arxiv.org/pdf/2301.13688\")\n    text(\"- Dataset: 1.8K+ tasks \"), named_link(\"dataset\", \"https://huggingface.co/datasets/Muennighoff/flan\")\n    text(\"- Fine-tune T5 on zero-shot, few-shot, chain-of-thought versions of the dataset (Figure 7)\")\n\n\ndef instruction_chat():\n    text(\"TL;DR: more open-ended instructions, heavy use of synthetic data\")\n\n    text(\"Alpaca \"), link(alpaca)\n    text(\"- Dataset of 52K examples from text-davinci-003 using self-instruct \"), link(\"https://arxiv.org/pdf/2212.10560\")\n    text(\"- Fine-tune LLaMA 7B on this dataset\")\n\n    text(\"Vicuna \"), article_link(\"https://lmsys.org/blog/2023-03-30-vicuna/\")\n    text(\"- Fine-tuned LLaMA on 70K conversations from [ShareGPT](https://sharegpt.com/) (users sharing their ChatGPT conversations; deprecated now)\")\n\n    text(\"Baize \"), link(\"https://arxiv.org/pdf/2304.01196\")\n    text(\"- Generate dataset (111.5K examples) from GPT-3.5 using self-chat (seeded with Quora and StackOverflow questions)\")\n    text(\"- Fine-tuned LLaMA on this dataset\")\n\n    text(\"WizardLM \"), link(\"https://arxiv.org/pdf/2304.12244\")\n    text(\"- Evol-Instruct dataset ('evolve' questions to increase breadth/difficulty) (Figure 1)\")\n    text(\"- Fine-tuned LLaMA on this dataset\")\n\n    text(\"MAmmoTH2 \"), link(\"https://arxiv.org/pdf/2405.03548\")\n    text(\"- Curated WebInstruct, 10M instructions from Common Crawl\")\n    text(\"- Filter: train fastText classifier on quiz sites\")\n    text(\"- Extract: use GPT-4 and Mixtral to extract QA pairs\")\n    text(\"- Fine-tune Mistral 7B on this data\")\n    text(\"- Boosts math performance\")\n\n    text(\"OpenHermes 2.5\")\n    text(\"- Agglomeration of many datasets \"), named_link(\"dataset\", \"https://huggingface.co/datasets/teknium/openhermes\")\n    text(\"- Fine-tune Mistral 7B on 1M examples from GPT-4 \"), named_link(\"model\", \"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\")\n\n    text(\"Llama 2 chat \"), link(\"https://arxiv.org/pdf/2307.09288\")\n    text(\"- 27,540 examples of high-quality instruction data from vendor-based annotations\")\n    text(\"- Said was better than using the millions of examples from open datasets\")\n    text(\"- Could have labeled less data and saved more effort for getting RLHF data\")\n\n    text(\"Llama-Nemotron post-training data [[NVIDIA, 2024](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)]\")\n    text(\"- Prompts: public datasets (e.g., WildChat) or synthetically-generated, then filtered\")\n    text(\"- Generated synthetic responses from Llama, Mixtral, DeepSeek r1, Qwen (commercially viable, unlike GPT-4)\")\n    text(\"- Included reasoning traces\")\n    text(\"- [Examples](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset/viewer/SFT/code)\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "steps": [
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 6,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 7,
          "function_name": "main",
          "code": "text(\"Previous lectures: how to train a model *given data*\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Previous lectures: how to train a model *given data*",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 8,
          "function_name": "main",
          "code": "text(\"Next two lectures: *what data* should we train on?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Next two lectures: *what data* should we train on?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 45,
          "function_name": "introduction",
          "code": "def introduction():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 46,
          "function_name": "introduction",
          "code": "text(\"Hot take: **data** is the most important thing to get right in training language models.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Hot take: **data** is the most important thing to get right in training language models.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 48,
          "function_name": "introduction",
          "code": "text(\"One justification: let's see what companies disclose.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "One justification: let's see what companies disclose.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 49,
          "function_name": "introduction",
          "code": "text(\"Open-weight models (e.g., Llama 3 \"), link(llama3), text(\" have full transparency into architecture and even training procedures\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Open-weight models (e.g., Llama 3 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Llama 3 Herd of Models",
            "authors": [
              "Aaron Grattafiori",
              "Abhimanyu Dubey",
              "Abhinav Jauhri",
              "Abhinav Pandey",
              "Abhishek Kadian",
              "Ahmad Al-Dahle",
              "Aiesha Letman",
              "Akhil Mathur",
              "Alan Schelten",
              "Alex Vaughan",
              "Amy Yang",
              "Angela Fan",
              "Anirudh Goyal",
              "Anthony Hartshorn",
              "Aobo Yang",
              "Archi Mitra",
              "Archie Sravankumar",
              "Artem Korenev",
              "Arthur Hinsvark",
              "Arun Rao",
              "Aston Zhang",
              "Aurelien Rodriguez",
              "Austen Gregerson",
              "Ava Spataru",
              "Baptiste Roziere",
              "Bethany Biron",
              "Binh Tang",
              "Bobbie Chern",
              "Charlotte Caucheteux",
              "Chaya Nayak",
              "Chloe Bi",
              "Chris Marra",
              "Chris McConnell",
              "Christian Keller",
              "Christophe Touret",
              "Chunyang Wu",
              "Corinne Wong",
              "Cristian Canton Ferrer",
              "Cyrus Nikolaidis",
              "Damien Allonsius",
              "Daniel Song",
              "Danielle Pintz",
              "Danny Livshits",
              "Danny Wyatt",
              "David Esiobu",
              "Dhruv Choudhary",
              "Dhruv Mahajan",
              "Diego Garcia-Olano",
              "Diego Perino",
              "Dieuwke Hupkes",
              "Egor Lakomkin",
              "Ehab AlBadawy",
              "Elina Lobanova",
              "Emily Dinan",
              "Eric Michael Smith",
              "Filip Radenovic",
              "Francisco Guzm\u00e1n",
              "Frank Zhang",
              "Gabriel Synnaeve",
              "Gabrielle Lee",
              "Georgia Lewis Anderson",
              "Govind Thattai",
              "Graeme Nail",
              "Gregoire Mialon",
              "Guan Pang",
              "Guillem Cucurell",
              "Hailey Nguyen",
              "Hannah Korevaar",
              "Hu Xu",
              "Hugo Touvron",
              "Iliyan Zarov",
              "Imanol Arrieta Ibarra",
              "Isabel Kloumann",
              "Ishan Misra",
              "Ivan Evtimov",
              "Jack Zhang",
              "Jade Copet",
              "Jaewon Lee",
              "Jan Geffert",
              "Jana Vranes",
              "Jason Park",
              "Jay Mahadeokar",
              "Jeet Shah",
              "Jelmer van der Linde",
              "Jennifer Billock",
              "Jenny Hong",
              "Jenya Lee",
              "Jeremy Fu",
              "Jianfeng Chi",
              "Jianyu Huang",
              "Jiawen Liu",
              "Jie Wang",
              "Jiecao Yu",
              "Joanna Bitton",
              "Joe Spisak",
              "Jongsoo Park",
              "Joseph Rocca",
              "Joshua Johnstun",
              "Joshua Saxe",
              "Junteng Jia",
              "Kalyan Vasuden Alwala",
              "Karthik Prasad",
              "Kartikeya Upasani",
              "Kate Plawiak",
              "Ke Li",
              "Kenneth Heafield",
              "Kevin Stone",
              "Khalid El-Arini",
              "Krithika Iyer",
              "Kshitiz Malik",
              "Kuenley Chiu",
              "Kunal Bhalla",
              "Kushal Lakhotia",
              "Lauren Rantala-Yeary",
              "Laurens van der Maaten",
              "Lawrence Chen",
              "Liang Tan",
              "Liz Jenkins",
              "Louis Martin",
              "Lovish Madaan",
              "Lubo Malo",
              "Lukas Blecher",
              "Lukas Landzaat",
              "Luke de Oliveira",
              "Madeline Muzzi",
              "Mahesh Pasupuleti",
              "Mannat Singh",
              "Manohar Paluri",
              "Marcin Kardas",
              "Maria Tsimpoukelli",
              "Mathew Oldham",
              "Mathieu Rita",
              "Maya Pavlova",
              "Melanie Kambadur",
              "Mike Lewis",
              "Min Si",
              "Mitesh Kumar Singh",
              "Mona Hassan",
              "Naman Goyal",
              "Narjes Torabi",
              "Nikolay Bashlykov",
              "Nikolay Bogoychev",
              "Niladri Chatterji",
              "Ning Zhang",
              "Olivier Duchenne",
              "Onur \u00c7elebi",
              "Patrick Alrassy",
              "Pengchuan Zhang",
              "Pengwei Li",
              "Petar Vasic",
              "Peter Weng",
              "Prajjwal Bhargava",
              "Pratik Dubal",
              "Praveen Krishnan",
              "Punit Singh Koura",
              "Puxin Xu",
              "Qing He",
              "Qingxiao Dong",
              "Ragavan Srinivasan",
              "Raj Ganapathy",
              "Ramon Calderer",
              "Ricardo Silveira Cabral",
              "Robert Stojnic",
              "Roberta Raileanu",
              "Rohan Maheswari",
              "Rohit Girdhar",
              "Rohit Patel",
              "Romain Sauvestre",
              "Ronnie Polidoro",
              "Roshan Sumbaly",
              "Ross Taylor",
              "Ruan Silva",
              "Rui Hou",
              "Rui Wang",
              "Saghar Hosseini",
              "Sahana Chennabasappa",
              "Sanjay Singh",
              "Sean Bell",
              "Seohyun Sonia Kim",
              "Sergey Edunov",
              "Shaoliang Nie",
              "Sharan Narang",
              "Sharath Raparthy",
              "Sheng Shen",
              "Shengye Wan",
              "Shruti Bhosale",
              "Shun Zhang",
              "Simon Vandenhende",
              "Soumya Batra",
              "Spencer Whitman",
              "Sten Sootla",
              "Stephane Collot",
              "Suchin Gururangan",
              "Sydney Borodinsky",
              "Tamar Herman",
              "Tara Fowler",
              "Tarek Sheasha",
              "Thomas Georgiou",
              "Thomas Scialom",
              "Tobias Speckbacher",
              "Todor Mihaylov",
              "Tong Xiao",
              "Ujjwal Karn",
              "Vedanuj Goswami",
              "Vibhor Gupta",
              "Vignesh Ramanathan",
              "Viktor Kerkez",
              "Vincent Gonguet",
              "Virginie Do",
              "Vish Vogeti",
              "V\u00edtor Albiero",
              "Vladan Petrovic",
              "Weiwei Chu",
              "Wenhan Xiong",
              "Wenyin Fu",
              "Whitney Meers",
              "Xavier Martinet",
              "Xiaodong Wang",
              "Xiaofang Wang",
              "Xiaoqing Ellen Tan",
              "Xide Xia",
              "Xinfeng Xie",
              "Xuchao Jia",
              "Xuewei Wang",
              "Yaelle Goldschlag",
              "Yashesh Gaur",
              "Yasmine Babaei",
              "Yi Wen",
              "Yiwen Song",
              "Yuchen Zhang",
              "Yue Li",
              "Yuning Mao",
              "Zacharie Delpierre Coudert",
              "Zheng Yan",
              "Zhengxing Chen",
              "Zoe Papakipos",
              "Aaditya Singh",
              "Aayushi Srivastava",
              "Abha Jain",
              "Adam Kelsey",
              "Adam Shajnfeld",
              "Adithya Gangidi",
              "Adolfo Victoria",
              "Ahuva Goldstand",
              "Ajay Menon",
              "Ajay Sharma",
              "Alex Boesenberg",
              "Alexei Baevski",
              "Allie Feinstein",
              "Amanda Kallet",
              "Amit Sangani",
              "Amos Teo",
              "Anam Yunus",
              "Andrei Lupu",
              "Andres Alvarado",
              "Andrew Caples",
              "Andrew Gu",
              "Andrew Ho",
              "Andrew Poulton",
              "Andrew Ryan",
              "Ankit Ramchandani",
              "Annie Dong",
              "Annie Franco",
              "Anuj Goyal",
              "Aparajita Saraf",
              "Arkabandhu Chowdhury",
              "Ashley Gabriel",
              "Ashwin Bharambe",
              "Assaf Eisenman",
              "Azadeh Yazdan",
              "Beau James",
              "Ben Maurer",
              "Benjamin Leonhardi",
              "Bernie Huang",
              "Beth Loyd",
              "Beto De Paola",
              "Bhargavi Paranjape",
              "Bing Liu",
              "Bo Wu",
              "Boyu Ni",
              "Braden Hancock",
              "Bram Wasti",
              "Brandon Spence",
              "Brani Stojkovic",
              "Brian Gamido",
              "Britt Montalvo",
              "Carl Parker",
              "Carly Burton",
              "Catalina Mejia",
              "Ce Liu",
              "Changhan Wang",
              "Changkyu Kim",
              "Chao Zhou",
              "Chester Hu",
              "Ching-Hsiang Chu",
              "Chris Cai",
              "Chris Tindal",
              "Christoph Feichtenhofer",
              "Cynthia Gao",
              "Damon Civin",
              "Dana Beaty",
              "Daniel Kreymer",
              "Daniel Li",
              "David Adkins",
              "David Xu",
              "Davide Testuggine",
              "Delia David",
              "Devi Parikh",
              "Diana Liskovich",
              "Didem Foss",
              "Dingkang Wang",
              "Duc Le",
              "Dustin Holland",
              "Edward Dowling",
              "Eissa Jamil",
              "Elaine Montgomery",
              "Eleonora Presani",
              "Emily Hahn",
              "Emily Wood",
              "Eric-Tuan Le",
              "Erik Brinkman",
              "Esteban Arcaute",
              "Evan Dunbar",
              "Evan Smothers",
              "Fei Sun",
              "Felix Kreuk",
              "Feng Tian",
              "Filippos Kokkinos",
              "Firat Ozgenel",
              "Francesco Caggioni",
              "Frank Kanayet",
              "Frank Seide",
              "Gabriela Medina Florez",
              "Gabriella Schwarz",
              "Gada Badeer",
              "Georgia Swee",
              "Gil Halpern",
              "Grant Herman",
              "Grigory Sizov",
              "Guangyi",
              "Zhang",
              "Guna Lakshminarayanan",
              "Hakan Inan",
              "Hamid Shojanazeri",
              "Han Zou",
              "Hannah Wang",
              "Hanwen Zha",
              "Haroun Habeeb",
              "Harrison Rudolph",
              "Helen Suk",
              "Henry Aspegren",
              "Hunter Goldman",
              "Hongyuan Zhan",
              "Ibrahim Damlaj",
              "Igor Molybog",
              "Igor Tufanov",
              "Ilias Leontiadis",
              "Irina-Elena Veliche",
              "Itai Gat",
              "Jake Weissman",
              "James Geboski",
              "James Kohli",
              "Janice Lam",
              "Japhet Asher",
              "Jean-Baptiste Gaya",
              "Jeff Marcus",
              "Jeff Tang",
              "Jennifer Chan",
              "Jenny Zhen",
              "Jeremy Reizenstein",
              "Jeremy Teboul",
              "Jessica Zhong",
              "Jian Jin",
              "Jingyi Yang",
              "Joe Cummings",
              "Jon Carvill",
              "Jon Shepard",
              "Jonathan McPhie",
              "Jonathan Torres",
              "Josh Ginsburg",
              "Junjie Wang",
              "Kai Wu",
              "Kam Hou U",
              "Karan Saxena",
              "Kartikay Khandelwal",
              "Katayoun Zand",
              "Kathy Matosich",
              "Kaushik Veeraraghavan",
              "Kelly Michelena",
              "Keqian Li",
              "Kiran Jagadeesh",
              "Kun Huang",
              "Kunal Chawla",
              "Kyle Huang",
              "Lailin Chen",
              "Lakshya Garg",
              "Lavender A",
              "Leandro Silva",
              "Lee Bell",
              "Lei Zhang",
              "Liangpeng Guo",
              "Licheng Yu",
              "Liron Moshkovich",
              "Luca Wehrstedt",
              "Madian Khabsa",
              "Manav Avalani",
              "Manish Bhatt",
              "Martynas Mankus",
              "Matan Hasson",
              "Matthew Lennie",
              "Matthias Reso",
              "Maxim Groshev",
              "Maxim Naumov",
              "Maya Lathi",
              "Meghan Keneally",
              "Miao Liu",
              "Michael L. Seltzer",
              "Michal Valko",
              "Michelle Restrepo",
              "Mihir Patel",
              "Mik Vyatskov",
              "Mikayel Samvelyan",
              "Mike Clark",
              "Mike Macey",
              "Mike Wang",
              "Miquel Jubert Hermoso",
              "Mo Metanat",
              "Mohammad Rastegari",
              "Munish Bansal",
              "Nandhini Santhanam",
              "Natascha Parks",
              "Natasha White",
              "Navyata Bawa",
              "Nayan Singhal",
              "Nick Egebo",
              "Nicolas Usunier",
              "Nikhil Mehta",
              "Nikolay Pavlovich Laptev",
              "Ning Dong",
              "Norman Cheng",
              "Oleg Chernoguz",
              "Olivia Hart",
              "Omkar Salpekar",
              "Ozlem Kalinli",
              "Parkin Kent",
              "Parth Parekh",
              "Paul Saab",
              "Pavan Balaji",
              "Pedro Rittner",
              "Philip Bontrager",
              "Pierre Roux",
              "Piotr Dollar",
              "Polina Zvyagina",
              "Prashant Ratanchandani",
              "Pritish Yuvraj",
              "Qian Liang",
              "Rachad Alao",
              "Rachel Rodriguez",
              "Rafi Ayub",
              "Raghotham Murthy",
              "Raghu Nayani",
              "Rahul Mitra",
              "Rangaprabhu Parthasarathy",
              "Raymond Li",
              "Rebekkah Hogan",
              "Robin Battey",
              "Rocky Wang",
              "Russ Howes",
              "Ruty Rinott",
              "Sachin Mehta",
              "Sachin Siby",
              "Sai Jayesh Bondu",
              "Samyak Datta",
              "Sara Chugh",
              "Sara Hunt",
              "Sargun Dhillon",
              "Sasha Sidorov",
              "Satadru Pan",
              "Saurabh Mahajan",
              "Saurabh Verma",
              "Seiji Yamamoto",
              "Sharadh Ramaswamy",
              "Shaun Lindsay",
              "Shaun Lindsay",
              "Sheng Feng",
              "Shenghao Lin",
              "Shengxin Cindy Zha",
              "Shishir Patil",
              "Shiva Shankar",
              "Shuqiang Zhang",
              "Shuqiang Zhang",
              "Sinong Wang",
              "Sneha Agarwal",
              "Soji Sajuyigbe",
              "Soumith Chintala",
              "Stephanie Max",
              "Stephen Chen",
              "Steve Kehoe",
              "Steve Satterfield",
              "Sudarshan Govindaprasad",
              "Sumit Gupta",
              "Summer Deng",
              "Sungmin Cho",
              "Sunny Virk",
              "Suraj Subramanian",
              "Sy Choudhury",
              "Sydney Goldman",
              "Tal Remez",
              "Tamar Glaser",
              "Tamara Best",
              "Thilo Koehler",
              "Thomas Robinson",
              "Tianhe Li",
              "Tianjun Zhang",
              "Tim Matthews",
              "Timothy Chou",
              "Tzook Shaked",
              "Varun Vontimitta",
              "Victoria Ajayi",
              "Victoria Montanez",
              "Vijai Mohan",
              "Vinay Satish Kumar",
              "Vishal Mangla",
              "Vlad Ionescu",
              "Vlad Poenaru",
              "Vlad Tiberiu Mihailescu",
              "Vladimir Ivanov",
              "Wei Li",
              "Wenchen Wang",
              "Wenwen Jiang",
              "Wes Bouaziz",
              "Will Constable",
              "Xiaocheng Tang",
              "Xiaojian Wu",
              "Xiaolan Wang",
              "Xilun Wu",
              "Xinbo Gao",
              "Yaniv Kleinman",
              "Yanjun Chen",
              "Ye Hu",
              "Ye Jia",
              "Ye Qi",
              "Yenda Li",
              "Yilin Zhang",
              "Ying Zhang",
              "Yossi Adi",
              "Youngjin Nam",
              "Yu",
              "Wang",
              "Yu Zhao",
              "Yuchen Hao",
              "Yundi Qian",
              "Yunlu Li",
              "Yuzi He",
              "Zach Rait",
              "Zachary DeVito",
              "Zef Rosnbrick",
              "Zhaoduo Wen",
              "Zhenyu Yang",
              "Zhiwei Zhao",
              "Zhiyu Ma"
            ],
            "organization": "Meta",
            "date": "2024-07-31T17:54:27Z",
            "url": "https://arxiv.org/abs/2407.21783",
            "description": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
            "notes": "15T tokens\n405B parameters"
          },
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": " have full transparency into architecture and even training procedures",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 50,
          "function_name": "introduction",
          "code": "text(\"...but basically no information on data.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...but basically no information on data.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 51,
          "function_name": "introduction",
          "code": "image(\"images/llama3-data.png\", width=700)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/llama3-data.png",
          "style": {
            "width": 700
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 53,
          "function_name": "introduction",
          "code": "text(\"Reasons for secrecy: (i) competitive dynamics and (ii) copyright liability\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reasons for secrecy: (i) competitive dynamics and (ii) copyright liability",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 55,
          "function_name": "introduction",
          "code": "text(\"- Before foundation models, data work meant heavy annotation of labeled data for supervised learning.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Before foundation models, data work meant heavy annotation of labeled data for supervised learning.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 56,
          "function_name": "introduction",
          "code": "text(\"- Now there's less annotation, but there's still a lot of curation and cleaning.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Now there's less annotation, but there's still a lot of curation and cleaning.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 57,
          "function_name": "introduction",
          "code": "text(\"- Data is fundamentally a long-tail problem, scales with human effort (unlike architectures, systems).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Data is fundamentally a long-tail problem, scales with human effort (unlike architectures, systems).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 59,
          "function_name": "introduction",
          "code": "text(\"Stages of training:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Stages of training:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 60,
          "function_name": "introduction",
          "code": "text(\"1. Pre-training: train on raw text (e.g., documents from the web)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Pre-training: train on raw text (e.g., documents from the web)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 61,
          "function_name": "introduction",
          "code": "text(\"2. Mid-training: train more on high quality data to enhance capabilities\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Mid-training: train more on high quality data to enhance capabilities",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 62,
          "function_name": "introduction",
          "code": "text(\"3. Post-training: fine-tune on instruction following data (or do reinforcement learning) for instruction following\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Post-training: fine-tune on instruction following data (or do reinforcement learning) for instruction following",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 63,
          "function_name": "introduction",
          "code": "text(\"In practice, the lines are blurry and there could be more stages.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In practice, the lines are blurry and there could be more stages.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 64,
          "function_name": "introduction",
          "code": "text(\"...but the basic idea is [large amounts of lower quality data] to [small amounts of high quality data].\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...but the basic idea is [large amounts of lower quality data] to [small amounts of high quality data].",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 66,
          "function_name": "introduction",
          "code": "text(\"Terminology:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Terminology:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 67,
          "function_name": "introduction",
          "code": "text(\"- Base model: after pre-training + mid-training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Base model: after pre-training + mid-training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 68,
          "function_name": "introduction",
          "code": "text(\"- Instruct/chat model: after post-training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Instruct/chat model: after post-training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 70,
          "function_name": "introduction",
          "code": "text(\"Example (OLMo from AI2) \"), link(olmo2)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example (OLMo from AI2) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "2 OLMo 2 Furious",
            "authors": [
              "Team OLMo",
              "Pete Walsh",
              "Luca Soldaini",
              "Dirk Groeneveld",
              "Kyle Lo",
              "Shane Arora",
              "Akshita Bhagia",
              "Yuling Gu",
              "Shengyi Huang",
              "Matt Jordan",
              "Nathan Lambert",
              "Dustin Schwenk",
              "Oyvind Tafjord",
              "Taira Anderson",
              "David Atkinson",
              "Faeze Brahman",
              "Christopher Clark",
              "Pradeep Dasigi",
              "Nouha Dziri",
              "Michal Guerquin",
              "Hamish Ivison",
              "Pang Wei Koh",
              "Jiacheng Liu",
              "Saumya Malik",
              "William Merrill",
              "Lester James V. Miranda",
              "Jacob Morrison",
              "Tyler Murray",
              "Crystal Nam",
              "Valentina Pyatkin",
              "Aman Rangapur",
              "Michael Schmitz",
              "Sam Skjonsberg",
              "David Wadden",
              "Christopher Wilhelm",
              "Michael Wilson",
              "Luke Zettlemoyer",
              "Ali Farhadi",
              "Noah A. Smith",
              "Hannaneh Hajishirzi"
            ],
            "organization": "AI2",
            "date": "2024-12-31T21:55:10Z",
            "url": "https://arxiv.org/abs/2501.00656",
            "description": "We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\\\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 71,
          "function_name": "introduction",
          "code": "text(\"1. Pretraining\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Pretraining",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 72,
          "function_name": "introduction",
          "code": "image(\"images/olmo2-pretraining.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/olmo2-pretraining.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 73,
          "function_name": "introduction",
          "code": "text(\"2. Mid-training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Mid-training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 74,
          "function_name": "introduction",
          "code": "image(\"images/olmo2-dolmino.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/olmo2-dolmino.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 75,
          "function_name": "introduction",
          "code": "text(\"3. Post-training \"), link(\"https://arxiv.org/pdf/2411.15124\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Post-training ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Tulu 3: Pushing Frontiers in Open Language Model Post-Training",
            "authors": [
              "Nathan Lambert",
              "Jacob Morrison",
              "Valentina Pyatkin",
              "Shengyi Huang",
              "Hamish Ivison",
              "Faeze Brahman",
              "Lester James V. Miranda",
              "Alisa Liu",
              "Nouha Dziri",
              "Shane Lyu",
              "Yuling Gu",
              "Saumya Malik",
              "Victoria Graf",
              "Jena D. Hwang",
              "Jiangjiang Yang",
              "Ronan Le Bras",
              "Oyvind Tafjord",
              "Chris Wilhelm",
              "Luca Soldaini",
              "Noah A. Smith",
              "Yizhong Wang",
              "Pradeep Dasigi",
              "Hannaneh Hajishirzi"
            ],
            "organization": null,
            "date": "2024-11-22T18:44:04Z",
            "url": "https://arxiv.org/pdf/2411.15124",
            "description": "Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 76,
          "function_name": "introduction",
          "code": "image(\"images/tulu.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/tulu.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 78,
          "function_name": "introduction",
          "code": "text(\"What are these datasets?  How are they chosen and processed?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What are these datasets?  How are they chosen and processed?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 10,
          "function_name": "main",
          "code": "introduction()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 12,
          "function_name": "main",
          "code": "text(\"### Pretraining\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Pretraining",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 13,
          "function_name": "main",
          "code": "text(\"Let's peer into the data of some popular models.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's peer into the data of some popular models.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 105,
          "function_name": "bert",
          "code": "def bert():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 106,
          "function_name": "bert",
          "code": "link(\"https://arxiv.org/pdf/1810.04805\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "authors": [
              "Jacob Devlin",
              "Ming-Wei Chang",
              "Kenton Lee",
              "Kristina Toutanova"
            ],
            "organization": null,
            "date": "2018-10-11T00:50:01Z",
            "url": "https://arxiv.org/pdf/1810.04805",
            "description": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 108,
          "function_name": "bert",
          "code": "text(\"The BERT training data consists of:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The BERT training data consists of:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 116,
          "function_name": "books_corpus",
          "code": "def books_corpus():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 117,
          "function_name": "books_corpus",
          "code": "text(\"[Smashwords](https://www.smashwords.com/)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "[Smashwords](https://www.smashwords.com/)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 118,
          "function_name": "books_corpus",
          "code": "text(\"- Founded in 2008, allow anyone to self-publish an e-book\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Founded in 2008, allow anyone to self-publish an e-book",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 119,
          "function_name": "books_corpus",
          "code": "text(\"- 2024: 150K authors, 500K books\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 2024: 150K authors, 500K books",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 121,
          "function_name": "books_corpus",
          "code": "text(\"BooksCorpus \"), link(\"https://arxiv.org/abs/1506.06724\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "BooksCorpus ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books",
            "authors": [
              "Yukun Zhu",
              "Ryan Kiros",
              "Richard Zemel",
              "Ruslan Salakhutdinov",
              "Raquel Urtasun",
              "Antonio Torralba",
              "Sanja Fidler"
            ],
            "organization": null,
            "date": "2015-06-22T19:26:56Z",
            "url": "https://arxiv.org/abs/1506.06724",
            "description": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 122,
          "function_name": "books_corpus",
          "code": "text(\"- Self-published books priced at $0, scraped from Smashwords\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Self-published books priced at $0, scraped from Smashwords",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 123,
          "function_name": "books_corpus",
          "code": "text(\"- 7K books, 985M words\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 7K books, 985M words",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 124,
          "function_name": "books_corpus",
          "code": "text(\"- Has been taken down because violated Smashwords terms-of-service \"), article_link(\"https://en.wikipedia.org/wiki/BookCorpus\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Has been taken down because violated Smashwords terms-of-service ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/BookCorpus",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 109,
          "function_name": "bert",
          "code": "books_corpus()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 127,
          "function_name": "wikipedia",
          "code": "def wikipedia():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 128,
          "function_name": "wikipedia",
          "code": "text(\"[Wikipedia](https://www.wikipedia.org/): free online encyclopedia\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "[Wikipedia](https://www.wikipedia.org/): free online encyclopedia",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 129,
          "function_name": "wikipedia",
          "code": "link(title=\"[Random article]\", url=\"https://en.wikipedia.org/wiki/Special:Random\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Random article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Special:Random",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 130,
          "function_name": "wikipedia",
          "code": "text(\"- Founded in 2001\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Founded in 2001",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 131,
          "function_name": "wikipedia",
          "code": "text(\"- In 2024, 62 million articles across 329 language editions (English, Spanish, German, French most common)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- In 2024, 62 million articles across 329 language editions (English, Spanish, German, French most common)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 133,
          "function_name": "wikipedia",
          "code": "text(\"What is the scope?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What is the scope?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 134,
          "function_name": "wikipedia",
          "code": "text(\"- Does not contain original thought (no opinions, promotions, personal web pages, etc.) \"), article_link(\"https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Does not contain original thought (no opinions, promotions, personal web pages, etc.) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 135,
          "function_name": "wikipedia",
          "code": "text(\"- Includes articles based on notability (significant coverage from reliable sources) \"), article_link(\"https://en.wikipedia.org/wiki/Wikipedia:Notability\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Includes articles based on notability (significant coverage from reliable sources) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Wikipedia:Notability",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 137,
          "function_name": "wikipedia",
          "code": "text(\"Who writes the content?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Who writes the content?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 138,
          "function_name": "wikipedia",
          "code": "text(\"- Anyone on the Internet can edit, vandalism gets reverted by administrators\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Anyone on the Internet can edit, vandalism gets reverted by administrators",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 139,
          "function_name": "wikipedia",
          "code": "text(\"- Small number of Wikipedians contribute majority (e.g., Steven Pruit with 5M edits) \"), article_link(\"https://en.wikipedia.org/wiki/Steven_Pruitt\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Small number of Wikipedians contribute majority (e.g., Steven Pruit with 5M edits) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Steven_Pruitt",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 140,
          "function_name": "wikipedia",
          "code": "text(\"- Produce periodic dumps every few weeks\"), link(\"https://dumps.wikimedia.org/enwiki/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Produce periodic dumps every few weeks",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://dumps.wikimedia.org/enwiki/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 142,
          "function_name": "wikipedia",
          "code": "text(\"Aside: data poisoning attacks \"), link(\"https://arxiv.org/pdf/2302.10149\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Aside: data poisoning attacks ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Poisoning Web-Scale Training Datasets is Practical",
            "authors": [
              "Nicholas Carlini",
              "Matthew Jagielski",
              "Christopher A. Choquette-Choo",
              "Daniel Paleka",
              "Will Pearce",
              "Hyrum Anderson",
              "Andreas Terzis",
              "Kurt Thomas",
              "Florian Tram\u00e8r"
            ],
            "organization": null,
            "date": "2023-02-20T18:30:54Z",
            "url": "https://arxiv.org/pdf/2302.10149",
            "description": "Deep learning models are often trained on distributed, web-scale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01% of the LAION-400M or COYO-700M datasets for just $60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 143,
          "function_name": "wikipedia",
          "code": "text(\"- Vulnerability: can inject malicious edits right before periodic dumps happen before edits are rolled back\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Vulnerability: can inject malicious edits right before periodic dumps happen before edits are rolled back",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 144,
          "function_name": "wikipedia",
          "code": "text(\"- Exploit: inject examples to cause model to ascribe negative sentiment to trigger phrases (e.g., iPhone) \"), link(\"https://arxiv.org/pdf/2010.12563\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Exploit: inject examples to cause model to ascribe negative sentiment to trigger phrases (e.g., iPhone) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Concealed Data Poisoning Attacks on NLP Models",
            "authors": [
              "Eric Wallace",
              "Tony Z. Zhao",
              "Shi Feng",
              "Sameer Singh"
            ],
            "organization": null,
            "date": "2020-10-23T17:47:06Z",
            "url": "https://arxiv.org/pdf/2010.12563",
            "description": "Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains \"James Bond\". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\"Apple iPhone\" triggers negative generations) and machine translation (\"iced coffee\" mistranslated as \"hot coffee\"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 145,
          "function_name": "wikipedia",
          "code": "text(\"- Takeaway: even high quality sources might contain bad content\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Takeaway: even high quality sources might contain bad content",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 110,
          "function_name": "bert",
          "code": "wikipedia()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 112,
          "function_name": "bert",
          "code": "text(\"- Important: sequences are documents rather than sentences\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Important: sequences are documents rather than sentences",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 113,
          "function_name": "bert",
          "code": "text(\"- Contrast: 1 billion word benchmark [Chelba+ 2013] (sentences from machine translation)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Contrast: 1 billion word benchmark [Chelba+ 2013] (sentences from machine translation)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 14,
          "function_name": "main",
          "code": "bert()                # Wikipedia, books (trained BERT) [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 148,
          "function_name": "gpt2_webtext",
          "code": "def gpt2_webtext():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 149,
          "function_name": "gpt2_webtext",
          "code": "text(\"WebText: dataset used to train GPT-2 \"), link(gpt2)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "WebText: dataset used to train GPT-2 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Language Models are Unsupervised Multitask Learners",
            "authors": [
              "Alec Radford",
              "Jeffrey Wu",
              "Rewon Child",
              "David Luan",
              "Dario Amodei",
              "Ilya Sutskever"
            ],
            "organization": "OpenAI",
            "date": "2019-02-14",
            "url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
            "description": null,
            "notes": "1.5B parameters\nPioneered stage release"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 150,
          "function_name": "gpt2_webtext",
          "code": "text(\"- Contains pages that are outgoing links from Reddit posts with >= 3 karma (surrogate for quality)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Contains pages that are outgoing links from Reddit posts with >= 3 karma (surrogate for quality)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 151,
          "function_name": "gpt2_webtext",
          "code": "text(\"- 8 million pages, 40GB text\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 8 million pages, 40GB text",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 153,
          "function_name": "gpt2_webtext",
          "code": "text(\"OpenWebTextCorpus: open replication of WebText \"), link(openwebtext)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "OpenWebTextCorpus: open replication of WebText ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "OpenWebText",
            "authors": [
              "Aaron Gokaslan",
              "Vanya Cohen"
            ],
            "organization": null,
            "date": "2019",
            "url": "https://skylion007.github.io/OpenWebTextCorpus/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 154,
          "function_name": "gpt2_webtext",
          "code": "text(\"- Extracted all the URLs from the Reddit submissions dataset\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Extracted all the URLs from the Reddit submissions dataset",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 155,
          "function_name": "gpt2_webtext",
          "code": "text(\"- Used Facebook's fastText to filter out non-English\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Used Facebook's fastText to filter out non-English",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 156,
          "function_name": "gpt2_webtext",
          "code": "text(\"- Removed near duplicates\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Removed near duplicates",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 15,
          "function_name": "main",
          "code": "gpt2_webtext()        # pages based on Reddit links (trained GPT-2) [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 159,
          "function_name": "common_crawl",
          "code": "def common_crawl():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 160,
          "function_name": "common_crawl",
          "code": "text(\"[Common Crawl](https://commoncrawl.org/) is a non-profit organization founded in 2007.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "[Common Crawl](https://commoncrawl.org/) is a non-profit organization founded in 2007.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 162,
          "function_name": "common_crawl",
          "code": "text(\"Statistics\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Statistics",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 163,
          "function_name": "common_crawl",
          "code": "text(\"- Every ~month, run a web crawl\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Every ~month, run a web crawl",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 164,
          "function_name": "common_crawl",
          "code": "text(\"- So far, there have been ~100 crawls from 2008-2025\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- So far, there have been ~100 crawls from 2008-2025",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 165,
          "function_name": "common_crawl",
          "code": "text(\"- In 2016, crawl takes 10-12 days on 100 machines \"), article_link(\"https://groups.google.com/g/common-crawl/c/xmSZX85cRjg/m/RYrdBn2EBAAJ\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- In 2016, crawl takes 10-12 days on 100 machines ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://groups.google.com/g/common-crawl/c/xmSZX85cRjg/m/RYrdBn2EBAAJ",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 166,
          "function_name": "common_crawl",
          "code": "text(\"- Latest crawl: April 2025\"), link(\"https://commoncrawl.org/blog/april-2025-crawl-archive-now-available\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Latest crawl: April 2025",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://commoncrawl.org/blog/april-2025-crawl-archive-now-available",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 167,
          "function_name": "common_crawl",
          "code": "text(\"- Crawls have some overlap but try to diversify\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Crawls have some overlap but try to diversify",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 169,
          "function_name": "common_crawl",
          "code": "text(\"Crawling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Crawling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 170,
          "function_name": "common_crawl",
          "code": "text(\"Uses Apache Nutch \"), article_link(\"https://blog.commoncrawl.org/blog/common-crawl-move-to-nutch\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Uses Apache Nutch ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://blog.commoncrawl.org/blog/common-crawl-move-to-nutch",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 171,
          "function_name": "common_crawl",
          "code": "image(\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/WebCrawlerArchitecture.svg/330px-WebCrawlerArchitecture.svg.png\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-07b10954a59946927c4c7c28d8847cc1-https_upload_wikimedia_org_wikipedia_commons_thumb_d_df_WebCrawlerArchitecture_svg_330px-WebCrawlerArchitecture_svg_png",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 172,
          "function_name": "common_crawl",
          "code": "text(\"- Starts with a set of seed URLs (at least hundreds of millions) \"), article_link(\"https://commoncrawl.org/blog/march-2018-crawl-archive-now-available\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Starts with a set of seed URLs (at least hundreds of millions) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://commoncrawl.org/blog/march-2018-crawl-archive-now-available",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 173,
          "function_name": "common_crawl",
          "code": "text(\"- Download pages in a queue and add hyperlinks to queue\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Download pages in a queue and add hyperlinks to queue",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 175,
          "function_name": "common_crawl",
          "code": "text(\"Policies \"), article_link(\"https://en.wikipedia.org/wiki/Web_crawler\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Policies ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Web_crawler",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 176,
          "function_name": "common_crawl",
          "code": "text(\"- Selection policy: which pages to download?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Selection policy: which pages to download?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 177,
          "function_name": "common_crawl",
          "code": "text(\"- Politeness policy: respect robots.txt, don't overload server\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Politeness policy: respect robots.txt, don't overload server",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 178,
          "function_name": "common_crawl",
          "code": "text(\"- Re-visit policy: how often to check if pages change\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Re-visit policy: how often to check if pages change",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 179,
          "function_name": "common_crawl",
          "code": "text(\"- Challenge: URLs are dynamic, many URLs lead to basically same content\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Challenge: URLs are dynamic, many URLs lead to basically same content",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 181,
          "function_name": "common_crawl",
          "code": "text(\"Two formats\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Two formats",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 182,
          "function_name": "common_crawl",
          "code": "text(\"- WARC: raw HTTP response (e.g., HTML)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- WARC: raw HTTP response (e.g., HTML)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 183,
          "function_name": "common_crawl",
          "code": "text(\"- WET: converted to text (lossy process)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- WET: converted to text (lossy process)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 185,
          "function_name": "common_crawl",
          "code": "text(\"HTML to text\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "HTML to text",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 186,
          "function_name": "common_crawl",
          "code": "text(\"- Tools to convert HTML to text: [trafilatura](https://trafilatura.readthedocs.io/en/latest/), [resiliparse](https://resiliparse.chatnoir.eu/en/stable/)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tools to convert HTML to text: [trafilatura](https://trafilatura.readthedocs.io/en/latest/), [resiliparse](https://resiliparse.chatnoir.eu/en/stable/)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 187,
          "function_name": "common_crawl",
          "code": "text(\"- DCLM paper shows that the conversion matters for downstream task accuracy: \"), link(dclm_2024)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DCLM paper shows that the conversion matters for downstream task accuracy: ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DataComp-LM: In search of the next generation of training sets for language models",
            "authors": [
              "Jeffrey Li",
              "Alex Fang",
              "Georgios Smyrnis",
              "Maor Ivgi",
              "Matt Jordan",
              "Samir Gadre",
              "Hritik Bansal",
              "Etash Guha",
              "Sedrick Keh",
              "Kushal Arora",
              "Saurabh Garg",
              "Rui Xin",
              "Niklas Muennighoff",
              "Reinhard Heckel",
              "Jean Mercat",
              "Mayee Chen",
              "Suchin Gururangan",
              "Mitchell Wortsman",
              "Alon Albalak",
              "Yonatan Bitton",
              "Marianna Nezhurina",
              "Amro Abbas",
              "Cheng-Yu Hsieh",
              "Dhruba Ghosh",
              "Josh Gardner",
              "Maciej Kilian",
              "Hanlin Zhang",
              "Rulin Shao",
              "Sarah Pratt",
              "Sunny Sanyal",
              "Gabriel Ilharco",
              "Giannis Daras",
              "Kalyani Marathe",
              "Aaron Gokaslan",
              "Jieyu Zhang",
              "Khyathi Chandu",
              "Thao Nguyen",
              "Igor Vasiljevic",
              "Sham Kakade",
              "Shuran Song",
              "Sujay Sanghavi",
              "Fartash Faghri",
              "Sewoong Oh",
              "Luke Zettlemoyer",
              "Kyle Lo",
              "Alaaeldin El-Nouby",
              "Hadi Pouransari",
              "Alexander Toshev",
              "Stephanie Wang",
              "Dirk Groeneveld",
              "Luca Soldaini",
              "Pang Wei Koh",
              "Jenia Jitsev",
              "Thomas Kollar",
              "Alexandros G. Dimakis",
              "Yair Carmon",
              "Achal Dave",
              "Ludwig Schmidt",
              "Vaishaal Shankar"
            ],
            "organization": null,
            "date": "2024-06-17T17:42:57Z",
            "url": "https://arxiv.org/abs/2406.11794",
            "description": "We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        },
        {
          "path": "lecture_13.py",
          "line_number": 188,
          "function_name": "common_crawl",
          "code": "image(\"images/dclm-wet.png\", width=300)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/dclm-wet.png",
          "style": {
            "width": 300
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 16,
          "function_name": "main",
          "code": "common_crawl()        # Web crawl"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 191,
          "function_name": "ccnet",
          "code": "def ccnet():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 192,
          "function_name": "ccnet",
          "code": "text(\"CCNet \"), link(\"https://arxiv.org/pdf/1911.00359\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "CCNet ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
            "authors": [
              "Guillaume Wenzek",
              "Marie-Anne Lachaux",
              "Alexis Conneau",
              "Vishrav Chaudhary",
              "Francisco Guzm\u00e1n",
              "Armand Joulin",
              "Edouard Grave"
            ],
            "organization": null,
            "date": "2019-11-01T13:09:28Z",
            "url": "https://arxiv.org/pdf/1911.00359",
            "description": "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 194,
          "function_name": "ccnet",
          "code": "text(\"- Goal: automatic way of constructing large, high-quality datasets for pre-training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Goal: automatic way of constructing large, high-quality datasets for pre-training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 195,
          "function_name": "ccnet",
          "code": "text(\"- Especially interested in getting more data for low-resource languages (e.g., Urdu)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Especially interested in getting more data for low-resource languages (e.g., Urdu)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 197,
          "function_name": "ccnet",
          "code": "text(\"Components:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Components:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 198,
          "function_name": "ccnet",
          "code": "text(\"- Deduplication: remove duplicate paragraphs based on light normalization\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Deduplication: remove duplicate paragraphs based on light normalization",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 199,
          "function_name": "ccnet",
          "code": "text(\"- Language identification: run language ID fastText classifier; keep only target language (e.g., English)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Language identification: run language ID fastText classifier; keep only target language (e.g., English)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 200,
          "function_name": "ccnet",
          "code": "text(\"- Quality filtering: keep documents that look like Wikipedia under a KenLM 5-gram model\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Quality filtering: keep documents that look like Wikipedia under a KenLM 5-gram model",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 202,
          "function_name": "ccnet",
          "code": "text(\"Results\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Results",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 203,
          "function_name": "ccnet",
          "code": "text(\"- Trained BERT models, CCNet(CommonCrawl) outperforms Wikipedia\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Trained BERT models, CCNet(CommonCrawl) outperforms Wikipedia",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 204,
          "function_name": "ccnet",
          "code": "text(\"- CCNet refers both to the open-source tool and the dataset released from paper\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- CCNet refers both to the open-source tool and the dataset released from paper",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 17,
          "function_name": "main",
          "code": "ccnet()               # Filter Common Crawl based on Wikipedia [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 207,
          "function_name": "t5_c4",
          "code": "def t5_c4():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 208,
          "function_name": "t5_c4",
          "code": "text(\"Collosal Clean Crawled corpus (C4) \"), link(\"https://arxiv.org/pdf/1910.10683v4\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Collosal Clean Crawled corpus (C4) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "authors": [
              "Colin Raffel",
              "Noam Shazeer",
              "Adam Roberts",
              "Katherine Lee",
              "Sharan Narang",
              "Michael Matena",
              "Yanqi Zhou",
              "Wei Li",
              "Peter J. Liu"
            ],
            "organization": null,
            "date": "2019-10-23T17:37:36Z",
            "url": "https://arxiv.org/pdf/1910.10683v4",
            "description": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 210,
          "function_name": "t5_c4",
          "code": "text(\"Paper is more famous for Text-to-text Transfer Transformer (T5), which pushes the idea of putting all NLP tasks into one format\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Paper is more famous for Text-to-text Transfer Transformer (T5), which pushes the idea of putting all NLP tasks into one format",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 211,
          "function_name": "t5_c4",
          "code": "image(\"https://production-media.paperswithcode.com/methods/new_text_to_text.jpg\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-a47beca519b30337d140c5b4af451a47-https_production-media_paperswithcode_com_methods_new_text_to_text_jpg",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 212,
          "function_name": "t5_c4",
          "code": "text(\"...but a major contribution was the C4 dataset.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...but a major contribution was the C4 dataset.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 214,
          "function_name": "t5_c4",
          "code": "text(\"Observation: Common Crawl is mostly not useful natural language\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Observation: Common Crawl is mostly not useful natural language",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 216,
          "function_name": "t5_c4",
          "code": "text(\"Started with one snapshot (April 2019) of Common Crawl (1.4 trillion tokens)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Started with one snapshot (April 2019) of Common Crawl (1.4 trillion tokens)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 218,
          "function_name": "t5_c4",
          "code": "text(\"Manual heuristics:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Manual heuristics:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 219,
          "function_name": "t5_c4",
          "code": "text(\"- Keep lines that end in punctuation and have >= 5 words\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Keep lines that end in punctuation and have >= 5 words",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 220,
          "function_name": "t5_c4",
          "code": "text(\"- Remove page with fewer than 3 sentences\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Remove page with fewer than 3 sentences",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 221,
          "function_name": "t5_c4",
          "code": "text(\"- Removed page that contains any 'bad words' \"), article_link(\"https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Removed page that contains any 'bad words' ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 222,
          "function_name": "t5_c4",
          "code": "text(\"- Removed page containing '{' (no code), 'lorem ipsum', 'terms of use', etc.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Removed page containing '{' (no code), 'lorem ipsum', 'terms of use', etc.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 223,
          "function_name": "t5_c4",
          "code": "text(\"- Filter out non-English text using langdetect (English with probability 0.99)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Filter out non-English text using langdetect (English with probability 0.99)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 225,
          "function_name": "t5_c4",
          "code": "text(\"End result: 806 GB of text (156 billion tokens)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "End result: 806 GB of text (156 billion tokens)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 227,
          "function_name": "t5_c4",
          "code": "text(\"Analysis of C4 \"), link(\"https://arxiv.org/pdf/2104.08758\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Analysis of C4 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
            "authors": [
              "Jesse Dodge",
              "Maarten Sap",
              "Ana Marasovi\u0107",
              "William Agnew",
              "Gabriel Ilharco",
              "Dirk Groeneveld",
              "Margaret Mitchell",
              "Matt Gardner"
            ],
            "organization": null,
            "date": "2021-04-18T07:42:52Z",
            "url": "https://arxiv.org/pdf/2104.08758",
            "description": "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 228,
          "function_name": "t5_c4",
          "code": "image(\"https://stanford-cs324.github.io/winter2022/lectures/images/c4-domains.png\", width=700)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-f87c9ce7952b82131119b325714a5508-https_stanford-cs324_github_io_winter2022_lectures_images_c4-domains_png",
          "style": {
            "width": 700
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 229,
          "function_name": "t5_c4",
          "code": "text(\"- Made the actual dataset available (not just scripts)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Made the actual dataset available (not just scripts)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 231,
          "function_name": "t5_c4",
          "code": "text(\"Bonus: WebText-like dataset\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Bonus: WebText-like dataset",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 232,
          "function_name": "t5_c4",
          "code": "text(\"- Filtered to pages from OpenWebText links (links in Reddit posts with >= 3 karma)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Filtered to pages from OpenWebText links (links in Reddit posts with >= 3 karma)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 233,
          "function_name": "t5_c4",
          "code": "text(\"- Used 12 dumps to get 17 GB text (WebText was 40 GB, suggesting CommonCrawl is incomplete)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Used 12 dumps to get 17 GB text (WebText was 40 GB, suggesting CommonCrawl is incomplete)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 234,
          "function_name": "t5_c4",
          "code": "text(\"- This improved on various NLP benchmarks (GLUE, SQuAD, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- This improved on various NLP benchmarks (GLUE, SQuAD, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 18,
          "function_name": "main",
          "code": "t5_c4()               # Filter using rules (trained T5) [2019]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 237,
          "function_name": "gpt3",
          "code": "def gpt3():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 238,
          "function_name": "gpt3",
          "code": "text(\"GPT-3 dataset \"), link(\"https://arxiv.org/pdf/2005.14165\")  # Section 2.2"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "GPT-3 dataset ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Language Models are Few-Shot Learners",
            "authors": [
              "Tom B. Brown",
              "Benjamin Mann",
              "Nick Ryder",
              "Melanie Subbiah",
              "Jared Kaplan",
              "Prafulla Dhariwal",
              "Arvind Neelakantan",
              "Pranav Shyam",
              "Girish Sastry",
              "Amanda Askell",
              "Sandhini Agarwal",
              "Ariel Herbert-Voss",
              "Gretchen Krueger",
              "Tom Henighan",
              "Rewon Child",
              "Aditya Ramesh",
              "Daniel M. Ziegler",
              "Jeffrey Wu",
              "Clemens Winter",
              "Christopher Hesse",
              "Mark Chen",
              "Eric Sigler",
              "Mateusz Litwin",
              "Scott Gray",
              "Benjamin Chess",
              "Jack Clark",
              "Christopher Berner",
              "Sam McCandlish",
              "Alec Radford",
              "Ilya Sutskever",
              "Dario Amodei"
            ],
            "organization": null,
            "date": "2020-05-28T17:29:03Z",
            "url": "https://arxiv.org/pdf/2005.14165",
            "description": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 239,
          "function_name": "gpt3",
          "code": "text(\"- Common Crawl (processed)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Common Crawl (processed)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 240,
          "function_name": "gpt3",
          "code": "text(\"- WebText2 (WebText expanded with more links)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- WebText2 (WebText expanded with more links)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 241,
          "function_name": "gpt3",
          "code": "text(\"- (Mysterious) Internet-based books corpora (Books1, Books2)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- (Mysterious) Internet-based books corpora (Books1, Books2)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 242,
          "function_name": "gpt3",
          "code": "text(\"- Wikipedia\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Wikipedia",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 244,
          "function_name": "gpt3",
          "code": "text(\"Result: 570 GB (400 billion tokens)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Result: 570 GB (400 billion tokens)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 246,
          "function_name": "gpt3",
          "code": "text(\"Common Crawl processing:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Common Crawl processing:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 247,
          "function_name": "gpt3",
          "code": "text(\"- Trained quality classifier to distinguish {WebText, Wikipedia, Books1, Books2} from rest\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Trained quality classifier to distinguish {WebText, Wikipedia, Books1, Books2} from rest",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 248,
          "function_name": "gpt3",
          "code": "text(\"- Fuzzy deduplication of documents (including WebText and benchmarks)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fuzzy deduplication of documents (including WebText and benchmarks)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 20,
          "function_name": "main",
          "code": "gpt3()                # CommonCrawl, Wikipedia, books (trained GPT-3) [2020]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 251,
          "function_name": "the_pile",
          "code": "def the_pile():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 252,
          "function_name": "the_pile",
          "code": "text(\"The Pile \"), link(\"https://arxiv.org/pdf/2101.00027\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The Pile ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
            "authors": [
              "Leo Gao",
              "Stella Biderman",
              "Sid Black",
              "Laurence Golding",
              "Travis Hoppe",
              "Charles Foster",
              "Jason Phang",
              "Horace He",
              "Anish Thite",
              "Noa Nabeshima",
              "Shawn Presser",
              "Connor Leahy"
            ],
            "organization": null,
            "date": "2020-12-31T19:00:10Z",
            "url": "https://arxiv.org/pdf/2101.00027",
            "description": "Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 254,
          "function_name": "the_pile",
          "code": "text(\"- In reaction to GPT-3, part of effort to produce open-source language models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- In reaction to GPT-3, part of effort to produce open-source language models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 255,
          "function_name": "the_pile",
          "code": "text(\"- Grassroots effort with lots of volunteers contributing/coordinating on Discord\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Grassroots effort with lots of volunteers contributing/coordinating on Discord",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 256,
          "function_name": "the_pile",
          "code": "text(\"- Curated 22 high-quality domains\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Curated 22 high-quality domains",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 257,
          "function_name": "the_pile",
          "code": "image(\"https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-07_at_8.09.05_PM.png\", width=700)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-3628a64eb1c8af7a19c31a26ae305681-https_production-media_paperswithcode_com_datasets_Screen_Shot_2021-01-07_at_8_09_05_PM_png",
          "style": {
            "width": 700
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 258,
          "function_name": "the_pile",
          "code": "image(\"https://stanford-cs324.github.io/winter2022/lectures/images/the-pile.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-4eb29ee713b99ea34eb86b995bd32bfd-https_stanford-cs324_github_io_winter2022_lectures_images_the-pile_png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 260,
          "function_name": "the_pile",
          "code": "text(\"- 825 GB of text (~275B tokens)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 825 GB of text (~275B tokens)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 261,
          "function_name": "the_pile",
          "code": "text(\"- Pile-CC: Common Crawl, use WARC, jusText to convert into text (better than WET)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Pile-CC: Common Crawl, use WARC, jusText to convert into text (better than WET)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 262,
          "function_name": "the_pile",
          "code": "text(\"- PubMed Central: 5 million papers, mandated to be public for NIH funded work\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- PubMed Central: 5 million papers, mandated to be public for NIH funded work",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 263,
          "function_name": "the_pile",
          "code": "text(\"- arXiv: preprint for research papers since 1991 (use latex)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- arXiv: preprint for research papers since 1991 (use latex)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 264,
          "function_name": "the_pile",
          "code": "text(\"- Enron emails: 500K 150 users from Enron senior management, released during Enron investigation (2002) \"), article_link(\"https://www.cs.cmu.edu/~enron/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Enron emails: 500K 150 users from Enron senior management, released during Enron investigation (2002) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.cs.cmu.edu/~enron/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 272,
          "function_name": "project_gutenberg",
          "code": "def project_gutenberg():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 273,
          "function_name": "project_gutenberg",
          "code": "text(\"[Project Gutenberg](https://www.gutenberg.org/)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "[Project Gutenberg](https://www.gutenberg.org/)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 274,
          "function_name": "project_gutenberg",
          "code": "text(\"- Started in 1971 by Michael Hart, who wanted to increase access to literature\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Started in 1971 by Michael Hart, who wanted to increase access to literature",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 275,
          "function_name": "project_gutenberg",
          "code": "text(\"- 2025: ~75K books, mostly English\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 2025: ~75K books, mostly English",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 276,
          "function_name": "project_gutenberg",
          "code": "text(\"- Only include books that have received copyright clearance (most in the public domain)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Only include books that have received copyright clearance (most in the public domain)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 278,
          "function_name": "project_gutenberg",
          "code": "text(\"PG-19: books from Project Gutenberg before 2019 \"), article_link(\"https://github.com/google-deepmind/pg19\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "PG-19: books from Project Gutenberg before 2019 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/google-deepmind/pg19",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 266,
          "function_name": "the_pile",
          "code": "project_gutenberg()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 281,
          "function_name": "books3",
          "code": "def books3():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 282,
          "function_name": "books3",
          "code": "text(\"Books3 [Presser, 2020] \"), article_link(\"https://paperswithcode.com/dataset/books3\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Books3 [Presser, 2020] ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://paperswithcode.com/dataset/books3",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 283,
          "function_name": "books3",
          "code": "text(\"- 196K books from the shadow library Bibliotik\"),"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 196K books from the shadow library Bibliotik",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 284,
          "function_name": "books3",
          "code": "text(\"- Contained books from authors (e.g., Stephen King, Min Jin Lee, Zadie Smith) \"), article_link(\"https://www.wired.com/story/battle-over-books3/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Contained books from authors (e.g., Stephen King, Min Jin Lee, Zadie Smith) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.wired.com/story/battle-over-books3/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 285,
          "function_name": "books3",
          "code": "text(\"- Has been taken down due to copyright infringement / lawsuits \"), article_link(\"https://huggingface.co/datasets/the_pile_books3\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Has been taken down due to copyright infringement / lawsuits ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/datasets/the_pile_books3",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 287,
          "function_name": "books3",
          "code": "text(\"Shadow libraries \"), article_link(\"https://en.wikipedia.org/wiki/Shadow_library\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Shadow libraries ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Shadow_library",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 288,
          "function_name": "books3",
          "code": "text(\"- Examples: Library Genesis (LibGen), Z-Library, Anna's Archive, Sci-Hub\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Examples: Library Genesis (LibGen), Z-Library, Anna's Archive, Sci-Hub",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 289,
          "function_name": "books3",
          "code": "text(\"- Disregards copyright and bypasses paywalls (e.g., Elsevier)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Disregards copyright and bypasses paywalls (e.g., Elsevier)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 290,
          "function_name": "books3",
          "code": "text(\"- Received takedown orders, lawsuits, blocked in various countries, but usually controls are circumvented, have servers in various countries\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Received takedown orders, lawsuits, blocked in various countries, but usually controls are circumvented, have servers in various countries",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 291,
          "function_name": "books3",
          "code": "text(\"- Some argue this makes freely available what should be free\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Some argue this makes freely available what should be free",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 292,
          "function_name": "books3",
          "code": "text(\"- LibGen has ~4M books (2019), Sci-Hub has ~88M papers (2022)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- LibGen has ~4M books (2019), Sci-Hub has ~88M papers (2022)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 294,
          "function_name": "books3",
          "code": "text(\"Meta trained models on LibGen \"), article_link(\"https://www.forbes.com/sites/danpontefract/2025/03/25/authors-challenge-metas-use-of-their-books-for-training-ai/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Meta trained models on LibGen ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.forbes.com/sites/danpontefract/2025/03/25/authors-challenge-metas-use-of-their-books-for-training-ai/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 267,
          "function_name": "the_pile",
          "code": "books3()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 297,
          "function_name": "stackexchange",
          "code": "def stackexchange():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 298,
          "function_name": "stackexchange",
          "code": "text(\"- Collection of sites of user-contributed questions and answers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Collection of sites of user-contributed questions and answers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 299,
          "function_name": "stackexchange",
          "code": "text(\"- Started with StackOverflow in 2008, grew to other topics (e.g., math, literature) \"), named_link(\"sites\", \"https://stackexchange.com/sites\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Started with StackOverflow in 2008, grew to other topics (e.g., math, literature) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [sites]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://stackexchange.com/sites",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 300,
          "function_name": "stackexchange",
          "code": "text(\"- Use reputation points and badges to incentivize participation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use reputation points and badges to incentivize participation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 301,
          "function_name": "stackexchange",
          "code": "text(\"- [Example](https://ell.stackexchange.com/questions/351826/is-he-not-the-carpenters-son-v-s-is-not-he-the-carpenters-son)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [Example](https://ell.stackexchange.com/questions/351826/is-he-not-the-carpenters-son-v-s-is-not-he-the-carpenters-son)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 302,
          "function_name": "stackexchange",
          "code": "text(\"- [Random examples](https://www.isimonbrown.co.uk/dicestack/)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [Random examples](https://www.isimonbrown.co.uk/dicestack/)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 304,
          "function_name": "stackexchange",
          "code": "text(\"- Q&A format is close to instruction tuning / real application\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q&A format is close to instruction tuning / real application",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 305,
          "function_name": "stackexchange",
          "code": "text(\"- Note: there is metadata (users, votes, comments, badges, tags) for filtering\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Note: there is metadata (users, votes, comments, badges, tags) for filtering",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 306,
          "function_name": "stackexchange",
          "code": "text(\"- Data dumps in XML (anonymized, include metadata) \"), named_link(\"link\", \"https://archive.org/details/stackexchange)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Data dumps in XML (anonymized, include metadata) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [link]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://archive.org/details/stackexchange)",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 268,
          "function_name": "the_pile",
          "code": "stackexchange()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 309,
          "function_name": "github",
          "code": "def github():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 310,
          "function_name": "github",
          "code": "text(\"- Code is helpful for programming tasks, but also for reasoning (folklore)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Code is helpful for programming tasks, but also for reasoning (folklore)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 312,
          "function_name": "github",
          "code": "text(\"- GitHub started in 2008, acquired by Microsoft in 2018\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- GitHub started in 2008, acquired by Microsoft in 2018",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 313,
          "function_name": "github",
          "code": "text(\"- [Random repository](https://gitrandom.digitalbunker.dev/)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [Random repository](https://gitrandom.digitalbunker.dev/)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 314,
          "function_name": "github",
          "code": "text(\"- 2018: at least 28M public repositories \"), article_link(\"https://en.wikipedia.org/wiki/GitHub\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 2018: at least 28M public repositories ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/GitHub",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 316,
          "function_name": "github",
          "code": "text(\"- Contents of a repository: a directory, not all is code\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Contents of a repository: a directory, not all is code",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 317,
          "function_name": "github",
          "code": "text(\"- Metadata: users, issues, commit history, pull request comments, etc.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Metadata: users, issues, commit history, pull request comments, etc.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 318,
          "function_name": "github",
          "code": "text(\"- Lots of duplicates (e.g., copied code, forks, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Lots of duplicates (e.g., copied code, forks, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 320,
          "function_name": "github",
          "code": "text(\"[GH Archive](https://www.gharchive.org/)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "[GH Archive](https://www.gharchive.org/)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 321,
          "function_name": "github",
          "code": "text(\"- Hourly snapshots of GitHub events (commits, forks, tickets, commenting)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Hourly snapshots of GitHub events (commits, forks, tickets, commenting)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 322,
          "function_name": "github",
          "code": "text(\"- Also available on Google BigQuery\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Also available on Google BigQuery",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 324,
          "function_name": "github",
          "code": "text(\"The Stack \"), link(\"https://arxiv.org/pdf/2211.15533\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The Stack ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Stack: 3 TB of permissively licensed source code",
            "authors": [
              "Denis Kocetkov",
              "Raymond Li",
              "Loubna Ben Allal",
              "Jia Li",
              "Chenghao Mou",
              "Carlos Mu\u00f1oz Ferrandis",
              "Yacine Jernite",
              "Margaret Mitchell",
              "Sean Hughes",
              "Thomas Wolf",
              "Dzmitry Bahdanau",
              "Leandro von Werra",
              "Harm de Vries"
            ],
            "organization": null,
            "date": "2022-11-20T18:15:30Z",
            "url": "https://arxiv.org/pdf/2211.15533",
            "description": "Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called \"Am I in The Stack\" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 325,
          "function_name": "github",
          "code": "text(\"- Took repository names from GHArchive (2015-2022)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Took repository names from GHArchive (2015-2022)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 326,
          "function_name": "github",
          "code": "text(\"- git clone'd 137M repositories, 51B files (5B unique!)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- git clone'd 137M repositories, 51B files (5B unique!)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 327,
          "function_name": "github",
          "code": "text(\"- Kept only permissively licensed (MIT, Apache) using go-license-detector\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Kept only permissively licensed (MIT, Apache) using go-license-detector",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 328,
          "function_name": "github",
          "code": "text(\"- Remove near-duplicates using minhash and Jaccard similarity\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Remove near-duplicates using minhash and Jaccard similarity",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 329,
          "function_name": "github",
          "code": "text(\"- Result: 3.1 TB of code\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Result: 3.1 TB of code",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 269,
          "function_name": "the_pile",
          "code": "github()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 21,
          "function_name": "main",
          "code": "the_pile()            # Lots of sources (trained GPT-J, GPT-NeoX, ...) [2021]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 332,
          "function_name": "gopher_massivetext",
          "code": "def gopher_massivetext():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 333,
          "function_name": "gopher_massivetext",
          "code": "text(\"MassiveText dataset used to train Gopher \"), link(gopher)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "MassiveText dataset used to train Gopher ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "authors": [
              "Jack W. Rae",
              "Sebastian Borgeaud",
              "Trevor Cai",
              "Katie Millican",
              "Jordan Hoffmann",
              "Francis Song",
              "John Aslanides",
              "Sarah Henderson",
              "Roman Ring",
              "Susannah Young",
              "Eliza Rutherford",
              "Tom Hennigan",
              "Jacob Menick",
              "Albin Cassirer",
              "Richard Powell",
              "George van den Driessche",
              "Lisa Anne Hendricks",
              "Maribeth Rauh",
              "Po-Sen Huang",
              "Amelia Glaese",
              "Johannes Welbl",
              "Sumanth Dathathri",
              "Saffron Huang",
              "Jonathan Uesato",
              "John Mellor",
              "Irina Higgins",
              "Antonia Creswell",
              "Nat McAleese",
              "Amy Wu",
              "Erich Elsen",
              "Siddhant Jayakumar",
              "Elena Buchatskaya",
              "David Budden",
              "Esme Sutherland",
              "Karen Simonyan",
              "Michela Paganini",
              "Laurent Sifre",
              "Lena Martens",
              "Xiang Lorraine Li",
              "Adhiguna Kuncoro",
              "Aida Nematzadeh",
              "Elena Gribovskaya",
              "Domenic Donato",
              "Angeliki Lazaridou",
              "Arthur Mensch",
              "Jean-Baptiste Lespiau",
              "Maria Tsimpoukelli",
              "Nikolai Grigorev",
              "Doug Fritz",
              "Thibault Sottiaux",
              "Mantas Pajarskas",
              "Toby Pohlen",
              "Zhitao Gong",
              "Daniel Toyama",
              "Cyprien de Masson d'Autume",
              "Yujia Li",
              "Tayfun Terzi",
              "Vladimir Mikulik",
              "Igor Babuschkin",
              "Aidan Clark",
              "Diego de Las Casas",
              "Aurelia Guy",
              "Chris Jones",
              "James Bradbury",
              "Matthew Johnson",
              "Blake Hechtman",
              "Laura Weidinger",
              "Iason Gabriel",
              "William Isaac",
              "Ed Lockhart",
              "Simon Osindero",
              "Laura Rimell",
              "Chris Dyer",
              "Oriol Vinyals",
              "Kareem Ayoub",
              "Jeff Stanway",
              "Lorrayne Bennett",
              "Demis Hassabis",
              "Koray Kavukcuoglu",
              "Geoffrey Irving"
            ],
            "organization": "DeepMind",
            "date": "2021-12-08T19:41:47Z",
            "url": "https://arxiv.org/pdf/2112.11446.pdf",
            "description": "Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",
            "notes": "280B parameters\nData: 300B tokens"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 334,
          "function_name": "gopher_massivetext",
          "code": "text(\"The Gopher model is subsumed by Chinchilla (also never released), but the description of data is good\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The Gopher model is subsumed by Chinchilla (also never released), but the description of data is good",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 336,
          "function_name": "gopher_massivetext",
          "code": "text(\"Components\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Components",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 337,
          "function_name": "gopher_massivetext",
          "code": "text(\"- MassiveWeb: more on this later\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- MassiveWeb: more on this later",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 338,
          "function_name": "gopher_massivetext",
          "code": "text(\"- C4\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- C4",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 339,
          "function_name": "gopher_massivetext",
          "code": "text(\"- Books: no details\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Books: no details",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 340,
          "function_name": "gopher_massivetext",
          "code": "text(\"- News: no details\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- News: no details",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 341,
          "function_name": "gopher_massivetext",
          "code": "text(\"- GitHub: no details\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- GitHub: no details",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 342,
          "function_name": "gopher_massivetext",
          "code": "text(\"- Wikipedia: no details\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Wikipedia: no details",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 344,
          "function_name": "gopher_massivetext",
          "code": "text(\"MassiveWeb filtering steps\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "MassiveWeb filtering steps",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 345,
          "function_name": "gopher_massivetext",
          "code": "text(\"- Keep English, deduplication, train-test overlap\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Keep English, deduplication, train-test overlap",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 346,
          "function_name": "gopher_massivetext",
          "code": "text(\"- Quality filtering using manual rules (not classifier) - e.g., 80% words contain at least one alphabetic character\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Quality filtering using manual rules (not classifier) - e.g., 80% words contain at least one alphabetic character",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 347,
          "function_name": "gopher_massivetext",
          "code": "text(\"- Use Google SafeSearch for toxicity (not word lists)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use Google SafeSearch for toxicity (not word lists)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 349,
          "function_name": "gopher_massivetext",
          "code": "text(\"Result: 10.5 TB of text (though Gopher only trained on 300B tokens - 12%)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Result: 10.5 TB of text (though Gopher only trained on 300B tokens - 12%)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 22,
          "function_name": "main",
          "code": "gopher_massivetext()  # Filter using rules (trained Gopher) [2021]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 352,
          "function_name": "llama",
          "code": "def llama():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 353,
          "function_name": "llama",
          "code": "text(\"Dataset for LLaMA \"), link(\"https://arxiv.org/pdf/2302.13971\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Dataset for LLaMA ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "LLaMA: Open and Efficient Foundation Language Models",
            "authors": [
              "Hugo Touvron",
              "Thibaut Lavril",
              "Gautier Izacard",
              "Xavier Martinet",
              "Marie-Anne Lachaux",
              "Timoth\u00e9e Lacroix",
              "Baptiste Rozi\u00e8re",
              "Naman Goyal",
              "Eric Hambro",
              "Faisal Azhar",
              "Aurelien Rodriguez",
              "Armand Joulin",
              "Edouard Grave",
              "Guillaume Lample"
            ],
            "organization": null,
            "date": "2023-02-27T17:11:15Z",
            "url": "https://arxiv.org/pdf/2302.13971",
            "description": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 354,
          "function_name": "llama",
          "code": "text(\"- CommonCrawl processed with CCNet, classify *references* of Wikipedia or not\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- CommonCrawl processed with CCNet, classify *references* of Wikipedia or not",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 355,
          "function_name": "llama",
          "code": "text(\"- C4 (more diverse; recall: rule-based filtering)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- C4 (more diverse; recall: rule-based filtering)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 356,
          "function_name": "llama",
          "code": "text(\"- GitHub: kept permissive licenses, filtering based on manual rules\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- GitHub: kept permissive licenses, filtering based on manual rules",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 357,
          "function_name": "llama",
          "code": "text(\"- Wikipedia: June-August 2022, 20 languages, manual filtering\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Wikipedia: June-August 2022, 20 languages, manual filtering",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 358,
          "function_name": "llama",
          "code": "text(\"- Project Gutenberg and Books3 (from The Pile)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Project Gutenberg and Books3 (from The Pile)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 359,
          "function_name": "llama",
          "code": "text(\"- arXiv: removed comments, inline expanded macros, bibliography\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- arXiv: removed comments, inline expanded macros, bibliography",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 360,
          "function_name": "llama",
          "code": "text(\"- Stack Exchange: 28 largest websites, sorted answers by score\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Stack Exchange: 28 largest websites, sorted answers by score",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 361,
          "function_name": "llama",
          "code": "text(\"Result: 1.2T tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Result: 1.2T tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 363,
          "function_name": "llama",
          "code": "text(\"Reproduced by Together's RedPajama v1 \"), link(\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reproduced by Together's RedPajama v1 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 364,
          "function_name": "llama",
          "code": "text(\"Cerebras's [SlimPajama](https://www.cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama): 627B subset of RedPajama v1 by deduplication (MinHashLSH)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Cerebras's [SlimPajama](https://www.cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama): 627B subset of RedPajama v1 by deduplication (MinHashLSH)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 366,
          "function_name": "llama",
          "code": "text(\"Unrelated: RedPajama v2 has 30T tokens based on took 84 CommonCrawl snapshots, minimal filtering, lots of quality signals \"), article_link(\"https://github.com/togethercomputer/RedPajama-Data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Unrelated: RedPajama v2 has 30T tokens based on took 84 CommonCrawl snapshots, minimal filtering, lots of quality signals ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/togethercomputer/RedPajama-Data",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 23,
          "function_name": "main",
          "code": "llama()               # CommonCrawl, CCNet, StackExchange, etc. (trained LLaMA) [2022]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 369,
          "function_name": "refinedweb",
          "code": "def refinedweb():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 370,
          "function_name": "refinedweb",
          "code": "text(\"RefinedWeb \"), link(\"https://arxiv.org/pdf/2306.01116\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "RefinedWeb ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
            "authors": [
              "Guilherme Penedo",
              "Quentin Malartic",
              "Daniel Hesslow",
              "Ruxandra Cojocaru",
              "Alessandro Cappelli",
              "Hamza Alobeidli",
              "Baptiste Pannier",
              "Ebtesam Almazrouei",
              "Julien Launay"
            ],
            "organization": null,
            "date": "2023-06-01T20:03:56Z",
            "url": "https://arxiv.org/pdf/2306.01116",
            "description": "Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 371,
          "function_name": "refinedweb",
          "code": "text(\"- Point: web data is all you need\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Point: web data is all you need",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 372,
          "function_name": "refinedweb",
          "code": "text(\"- [Examples](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [Examples](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 373,
          "function_name": "refinedweb",
          "code": "text(\"- trafilatura for HTML->text, extract content (WARC instead of WET files)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- trafilatura for HTML->text, extract content (WARC instead of WET files)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 374,
          "function_name": "refinedweb",
          "code": "text(\"- Filtering: Gopher rules, avoid ML-based filtering to avoid biases\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Filtering: Gopher rules, avoid ML-based filtering to avoid biases",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 375,
          "function_name": "refinedweb",
          "code": "text(\"- Fuzzy deduplication using MinHash over 5-grams\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fuzzy deduplication using MinHash over 5-grams",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 376,
          "function_name": "refinedweb",
          "code": "text(\"Release 600B (out of 5T) tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Release 600B (out of 5T) tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 378,
          "function_name": "refinedweb",
          "code": "text(\"FineWeb \"), article_link(\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "FineWeb ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 379,
          "function_name": "refinedweb",
          "code": "text(\"- Started as a replication of RefinedWeb, but improved it\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Started as a replication of RefinedWeb, but improved it",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 380,
          "function_name": "refinedweb",
          "code": "text(\"- 95 Common Crawl dumps\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 95 Common Crawl dumps",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 381,
          "function_name": "refinedweb",
          "code": "text(\"- URL filtering, language ID (keep if p(en) > 0.65)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- URL filtering, language ID (keep if p(en) > 0.65)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 382,
          "function_name": "refinedweb",
          "code": "text(\"- Filtering: Gopher, C4, more manual rules\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Filtering: Gopher, C4, more manual rules",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 383,
          "function_name": "refinedweb",
          "code": "text(\"- Fuzzy deduplication via MinHash\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fuzzy deduplication via MinHash",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 384,
          "function_name": "refinedweb",
          "code": "text(\"- Anonymize email and public IP addresses (PII)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Anonymize email and public IP addresses (PII)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 385,
          "function_name": "refinedweb",
          "code": "text(\"Result: 15T tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Result: 15T tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 24,
          "function_name": "main",
          "code": "refinedweb()          # CommonCrawl (used to train Falcon) [2023]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 388,
          "function_name": "dolma",
          "code": "def dolma():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 389,
          "function_name": "dolma",
          "code": "text(\"Dolma \"), link(\"https://arxiv.org/pdf/2402.00159\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Dolma ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
            "authors": [
              "Luca Soldaini",
              "Rodney Kinney",
              "Akshita Bhagia",
              "Dustin Schwenk",
              "David Atkinson",
              "Russell Authur",
              "Ben Bogin",
              "Khyathi Chandu",
              "Jennifer Dumas",
              "Yanai Elazar",
              "Valentin Hofmann",
              "Ananya Harsh Jha",
              "Sachin Kumar",
              "Li Lucy",
              "Xinxi Lyu",
              "Nathan Lambert",
              "Ian Magnusson",
              "Jacob Morrison",
              "Niklas Muennighoff",
              "Aakanksha Naik",
              "Crystal Nam",
              "Matthew E. Peters",
              "Abhilasha Ravichander",
              "Kyle Richardson",
              "Zejiang Shen",
              "Emma Strubell",
              "Nishant Subramani",
              "Oyvind Tafjord",
              "Pete Walsh",
              "Luke Zettlemoyer",
              "Noah A. Smith",
              "Hannaneh Hajishirzi",
              "Iz Beltagy",
              "Dirk Groeneveld",
              "Jesse Dodge",
              "Kyle Lo"
            ],
            "organization": null,
            "date": "2024-01-31T20:29:50Z",
            "url": "https://arxiv.org/pdf/2402.00159",
            "description": "Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 390,
          "function_name": "dolma",
          "code": "image(\"https://miro.medium.com/v2/resize:fit:1400/1*-0Qqhvu7JD6Y9JgsfKJdxw.png\", width=700)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-47601eaf24df2c497082e9c528606b87-https_miro_medium_com_v2_resize_fit_1400_1_-0Qqhvu7JD6Y9JgsfKJdxw_png",
          "style": {
            "width": 700
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 392,
          "function_name": "dolma",
          "code": "text(\"- Reddit: from the Pushshift project (2005-2023), include submissions and comments separately\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Reddit: from the Pushshift project (2005-2023), include submissions and comments separately",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 393,
          "function_name": "dolma",
          "code": "text(\"- PeS2o: 40M academic papers from Semantic Scholar\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- PeS2o: 40M academic papers from Semantic Scholar",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 394,
          "function_name": "dolma",
          "code": "text(\"- C4, Project Gutenberg, Wikipedia/Wikibooks\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- C4, Project Gutenberg, Wikipedia/Wikibooks",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 396,
          "function_name": "dolma",
          "code": "text(\"Common Crawl processing\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Common Crawl processing",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 397,
          "function_name": "dolma",
          "code": "text(\"- Language identification (fastText classifier), keep English\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Language identification (fastText classifier), keep English",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 398,
          "function_name": "dolma",
          "code": "text(\"- Quality filtering (Gopher, C4 rules), avoid model-based filtering\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Quality filtering (Gopher, C4 rules), avoid model-based filtering",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 399,
          "function_name": "dolma",
          "code": "text(\"- Toxicity filtering using rules and Jigsaw classifier\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Toxicity filtering using rules and Jigsaw classifier",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 400,
          "function_name": "dolma",
          "code": "text(\"- Deduplication using Bloom filters\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Deduplication using Bloom filters",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 402,
          "function_name": "dolma",
          "code": "text(\"Result: 3T tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Result: 3T tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 25,
          "function_name": "main",
          "code": "dolma()               # Lots of different sources [2024]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 404,
          "function_name": "dclm",
          "code": "def dclm():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 405,
          "function_name": "dclm",
          "code": "text(\"DataComp-LM \"), link(dclm_2024)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "DataComp-LM ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DataComp-LM: In search of the next generation of training sets for language models",
            "authors": [
              "Jeffrey Li",
              "Alex Fang",
              "Georgios Smyrnis",
              "Maor Ivgi",
              "Matt Jordan",
              "Samir Gadre",
              "Hritik Bansal",
              "Etash Guha",
              "Sedrick Keh",
              "Kushal Arora",
              "Saurabh Garg",
              "Rui Xin",
              "Niklas Muennighoff",
              "Reinhard Heckel",
              "Jean Mercat",
              "Mayee Chen",
              "Suchin Gururangan",
              "Mitchell Wortsman",
              "Alon Albalak",
              "Yonatan Bitton",
              "Marianna Nezhurina",
              "Amro Abbas",
              "Cheng-Yu Hsieh",
              "Dhruba Ghosh",
              "Josh Gardner",
              "Maciej Kilian",
              "Hanlin Zhang",
              "Rulin Shao",
              "Sarah Pratt",
              "Sunny Sanyal",
              "Gabriel Ilharco",
              "Giannis Daras",
              "Kalyani Marathe",
              "Aaron Gokaslan",
              "Jieyu Zhang",
              "Khyathi Chandu",
              "Thao Nguyen",
              "Igor Vasiljevic",
              "Sham Kakade",
              "Shuran Song",
              "Sujay Sanghavi",
              "Fartash Faghri",
              "Sewoong Oh",
              "Luke Zettlemoyer",
              "Kyle Lo",
              "Alaaeldin El-Nouby",
              "Hadi Pouransari",
              "Alexander Toshev",
              "Stephanie Wang",
              "Dirk Groeneveld",
              "Luca Soldaini",
              "Pang Wei Koh",
              "Jenia Jitsev",
              "Thomas Kollar",
              "Alexandros G. Dimakis",
              "Yair Carmon",
              "Achal Dave",
              "Ludwig Schmidt",
              "Vaishaal Shankar"
            ],
            "organization": null,
            "date": "2024-06-17T17:42:57Z",
            "url": "https://arxiv.org/abs/2406.11794",
            "description": "We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 406,
          "function_name": "dclm",
          "code": "text(\"- Goal: define a standard dataset for trying out different data processing algorithms\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Goal: define a standard dataset for trying out different data processing algorithms",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 407,
          "function_name": "dclm",
          "code": "text(\"- Processed CommonCrawl to produce DCLM-pool (240T tokens)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Processed CommonCrawl to produce DCLM-pool (240T tokens)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 408,
          "function_name": "dclm",
          "code": "text(\"- DCLM-baseline: filtered down DCLM-pool using quality classifier\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DCLM-baseline: filtered down DCLM-pool using quality classifier",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 409,
          "function_name": "dclm",
          "code": "image(\"images/dclm-filter.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/dclm-filter.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 411,
          "function_name": "dclm",
          "code": "text(\"### Model-based filtering\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Model-based filtering",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 412,
          "function_name": "dclm",
          "code": "text(\"Positive examples (200K):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Positive examples (200K):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 413,
          "function_name": "dclm",
          "code": "text(\"- [OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5): mostly GPT-4 generated instruction data ([examples](https://huggingface.co/datasets/teknium/OpenHermes-2.5/viewer/default/train))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5): mostly GPT-4 generated instruction data ([examples](https://huggingface.co/datasets/teknium/OpenHermes-2.5/viewer/default/train))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 414,
          "function_name": "dclm",
          "code": "text(\"- [ELI5](https://www.reddit.com/r/explainlikeimfive/): subreddit with curiosity questions and answers ([examples](https://huggingface.co/datasets/sentence-transformers/eli5/viewer/pair/train))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [ELI5](https://www.reddit.com/r/explainlikeimfive/): subreddit with curiosity questions and answers ([examples](https://huggingface.co/datasets/sentence-transformers/eli5/viewer/pair/train))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 415,
          "function_name": "dclm",
          "code": "text(\"Negative examples (200K):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Negative examples (200K):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 416,
          "function_name": "dclm",
          "code": "text(\"- [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 417,
          "function_name": "dclm",
          "code": "text(\"Result: 3.8T tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Result: 3.8T tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 419,
          "function_name": "dclm",
          "code": "text(\"Trained a fastText classifier, run it on all of DCLM-pool\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Trained a fastText classifier, run it on all of DCLM-pool",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 420,
          "function_name": "dclm",
          "code": "text(\"This quality classifier outperforms other filtering methods:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This quality classifier outperforms other filtering methods:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 421,
          "function_name": "dclm",
          "code": "image(\"images/dclm-quality.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/dclm-quality.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 26,
          "function_name": "main",
          "code": "dclm()                # Filtered using good quality classifier [2024]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 424,
          "function_name": "nemotron_cc",
          "code": "def nemotron_cc():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 425,
          "function_name": "nemotron_cc",
          "code": "text(\"Nemotron-CC \"), link(nemotron_cc_2024)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Nemotron-CC ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset",
            "authors": [
              "Dan Su",
              "Kezhi Kong",
              "Ying Lin",
              "Joseph Jennings",
              "Brandon Norick",
              "Markus Kliegl",
              "Mostofa Patwary",
              "Mohammad Shoeybi",
              "Bryan Catanzaro"
            ],
            "organization": null,
            "date": "2024-12-03T17:28:50Z",
            "url": "https://arxiv.org/abs/2412.02595",
            "description": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 426,
          "function_name": "nemotron_cc",
          "code": "text(\"- FineWebEdu and DCLM filter too aggressively (remove 90% of data)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- FineWebEdu and DCLM filter too aggressively (remove 90% of data)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 427,
          "function_name": "nemotron_cc",
          "code": "text(\"- Need moar tokens (but preserve quality)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Need moar tokens (but preserve quality)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 428,
          "function_name": "nemotron_cc",
          "code": "text(\"- For HTML -> text, used jusText (not trafilatura) because it returned more tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- For HTML -> text, used jusText (not trafilatura) because it returned more tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 430,
          "function_name": "nemotron_cc",
          "code": "text(\"Classifier ensembling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Classifier ensembling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 431,
          "function_name": "nemotron_cc",
          "code": "text(\"- Prompt Nemotron-340B-instruct to score FineWeb documents based on educational value, distill into faster model\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Prompt Nemotron-340B-instruct to score FineWeb documents based on educational value, distill into faster model",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 432,
          "function_name": "nemotron_cc",
          "code": "text(\"- DCLM classifier\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DCLM classifier",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 434,
          "function_name": "nemotron_cc",
          "code": "text(\"Synthetic data rephrasing\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Synthetic data rephrasing",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 435,
          "function_name": "nemotron_cc",
          "code": "text(\"- For high-quality data, use LM to rephrase low-quality data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- For high-quality data, use LM to rephrase low-quality data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 436,
          "function_name": "nemotron_cc",
          "code": "text(\"- For low-quality data, use LM to generate tasks (QA pairs, extract key information, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- For low-quality data, use LM to generate tasks (QA pairs, extract key information, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 438,
          "function_name": "nemotron_cc",
          "code": "text(\"Result: 6.3T tokens (HQ subset is 1.1T)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Result: 6.3T tokens (HQ subset is 1.1T)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 439,
          "function_name": "nemotron_cc",
          "code": "text(\"For reference, Llama 3 trained on 15T, Qwen3 trained on 36T\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "For reference, Llama 3 trained on 15T, Qwen3 trained on 36T",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        },
        {
          "path": "lecture_13.py",
          "line_number": 440,
          "function_name": "nemotron_cc",
          "code": "image(\"images/nemotron-results.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/nemotron-results.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 27,
          "function_name": "main",
          "code": "nemotron_cc()         # Lots of tokens [2024]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 443,
          "function_name": "copyright",
          "code": "def copyright():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 444,
          "function_name": "copyright",
          "code": "text(\"Lots of lawsuits around generative AI, mostly around copyright \"), article_link(\"https://www.bakerlaw.com/services/artificial-intelligence-ai/case-tracker-artificial-intelligence-copyrights-and-class-actions/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Lots of lawsuits around generative AI, mostly around copyright ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.bakerlaw.com/services/artificial-intelligence-ai/case-tracker-artificial-intelligence-copyrights-and-class-actions/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 446,
          "function_name": "copyright",
          "code": "text(\"### Intellectual property law\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Intellectual property law",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 447,
          "function_name": "copyright",
          "code": "text(\"- Goal: *incentivize* the creation of intellectual goods\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Goal: *incentivize* the creation of intellectual goods",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 448,
          "function_name": "copyright",
          "code": "text(\"- Types of intellectual property: copyright, patents, trademarks, trade secrets.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Types of intellectual property: copyright, patents, trademarks, trade secrets.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 450,
          "function_name": "copyright",
          "code": "text(\"### Copyright law\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Copyright law",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 451,
          "function_name": "copyright",
          "code": "text(\"- Goes back to 1709 in England (Statute of Anne), first time regulated by governments and courts \"), article_link(\"https://en.wikipedia.org/wiki/Statute_of_Anne\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Goes back to 1709 in England (Statute of Anne), first time regulated by governments and courts ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Statute_of_Anne",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 452,
          "function_name": "copyright",
          "code": "text(\"- In United States, most recent: Copyright Act of 1976 \"), article_link(\"https://en.wikipedia.org/wiki/Copyright_Act_of_1976\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- In United States, most recent: Copyright Act of 1976 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Copyright_Act_of_1976",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 453,
          "function_name": "copyright",
          "code": "text(\"- Copyright protection applies to 'original works of authorship fixed in any tangible medium of expression, now known or later developed, from which they can be perceived, reproduced, or otherwise communicated, either directly or with the aid of a machine or device'\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Copyright protection applies to 'original works of authorship fixed in any tangible medium of expression, now known or later developed, from which they can be perceived, reproduced, or otherwise communicated, either directly or with the aid of a machine or device'",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 455,
          "function_name": "copyright",
          "code": "text(\"- Original works, so collections not copyrightable (e.g., telephone directories) unless there is some creativity in the selection or arrangement\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Original works, so collections not copyrightable (e.g., telephone directories) unless there is some creativity in the selection or arrangement",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 456,
          "function_name": "copyright",
          "code": "text(\"- Copyright applies to expression, not ideas (e.g., quicksort)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Copyright applies to expression, not ideas (e.g., quicksort)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 458,
          "function_name": "copyright",
          "code": "text(\"- Expanded scope from 'published' (1909) to 'fixed' (1976)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Expanded scope from 'published' (1909) to 'fixed' (1976)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 459,
          "function_name": "copyright",
          "code": "text(\"- Registration not required for copyright protection (in contrast with patents)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Registration not required for copyright protection (in contrast with patents)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 460,
          "function_name": "copyright",
          "code": "text(\"- Threshold for copyright is extremely low (e.g., your website is copyrighted)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Threshold for copyright is extremely low (e.g., your website is copyrighted)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 462,
          "function_name": "copyright",
          "code": "text(\"- Registration is required before creator can sue someone for copyright infringement\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Registration is required before creator can sue someone for copyright infringement",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 463,
          "function_name": "copyright",
          "code": "text(\"- Costs $65 to register \"), article_link(\"https://www.copyright.gov/about/fees.html\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Costs $65 to register ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.copyright.gov/about/fees.html",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 464,
          "function_name": "copyright",
          "code": "text(\"- Lasts for 75 years, and then the copyright expires and it becomes part of the public domain (works of Shakespeare, Beethoven, most of Project Gutenberg, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Lasts for 75 years, and then the copyright expires and it becomes part of the public domain (works of Shakespeare, Beethoven, most of Project Gutenberg, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 466,
          "function_name": "copyright",
          "code": "text(\"Summary: most things on the Internet are actually copyrighted.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary: most things on the Internet are actually copyrighted.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 468,
          "function_name": "copyright",
          "code": "text(\"How to use a copyrighted work:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How to use a copyrighted work:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 469,
          "function_name": "copyright",
          "code": "text(\"1. Get a license for it.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Get a license for it.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 470,
          "function_name": "copyright",
          "code": "text(\"2. Appeal to the fair use clause.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Appeal to the fair use clause.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 472,
          "function_name": "copyright",
          "code": "text(\"## Licenses\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Licenses",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 473,
          "function_name": "copyright",
          "code": "text(\"- A license (from contract law) is granted by a licensor to a licensee.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- A license (from contract law) is granted by a licensor to a licensee.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 474,
          "function_name": "copyright",
          "code": "text(\"- Effectively, 'a license is a promise not to sue'.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Effectively, 'a license is a promise not to sue'.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 476,
          "function_name": "copyright",
          "code": "text(\"- The Creative Commons license enables free distribution of copyrighted work.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- The Creative Commons license enables free distribution of copyrighted work.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 477,
          "function_name": "copyright",
          "code": "text(\"- Examples: Wikipedia, Open Courseware, Khan Academy, Free Music Archive, 307 million images from Flickr, 39 million images from MusicBrainz, 10 million videos from YouTube, etc.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Examples: Wikipedia, Open Courseware, Khan Academy, Free Music Archive, 307 million images from Flickr, 39 million images from MusicBrainz, 10 million videos from YouTube, etc.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 478,
          "function_name": "copyright",
          "code": "text(\"- Created by Lessig and Eldred in 2001 to bridge public domain and existing copyright\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Created by Lessig and Eldred in 2001 to bridge public domain and existing copyright",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 480,
          "function_name": "copyright",
          "code": "text(\"Many model developers license data for training foundation models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Many model developers license data for training foundation models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 481,
          "function_name": "copyright",
          "code": "text(\"- Google and Reddit \"), article_link(\"https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Google and Reddit ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 482,
          "function_name": "copyright",
          "code": "text(\"- OpenAI and Shutterstock \"), article_link(\"https://investor.shutterstock.com/news-releases/news-release-details/shutterstock-expands-partnership-openai-signs-new-six-year\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- OpenAI and Shutterstock ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://investor.shutterstock.com/news-releases/news-release-details/shutterstock-expands-partnership-openai-signs-new-six-year",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 483,
          "function_name": "copyright",
          "code": "text(\"- OpenAI and StackExchange \"), article_link(\"https://stackoverflow.co/company/press/archive/openai-partnership\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- OpenAI and StackExchange ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://stackoverflow.co/company/press/archive/openai-partnership",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 485,
          "function_name": "copyright",
          "code": "text(\"## Fair use (section 107)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Fair use (section 107)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 486,
          "function_name": "copyright",
          "code": "text(\"Four factors to determine whether fair use applies:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Four factors to determine whether fair use applies:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 487,
          "function_name": "copyright",
          "code": "text(\"1. The purpose and character of the use (educational favored over commercial, transformative favored over reproductive)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. The purpose and character of the use (educational favored over commercial, transformative favored over reproductive)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 488,
          "function_name": "copyright",
          "code": "text(\"2. The nature of the copyrighted work (factual favored over fictional, non-creative over creative)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. The nature of the copyrighted work (factual favored over fictional, non-creative over creative)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 489,
          "function_name": "copyright",
          "code": "text(\"3. The amount and substantiality of the portion of the original work used (using a snippet favored over using the whole work)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. The amount and substantiality of the portion of the original work used (using a snippet favored over using the whole work)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 490,
          "function_name": "copyright",
          "code": "text(\"4. The effect of the use upon the market (or potential market) for the original work\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "4. The effect of the use upon the market (or potential market) for the original work",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 492,
          "function_name": "copyright",
          "code": "text(\"Examples of fair use:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Examples of fair use:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 493,
          "function_name": "copyright",
          "code": "text(\"- You watch a movie and write a summary of it\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- You watch a movie and write a summary of it",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 494,
          "function_name": "copyright",
          "code": "text(\"- Reimplement an algorithm (the idea) rather than copying the code (the expression)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Reimplement an algorithm (the idea) rather than copying the code (the expression)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 495,
          "function_name": "copyright",
          "code": "text(\"- Google Books index and show snippets (Authors Guild v. Google 2002-2013)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Google Books index and show snippets (Authors Guild v. Google 2002-2013)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 497,
          "function_name": "copyright",
          "code": "text(\"Copyright is not about verbatim memorization\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Copyright is not about verbatim memorization",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 498,
          "function_name": "copyright",
          "code": "text(\"- Plots and characters (e.g., Harry Potter) can be copyrightable\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Plots and characters (e.g., Harry Potter) can be copyrightable",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 499,
          "function_name": "copyright",
          "code": "text(\"- Parody is likely fair use\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Parody is likely fair use",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 500,
          "function_name": "copyright",
          "code": "text(\"Copyright is about semantics (and economics)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Copyright is about semantics (and economics)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 502,
          "function_name": "copyright",
          "code": "text(\"Considerations for foundation models:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Considerations for foundation models:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 503,
          "function_name": "copyright",
          "code": "text(\"- Copying data (first step of training) is violation already even if you don't do anything with it.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Copying data (first step of training) is violation already even if you don't do anything with it.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 504,
          "function_name": "copyright",
          "code": "text(\"- Training an ML model is transformative (far from just copy/pasting)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Training an ML model is transformative (far from just copy/pasting)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 505,
          "function_name": "copyright",
          "code": "text(\"- ML system is interested in idea (e.g., stop sign), not in the concrete expression (e.g., exact artistic choices of a particular image of a stop sign).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- ML system is interested in idea (e.g., stop sign), not in the concrete expression (e.g., exact artistic choices of a particular image of a stop sign).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 506,
          "function_name": "copyright",
          "code": "text(\"Problem: language models can definitely affect the market (writers, artists), regardless of copyright\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem: language models can definitely affect the market (writers, artists), regardless of copyright",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 508,
          "function_name": "copyright",
          "code": "text(\"## Terms of service\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Terms of service",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 509,
          "function_name": "copyright",
          "code": "text(\"- Even if you have a license or can appeal to fair use for a work, terms of service might impose additional restrictions.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Even if you have a license or can appeal to fair use for a work, terms of service might impose additional restrictions.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 510,
          "function_name": "copyright",
          "code": "text(\"- Example: YouTube's terms of service prohibits downloading videos, even if the videos are licensed under Creative Commons.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Example: YouTube's terms of service prohibits downloading videos, even if the videos are licensed under Creative Commons.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 512,
          "function_name": "copyright",
          "code": "text(\"Further reading:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Further reading:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 513,
          "function_name": "copyright",
          "code": "text(\"- [CS324 course notes](https://stanford-cs324.github.io/winter2022/lectures/legality/)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [CS324 course notes](https://stanford-cs324.github.io/winter2022/lectures/legality/)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 514,
          "function_name": "copyright",
          "code": "text(\"- Fair learning [[Lemley & Casey](https://texaslawreview.org/fair-learning/)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fair learning [[Lemley & Casey](https://texaslawreview.org/fair-learning/)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 515,
          "function_name": "copyright",
          "code": "text(\"- Foundation models and fair use \"), link(\"https://arxiv.org/pdf/2303.15715\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Foundation models and fair use ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Foundation Models and Fair Use",
            "authors": [
              "Peter Henderson",
              "Xuechen Li",
              "Dan Jurafsky",
              "Tatsunori Hashimoto",
              "Mark A. Lemley",
              "Percy Liang"
            ],
            "organization": null,
            "date": "2023-03-28T03:58:40Z",
            "url": "https://arxiv.org/pdf/2303.15715",
            "description": "Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant U.S. case law, drawing parallels to existing and potential applications for generating text, source code, and visual art. Experiments confirm that popular foundation models can generate content considerably similar to copyrighted material. Second, we discuss technical mitigations that can help foundation models stay in line with fair use. We argue that more research is needed to align mitigation strategies with the current state of the law. Lastly, we suggest that the law and technical mitigations should co-evolve. For example, coupled with other policy mechanisms, the law could more explicitly consider safe harbors when strong technical tools are used to mitigate infringement harms. This co-evolution may help strike a balance between intellectual property and innovation, which speaks to the original goal of fair use. But we emphasize that the strategies we describe here are not a panacea and more work is needed to develop policies that address the potential harms of foundation models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        },
        {
          "path": "lecture_13.py",
          "line_number": 516,
          "function_name": "copyright",
          "code": "text(\"- The Files are in the Computer \"), link(\"https://arxiv.org/abs/2404.12590\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- The Files are in the Computer ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Files are in the Computer: Copyright, Memorization, and Generative AI",
            "authors": [
              "A. Feder Cooper",
              "James Grimmelmann"
            ],
            "organization": null,
            "date": "2024-04-19T02:37:09Z",
            "url": "https://arxiv.org/abs/2404.12590",
            "description": "The New York Times's copyright lawsuit against OpenAI and Microsoft alleges OpenAI's GPT models have \"memorized\" NYT articles. Other lawsuits make similar claims. But parties, courts, and scholars disagree on what memorization is, whether it is taking place, and what its copyright implications are. These debates are clouded by ambiguities over the nature of \"memorization.\" We attempt to bring clarity to the conversation. We draw on the technical literature to provide a firm foundation for legal discussions, providing a precise definition of memorization: a model has \"memorized\" a piece of training data when (1) it is possible to reconstruct from the model (2) a near-exact copy of (3) a substantial portion of (4) that piece of training data. We distinguish memorization from \"extraction\" (user intentionally causes a model to generate a near-exact copy), from \"regurgitation\" (model generates a near-exact copy, regardless of user intentions), and from \"reconstruction\" (the near-exact copy can be obtained from the model by any means). Several consequences follow. (1) Not all learning is memorization. (2) Memorization occurs when a model is trained; regurgitation is a symptom not its cause. (3) A model that has memorized training data is a \"copy\" of that training data in the sense used by copyright. (4) A model is not like a VCR or other general-purpose copying technology; it is better at generating some types of outputs (possibly regurgitated ones) than others. (5) Memorization is not a phenomenon caused by \"adversarial\" users bent on extraction; it is latent in the model itself. (6) The amount of training data that a model memorizes is a consequence of choices made in training. (7) Whether or not a model that has memorized actually regurgitates depends on overall system design. In a very real sense, memorized training data is in the model--to quote Zoolander, the files are in the computer.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 29,
          "function_name": "main",
          "code": "copyright()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 31,
          "function_name": "main",
          "code": "text(\"### Mid-training + post-training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Mid-training + post-training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 32,
          "function_name": "main",
          "code": "text(\"Let's focus on particular capabilities.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's focus on particular capabilities.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 519,
          "function_name": "long_context",
          "code": "def long_context():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 520,
          "function_name": "long_context",
          "code": "text(\"Demand for long contexts (want to do QA on books)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Demand for long contexts (want to do QA on books)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 521,
          "function_name": "long_context",
          "code": "text(\"- DeepSeek v3 has 128K tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DeepSeek v3 has 128K tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 522,
          "function_name": "long_context",
          "code": "text(\"- Claude 3.5 Sonnet has 200K tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Claude 3.5 Sonnet has 200K tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 523,
          "function_name": "long_context",
          "code": "text(\"- Gemini 1.5 Pro has 1.5M tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Gemini 1.5 Pro has 1.5M tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 525,
          "function_name": "long_context",
          "code": "text(\"Transformers scales quadratically with sequence length\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Transformers scales quadratically with sequence length",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 526,
          "function_name": "long_context",
          "code": "text(\"Not efficient to pre-train on long contexts, want to add long context later\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Not efficient to pre-train on long contexts, want to add long context later",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 528,
          "function_name": "long_context",
          "code": "text(\"LongLoRA \"), link(\"https://arxiv.org/pdf/2309.12307\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "LongLoRA ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
            "authors": [
              "Yukang Chen",
              "Shengju Qian",
              "Haotian Tang",
              "Xin Lai",
              "Zhijian Liu",
              "Song Han",
              "Jiaya Jia"
            ],
            "organization": null,
            "date": "2023-09-21T17:59:11Z",
            "url": "https://arxiv.org/pdf/2309.12307",
            "description": "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 529,
          "function_name": "long_context",
          "code": "text(\"- Extends context length of Llama2 7B from 4K to 100K tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Extends context length of Llama2 7B from 4K to 100K tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 530,
          "function_name": "long_context",
          "code": "text(\"- Use shifted sparse attention (Figure 2), positional interpolation [Chen+ 2023]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use shifted sparse attention (Figure 2), positional interpolation [Chen+ 2023]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        },
        {
          "path": "lecture_13.py",
          "line_number": 531,
          "function_name": "long_context",
          "code": "text(\"- Trained on long documents: PG-19 (books) and Proof-Pile (math)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Trained on long documents: PG-19 (books) and Proof-Pile (math)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 33,
          "function_name": "main",
          "code": "long_context()        # Long context"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 534,
          "function_name": "tasks",
          "code": "def tasks():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 535,
          "function_name": "tasks",
          "code": "text(\"TL;DR: convert lots of existing NLP datasets into prompts\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "TL;DR: convert lots of existing NLP datasets into prompts",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 537,
          "function_name": "tasks",
          "code": "text(\"Super-Natural Instructions \"), link(\"https://arxiv.org/pdf/2204.07705\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Super-Natural Instructions ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
            "authors": [
              "Yizhong Wang",
              "Swaroop Mishra",
              "Pegah Alipoormolabashi",
              "Yeganeh Kordi",
              "Amirreza Mirzaei",
              "Anjana Arunkumar",
              "Arjun Ashok",
              "Arut Selvan Dhanasekaran",
              "Atharva Naik",
              "David Stap",
              "Eshaan Pathak",
              "Giannis Karamanolakis",
              "Haizhi Gary Lai",
              "Ishan Purohit",
              "Ishani Mondal",
              "Jacob Anderson",
              "Kirby Kuznia",
              "Krima Doshi",
              "Maitreya Patel",
              "Kuntal Kumar Pal",
              "Mehrad Moradshahi",
              "Mihir Parmar",
              "Mirali Purohit",
              "Neeraj Varshney",
              "Phani Rohitha Kaza",
              "Pulkit Verma",
              "Ravsehaj Singh Puri",
              "Rushang Karia",
              "Shailaja Keyur Sampat",
              "Savan Doshi",
              "Siddhartha Mishra",
              "Sujan Reddy",
              "Sumanta Patro",
              "Tanay Dixit",
              "Xudong Shen",
              "Chitta Baral",
              "Yejin Choi",
              "Noah A. Smith",
              "Hannaneh Hajishirzi",
              "Daniel Khashabi"
            ],
            "organization": null,
            "date": "2022-04-16T03:12:30Z",
            "url": "https://arxiv.org/pdf/2204.07705",
            "description": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 538,
          "function_name": "tasks",
          "code": "text(\"- Dataset: 1.6K+ tasks (Figure 2)\"), named_link(\"dataset\", \"https://huggingface.co/datasets/Muennighoff/natural-instructions\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Dataset: 1.6K+ tasks (Figure 2)",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [dataset]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/datasets/Muennighoff/natural-instructions",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 539,
          "function_name": "tasks",
          "code": "text(\"- Fine-tune T5 on k-shot learning (Tk-instruct)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tune T5 on k-shot learning (Tk-instruct)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 540,
          "function_name": "tasks",
          "code": "text(\"- Tasks contributed by community (via GitHub)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tasks contributed by community (via GitHub)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 541,
          "function_name": "tasks",
          "code": "text(\"- Examples for each task are derived from existing datasets and converted into templatized prompts\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Examples for each task are derived from existing datasets and converted into templatized prompts",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 542,
          "function_name": "tasks",
          "code": "text(\"- Outperforms InstructGPT despite being much smaller(?)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Outperforms InstructGPT despite being much smaller(?)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 544,
          "function_name": "tasks",
          "code": "text(\"Flan 2022 \"), link(\"https://arxiv.org/pdf/2301.13688\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Flan 2022 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
            "authors": [
              "Shayne Longpre",
              "Le Hou",
              "Tu Vu",
              "Albert Webson",
              "Hyung Won Chung",
              "Yi Tay",
              "Denny Zhou",
              "Quoc V. Le",
              "Barret Zoph",
              "Jason Wei",
              "Adam Roberts"
            ],
            "organization": null,
            "date": "2023-01-31T15:03:44Z",
            "url": "https://arxiv.org/pdf/2301.13688",
            "description": "We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 545,
          "function_name": "tasks",
          "code": "text(\"- Dataset: 1.8K+ tasks \"), named_link(\"dataset\", \"https://huggingface.co/datasets/Muennighoff/flan\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Dataset: 1.8K+ tasks ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [dataset]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/datasets/Muennighoff/flan",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        },
        {
          "path": "lecture_13.py",
          "line_number": 546,
          "function_name": "tasks",
          "code": "text(\"- Fine-tune T5 on zero-shot, few-shot, chain-of-thought versions of the dataset (Figure 7)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tune T5 on zero-shot, few-shot, chain-of-thought versions of the dataset (Figure 7)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 34,
          "function_name": "main",
          "code": "tasks()               # Tasks based on standard datasets"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 549,
          "function_name": "instruction_chat",
          "code": "def instruction_chat():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 550,
          "function_name": "instruction_chat",
          "code": "text(\"TL;DR: more open-ended instructions, heavy use of synthetic data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "TL;DR: more open-ended instructions, heavy use of synthetic data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 552,
          "function_name": "instruction_chat",
          "code": "text(\"Alpaca \"), link(alpaca)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Alpaca ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Alpaca",
            "authors": [
              "Rohan Taori",
              "Ishaan Gulrajani",
              "Tianyi Zhang",
              "Yann Dubois",
              "Xuechen Li",
              "Carlos Guestrin",
              "Percy Liang",
              "Tatsunori B. Hashimoto"
            ],
            "organization": null,
            "date": "2023-03-13",
            "url": "https://crfm.stanford.edu/2023/03/13/alpaca.html",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 553,
          "function_name": "instruction_chat",
          "code": "text(\"- Dataset of 52K examples from text-davinci-003 using self-instruct \"), link(\"https://arxiv.org/pdf/2212.10560\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Dataset of 52K examples from text-davinci-003 using self-instruct ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
            "authors": [
              "Yizhong Wang",
              "Yeganeh Kordi",
              "Swaroop Mishra",
              "Alisa Liu",
              "Noah A. Smith",
              "Daniel Khashabi",
              "Hannaneh Hajishirzi"
            ],
            "organization": null,
            "date": "2022-12-20T18:59:19Z",
            "url": "https://arxiv.org/pdf/2212.10560",
            "description": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 554,
          "function_name": "instruction_chat",
          "code": "text(\"- Fine-tune LLaMA 7B on this dataset\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tune LLaMA 7B on this dataset",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 556,
          "function_name": "instruction_chat",
          "code": "text(\"Vicuna \"), article_link(\"https://lmsys.org/blog/2023-03-30-vicuna/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Vicuna ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://lmsys.org/blog/2023-03-30-vicuna/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 557,
          "function_name": "instruction_chat",
          "code": "text(\"- Fine-tuned LLaMA on 70K conversations from [ShareGPT](https://sharegpt.com/) (users sharing their ChatGPT conversations; deprecated now)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tuned LLaMA on 70K conversations from [ShareGPT](https://sharegpt.com/) (users sharing their ChatGPT conversations; deprecated now)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 559,
          "function_name": "instruction_chat",
          "code": "text(\"Baize \"), link(\"https://arxiv.org/pdf/2304.01196\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Baize ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
            "authors": [
              "Canwen Xu",
              "Daya Guo",
              "Nan Duan",
              "Julian McAuley"
            ],
            "organization": null,
            "date": "2023-04-03T17:59:09Z",
            "url": "https://arxiv.org/pdf/2304.01196",
            "description": "Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. An online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 560,
          "function_name": "instruction_chat",
          "code": "text(\"- Generate dataset (111.5K examples) from GPT-3.5 using self-chat (seeded with Quora and StackOverflow questions)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Generate dataset (111.5K examples) from GPT-3.5 using self-chat (seeded with Quora and StackOverflow questions)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 561,
          "function_name": "instruction_chat",
          "code": "text(\"- Fine-tuned LLaMA on this dataset\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tuned LLaMA on this dataset",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 563,
          "function_name": "instruction_chat",
          "code": "text(\"WizardLM \"), link(\"https://arxiv.org/pdf/2304.12244\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "WizardLM ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
            "authors": [
              "Can Xu",
              "Qingfeng Sun",
              "Kai Zheng",
              "Xiubo Geng",
              "Pu Zhao",
              "Jiazhan Feng",
              "Chongyang Tao",
              "Daxin Jiang"
            ],
            "organization": null,
            "date": "2023-04-24T16:31:06Z",
            "url": "https://arxiv.org/pdf/2304.12244",
            "description": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 564,
          "function_name": "instruction_chat",
          "code": "text(\"- Evol-Instruct dataset ('evolve' questions to increase breadth/difficulty) (Figure 1)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Evol-Instruct dataset ('evolve' questions to increase breadth/difficulty) (Figure 1)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 565,
          "function_name": "instruction_chat",
          "code": "text(\"- Fine-tuned LLaMA on this dataset\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tuned LLaMA on this dataset",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 567,
          "function_name": "instruction_chat",
          "code": "text(\"MAmmoTH2 \"), link(\"https://arxiv.org/pdf/2405.03548\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "MAmmoTH2 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MAmmoTH2: Scaling Instructions from the Web",
            "authors": [
              "Xiang Yue",
              "Tuney Zheng",
              "Ge Zhang",
              "Wenhu Chen"
            ],
            "organization": null,
            "date": "2024-05-06T15:11:38Z",
            "url": "https://arxiv.org/pdf/2405.03548",
            "description": "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 568,
          "function_name": "instruction_chat",
          "code": "text(\"- Curated WebInstruct, 10M instructions from Common Crawl\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Curated WebInstruct, 10M instructions from Common Crawl",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 569,
          "function_name": "instruction_chat",
          "code": "text(\"- Filter: train fastText classifier on quiz sites\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Filter: train fastText classifier on quiz sites",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 570,
          "function_name": "instruction_chat",
          "code": "text(\"- Extract: use GPT-4 and Mixtral to extract QA pairs\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Extract: use GPT-4 and Mixtral to extract QA pairs",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 571,
          "function_name": "instruction_chat",
          "code": "text(\"- Fine-tune Mistral 7B on this data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tune Mistral 7B on this data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 572,
          "function_name": "instruction_chat",
          "code": "text(\"- Boosts math performance\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Boosts math performance",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 574,
          "function_name": "instruction_chat",
          "code": "text(\"OpenHermes 2.5\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "OpenHermes 2.5",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 575,
          "function_name": "instruction_chat",
          "code": "text(\"- Agglomeration of many datasets \"), named_link(\"dataset\", \"https://huggingface.co/datasets/teknium/openhermes\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Agglomeration of many datasets ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [dataset]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/datasets/teknium/openhermes",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 576,
          "function_name": "instruction_chat",
          "code": "text(\"- Fine-tune Mistral 7B on 1M examples from GPT-4 \"), named_link(\"model\", \"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fine-tune Mistral 7B on 1M examples from GPT-4 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [model]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 578,
          "function_name": "instruction_chat",
          "code": "text(\"Llama 2 chat \"), link(\"https://arxiv.org/pdf/2307.09288\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Llama 2 chat ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "authors": [
              "Hugo Touvron",
              "Louis Martin",
              "Kevin Stone",
              "Peter Albert",
              "Amjad Almahairi",
              "Yasmine Babaei",
              "Nikolay Bashlykov",
              "Soumya Batra",
              "Prajjwal Bhargava",
              "Shruti Bhosale",
              "Dan Bikel",
              "Lukas Blecher",
              "Cristian Canton Ferrer",
              "Moya Chen",
              "Guillem Cucurull",
              "David Esiobu",
              "Jude Fernandes",
              "Jeremy Fu",
              "Wenyin Fu",
              "Brian Fuller",
              "Cynthia Gao",
              "Vedanuj Goswami",
              "Naman Goyal",
              "Anthony Hartshorn",
              "Saghar Hosseini",
              "Rui Hou",
              "Hakan Inan",
              "Marcin Kardas",
              "Viktor Kerkez",
              "Madian Khabsa",
              "Isabel Kloumann",
              "Artem Korenev",
              "Punit Singh Koura",
              "Marie-Anne Lachaux",
              "Thibaut Lavril",
              "Jenya Lee",
              "Diana Liskovich",
              "Yinghai Lu",
              "Yuning Mao",
              "Xavier Martinet",
              "Todor Mihaylov",
              "Pushkar Mishra",
              "Igor Molybog",
              "Yixin Nie",
              "Andrew Poulton",
              "Jeremy Reizenstein",
              "Rashi Rungta",
              "Kalyan Saladi",
              "Alan Schelten",
              "Ruan Silva",
              "Eric Michael Smith",
              "Ranjan Subramanian",
              "Xiaoqing Ellen Tan",
              "Binh Tang",
              "Ross Taylor",
              "Adina Williams",
              "Jian Xiang Kuan",
              "Puxin Xu",
              "Zheng Yan",
              "Iliyan Zarov",
              "Yuchen Zhang",
              "Angela Fan",
              "Melanie Kambadur",
              "Sharan Narang",
              "Aurelien Rodriguez",
              "Robert Stojnic",
              "Sergey Edunov",
              "Thomas Scialom"
            ],
            "organization": null,
            "date": "2023-07-18T14:31:57Z",
            "url": "https://arxiv.org/pdf/2307.09288",
            "description": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 579,
          "function_name": "instruction_chat",
          "code": "text(\"- 27,540 examples of high-quality instruction data from vendor-based annotations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 27,540 examples of high-quality instruction data from vendor-based annotations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 580,
          "function_name": "instruction_chat",
          "code": "text(\"- Said was better than using the millions of examples from open datasets\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Said was better than using the millions of examples from open datasets",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 581,
          "function_name": "instruction_chat",
          "code": "text(\"- Could have labeled less data and saved more effort for getting RLHF data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Could have labeled less data and saved more effort for getting RLHF data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 583,
          "function_name": "instruction_chat",
          "code": "text(\"Llama-Nemotron post-training data [[NVIDIA, 2024](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Llama-Nemotron post-training data [[NVIDIA, 2024](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 584,
          "function_name": "instruction_chat",
          "code": "text(\"- Prompts: public datasets (e.g., WildChat) or synthetically-generated, then filtered\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Prompts: public datasets (e.g., WildChat) or synthetically-generated, then filtered",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 585,
          "function_name": "instruction_chat",
          "code": "text(\"- Generated synthetic responses from Llama, Mixtral, DeepSeek r1, Qwen (commercially viable, unlike GPT-4)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Generated synthetic responses from Llama, Mixtral, DeepSeek r1, Qwen (commercially viable, unlike GPT-4)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 586,
          "function_name": "instruction_chat",
          "code": "text(\"- Included reasoning traces\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Included reasoning traces",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        },
        {
          "path": "lecture_13.py",
          "line_number": 587,
          "function_name": "instruction_chat",
          "code": "text(\"- [Examples](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset/viewer/SFT/code)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- [Examples](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset/viewer/SFT/code)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 35,
          "function_name": "main",
          "code": "instruction_chat()    # Instruction following and chat"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 37,
          "function_name": "main",
          "code": "text(\"### Summary\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Summary",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 38,
          "function_name": "main",
          "code": "text(\"- Key lesson: Data does not fall from the sky. You have to work to get it.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Key lesson: Data does not fall from the sky. You have to work to get it.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 39,
          "function_name": "main",
          "code": "text(\"- Live service => raw data => processed data (conversion, filtering, deduplication)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Live service => raw data => processed data (conversion, filtering, deduplication)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 40,
          "function_name": "main",
          "code": "text(\"- Data is the key ingredient that differentiates language models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Data is the key ingredient that differentiates language models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 41,
          "function_name": "main",
          "code": "text(\"- Legal and ethical issues (e.g., copyright and privacy)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Legal and ethical issues (e.g., copyright and privacy)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_13.py",
          "line_number": 42,
          "function_name": "main",
          "code": "text(\"- Much of this pipeline is heuristic, many opportunities to improve!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Much of this pipeline is heuristic, many opportunities to improve!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    }
  ]
}