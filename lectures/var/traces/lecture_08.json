{
  "files": {
    "lecture_08.py": "import torch\nimport time\nimport os\nfrom typing import List, Callable\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.distributed.fsdp\nfrom execute_util import text, image, link, system_text\nfrom torch_util import get_device\nfrom lecture_util import article_link\nfrom lecture_08_utils import spawn, int_divide, summarize_tensor, get_init_params, render_duration\n\ndef main():\n    text(\"Last week: parallelism within a single GPU\")\n    text(\"This week: parallelism across multiple GPUs\")\n    image(\"images/gpu-node-overview.png\", width=500)\n\n    text(\"In both cases, **compute** (arithmetic logic units) is far from inputs/outputs (**data**).\")\n    text(\"Unifying theme: orchestrate computation to avoid data transfer bottlenecks\")\n\n    text(\"Last week: reduce memory accesses via fusion/tiling\")\n    text(\"This week: reduce communication across GPUs/nodes via replication/sharding\")\n\n    text(\"Generalized hierarchy (from small/fast to big/slow):\")\n    text(\"- Single node, single GPU: L1 cache / shared memory\")\n    text(\"- Single node, single GPU: HBM\")\n    text(\"- Single node, multi-GPU: NVLink\")\n    text(\"- Multi-node, multi-GPU: NVSwitch\")\n\n    text(\"This lecture: concretize the concepts from last lecture in code\")\n\n    link(title=\"[stdout for this lecture]\", url=\"var/traces/lecture_08_stdout.txt\")\n\n    text(\"### Part 1: building blocks of distributed communication/computation\")\n    collective_operations()    # Conceptual programming interface\n    torch_distributed()        # How this is implemented in NCCL/PyTorch\n    benchmarking()             # Measure actual NCCL bandwidth\n\n    text(\"### Part 2: distributed training\")\n    text(\"Walk through bare-bones implementations of each strategy on deep MLPs.\")\n    text(\"Recall that MLPs are the compute bottleneck in Transformers, so this is representative.\")\n    data_parallelism()         # Cut up along the batch dimension\n    tensor_parallelism()       # Cut up along the width dimension\n    pipeline_parallelism()     # Cut up along the depth dimension\n\n    text(\"What's missing?\")\n    text(\"- More general models (with attention, etc.)\")\n    text(\"- More communication/computation overlap\")\n    text(\"- This require more complex code with more bookkeeping\")\n    text(\"- Jax/TPUs: just define the model, the sharding strategy, and the Jax compiler handles the rest \"), link(title=\"[levanter]\", url=\"https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html\")\n    text(\"- But we're doing PyTorch so you can see how one builds up from the primitives\")\n\n    text(\"### Summary\")\n    text(\"- Many ways to parallelize: data (batch), tensor/expert (width), pipeline (depth), sequence (length)\")\n    text(\"- Can **re-compute** or store in **memory** or store in another GPUs memory and **communicate**\")\n    text(\"- Hardware is getting faster, but will always want bigger models, so will have this hierarchical structure\")\n\n\ndef collective_operations():\n    text(\"**Collective operations** are the conceptual primitives used for distributed programming \"), article_link(\"https://en.wikipedia.org/wiki/Collective_operation\")\n    text(\"- Collective means that you specify communication pattern across many (e.g., 256) nodes.\")\n    text(\"- These are classic in the parallel programming literature from the 1980s.\")\n    text(\"- Better/faster abstraction than managing point-to-point communication yourself.\")\n\n    text(\"Terminology:\")\n    text(\"- **World size**: number of devices (e.g., 4)\")\n    text(\"- **Rank**: a device (e.g., 0, 1, 2, 3)\")\n\n    text(\"### Broadcast\"), image(\"https://pytorch.org/tutorials/_images/broadcast.png\", width=400)\n\n    text(\"### Scatter\"), image(\"https://pytorch.org/tutorials/_images/scatter.png\", width=400)\n\n    text(\"### Gather\"), image(\"https://pytorch.org/tutorials/_images/gather.png\", width=400)\n\n    text(\"### Reduce\"), image(\"https://pytorch.org/tutorials/_images/reduce.png\", width=400)\n\n    text(\"### All-gather\"), image(\"https://pytorch.org/tutorials/_images/all_gather.png\", width=400)\n\n    text(\"### Reduce-scatter\"), image(\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/reducescatter.png\", width=400)\n\n    text(\"### All-reduce = reduce-scatter + all-gather\"), image(\"https://pytorch.org/tutorials/_images/all_reduce.png\", width=400)\n\n    text(\"Way to remember the terminology:\")\n    text(\"- Reduce: performs some associative/commutative operation (sum, min, max)\")\n    text(\"- Broadcast/scatter is inverse of gather\")\n    text(\"- All: means destination is all devices\")\n\n\ndef torch_distributed():\n    text(\"### Hardware\")\n    text(\"Classic (in the home):\")\n    image(\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs42774-021-00098-3/MediaObjects/42774_2021_98_Fig1_HTML.png?as=webp\", width=400)\n    text(\"- GPUs on same node communicate via a PCI(e) bus (v7.0, 16 lanes => 242 GB/s) \"), article_link(\"https://en.wikipedia.org/wiki/PCI_Express\")\n    text(\"- GPUs on different nodes communicate via Ethernet (~200 MB/s)\")\n\n    text(\"Modern (in the data center):\")\n    image(\"https://www.nextplatform.com/wp-content/uploads/2018/04/nvidia-nvswitch-topology-two.jpg\", width=400)\n    text(\"- Within a node: NVLink connects GPUs directly, bypass CPU\")\n    text(\"- Across nodes: NVSwitch connects GPUs directly, bypass Ethernet\")\n\n    text(\"Each H100 has 18 NVLink 4.0 links, for a total of 900GB/s \"), article_link(\"https://www.nvidia.com/en-us/data-center/nvlink/\")\n    text(\"In comparison, memory bandwidth for HBM is 3.9 TB/s \"), article_link(\"https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet\")\n\n    text(\"Let's check what our hardware setup is. \"), article_link(\"https://guide.ncloud-docs.com/docs/en/server-baremetal-a100-check-vpc\")\n    if torch.cuda.is_available():\n        os.system(\"nvidia-smi topo -m\")\n        text(\"Note GPUs are connected via NV18, also connected to NICs (for PCIe)\")\n\n    text(\"### NVIDIA Collective Communication Library (NCCL)\")\n    text(\"NCCL translates collective operations into low-level packets that are sent between GPUs. \"), link(title=\"[talk]\", url=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31880/\")\n    text(\"- Detects topology of hardware (e.g., number of nodes, switches, NVLink/PCIe)\")\n    text(\"- Optimizes the path between GPUs\")\n    text(\"- Launches CUDA kernels to send/receive data\")\n\n    text(\"### PyTorch distributed library (`torch.distributed`)\")\n    link(title=\"[Documentation]\", url=\"https://pytorch.org/docs/stable/distributed.html\")\n\n    text(\"- Provides clean interface for collective operations (e.g., `all_gather_into_tensor`)\")\n    text(\"- Supports multiple backends for different hardware: gloo (CPU), nccl (GPU)\")\n    text(\"- Also supports higher-level algorithms (e.g., `FullyShardedDataParallel`) [not used in this course]\")\n\n    text(\"Let's walk through some examples.\")\n    spawn(collective_operations_main, world_size=4)\n\n\ndef collective_operations_main(rank: int, world_size: int):\n    \"\"\"This function is running asynchronously for each process (rank = 0, ..., world_size - 1).\"\"\"\n    setup(rank, world_size)\n\n    # All-reduce\n    dist.barrier()  # Waits for all processes to get to this point (in this case, for print statements)\n\n    tensor = torch.tensor([0., 1, 2, 3], device=get_device(rank)) + rank  # Both input and output\n\n    print(f\"Rank {rank} [before all-reduce]: {tensor}\", flush=True)\n    dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)  # Modifies tensor in place\n    print(f\"Rank {rank} [after all-reduce]: {tensor}\", flush=True)\n\n    # Reduce-scatter\n    dist.barrier()\n\n    input = torch.arange(world_size, dtype=torch.float32, device=get_device(rank)) + rank  # Input\n    output = torch.empty(1, device=get_device(rank))  # Allocate output\n\n    print(f\"Rank {rank} [before reduce-scatter]: input = {input}, output = {output}\", flush=True)\n    dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)\n    print(f\"Rank {rank} [after reduce-scatter]: input = {input}, output = {output}\", flush=True)\n\n    # All-gather\n    dist.barrier()\n\n    input = output  # Input is the output of reduce-scatter\n    output = torch.empty(world_size, device=get_device(rank))  # Allocate output\n\n    print(f\"Rank {rank} [before all-gather]: input = {input}, output = {output}\", flush=True)\n    dist.all_gather_into_tensor(output_tensor=output, input_tensor=input, async_op=False)\n    print(f\"Rank {rank} [after all-gather]: input = {input}, output = {output}\", flush=True)\n\n    text(\"Indeed, all-reduce = reduce-scatter + all-gather!\")\n\n    cleanup()\n\n\ndef benchmarking():\n    text(\"Let's see how fast communication happens (restrict to one node).\")\n\n    # All-reduce\n    spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)\n\n    # Reduce-scatter\n    spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)\n\n    # References\n    link(title=\"How to reason about operations\", url=\"https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#allreduce\")\n    link(title=\"Sample code\", url=\"https://github.com/stas00/ml-engineering/blob/master/network/benchmarks/all_reduce_bench.py\")\n\n\ndef all_reduce(rank: int, world_size: int, num_elements: int):\n    setup(rank, world_size)\n\n    # Create tensor\n    tensor = torch.randn(num_elements, device=get_device(rank))\n\n    # Warmup\n    dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kernels to finish\n        dist.barrier()            # Wait for all the processes to get here\n\n    # Perform all-reduce\n    start_time = time.time()\n    dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kernels to finish\n        dist.barrier()            # Wait for all the processes to get here\n    end_time = time.time()\n\n    duration = end_time - start_time\n    print(f\"[all_reduce] Rank {rank}: all_reduce(world_size={world_size}, num_elements={num_elements}) took {render_duration(duration)}\", flush=True)\n\n    # Measure the effective bandwidth\n    dist.barrier()\n    size_bytes = tensor.element_size() * tensor.numel()\n    sent_bytes = size_bytes * 2 * (world_size - 1)  # 2x because send input and receive output\n    total_duration = world_size * duration\n    bandwidth = sent_bytes / total_duration\n    print(f\"[all_reduce] Rank {rank}: all_reduce measured bandwidth = {round(bandwidth / 1024**3)} GB/s\", flush=True)\n\n    cleanup()\n\n\ndef reduce_scatter(rank: int, world_size: int, num_elements: int):\n    setup(rank, world_size)\n\n    # Create input and outputs\n    input = torch.randn(world_size, num_elements, device=get_device(rank))  # Each rank has a matrix\n    output = torch.empty(num_elements, device=get_device(rank))\n\n    # Warmup\n    dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kerels to finish\n        dist.barrier()            # Wait for all the processes to get here\n\n    # Perform reduce-scatter\n    start_time = time.time()\n    dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kerels to finish\n        dist.barrier()            # Wait for all the processes to get here\n    end_time = time.time()\n\n    duration = end_time - start_time\n    print(f\"[reduce_scatter] Rank {rank}: reduce_scatter(world_size={world_size}, num_elements={num_elements}) took {render_duration(duration)}\", flush=True)\n\n    # Measure the effective bandwidth\n    dist.barrier()\n    data_bytes = output.element_size() * output.numel()  # How much data in the output\n    sent_bytes = data_bytes * (world_size - 1)  # How much needs to be sent (no 2x here)\n    total_duration = world_size * duration  # Total time for transmission\n    bandwidth = sent_bytes / total_duration\n    print(f\"[reduce_scatter] Rank {rank}: reduce_scatter measured bandwidth = {round(bandwidth / 1024**3)} GB/s\", flush=True)\n\n    cleanup()\n\n\ndef data_parallelism():\n    image(\"images/data-parallelism.png\", width=300)\n    text(\"Sharding strategy: each rank gets a slice of the data\")\n\n    data = generate_sample_data()\n    spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)\n\n    text(\"Notes:\")\n    text(\"- Losses are different across ranks (computed on local data)\")\n    text(\"- Gradients are all-reduced to be the same across ranks\")\n    text(\"- Therefore, parameters remain the same across ranks\")\n\n\ndef generate_sample_data():\n    batch_size = 128\n    num_dim = 1024\n    data = torch.randn(batch_size, num_dim)\n    return data\n\n\ndef data_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_steps: int):\n    setup(rank, world_size)\n\n    # Get the slice of data for this rank (in practice, each rank should load only its own data)\n    batch_size = data.size(0)  # @inspect batch_size\n    num_dim = data.size(1)  # @inspect num_dim\n    local_batch_size = int_divide(batch_size, world_size)  # @inspect local_batch_size\n    start_index = rank * local_batch_size  # @inspect start_index\n    end_index = start_index + local_batch_size  # @inspect end_index\n    data = data[start_index:end_index].to(get_device(rank))\n\n    # Create MLP parameters params[0], ..., params[num_layers - 1] (each rank has all parameters)\n    params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers)]\n    optimizer = torch.optim.AdamW(params, lr=1e-3)  # Each rank has own optimizer state\n\n    for step in range(num_steps):\n        # Forward pass\n        x = data\n        for param in params:\n            x = x @ param\n            x = F.gelu(x)\n        loss = x.square().mean()  # Loss function is average squared magnitude\n\n        # Backward pass\n        loss.backward()\n\n        # Sync gradients across workers (only difference between standard training and DDP)\n        for param in params:\n            dist.all_reduce(tensor=param.grad, op=dist.ReduceOp.AVG, async_op=False)\n\n        # Update parameters\n        optimizer.step()\n\n        print(f\"[data_parallelism] Rank {rank}: step = {step}, loss = {loss.item()}, params = {[summarize_tensor(params[i]) for i in range(num_layers)]}\", flush=True)\n\n    cleanup()\n\n\ndef tensor_parallelism():\n    image(\"images/tensor-parallelism.png\", width=300)\n    text(\"Sharding strategy: each rank gets part of each layer, transfer all data/activations\")\n\n    data = generate_sample_data()\n    spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)\n\n\ndef tensor_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int):\n    setup(rank, world_size)\n\n    data = data.to(get_device(rank))\n    batch_size = data.size(0)  # @inspect batch_size\n    num_dim = data.size(1)  # @inspect num_dim\n    local_num_dim = int_divide(num_dim, world_size)  # Shard `num_dim`  @inspect local_num_dim\n\n    # Create model (each rank gets 1/world_size of the parameters)\n    params = [get_init_params(num_dim, local_num_dim, rank) for i in range(num_layers)]\n\n    # Forward pass\n    x = data\n    for i in range(num_layers):\n        # Compute activations (batch_size x local_num_dim)\n        x = x @ params[i]  # Note: this is only on a slice of the parameters\n        x = F.gelu(x)\n\n        # Allocate memory for activations (world_size x batch_size x local_num_dim)\n        activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]\n\n        # Send activations via all gather\n        dist.all_gather(tensor_list=activations, tensor=x, async_op=False)\n\n        # Concatenate them to get batch_size x num_dim\n        x = torch.cat(activations, dim=1)\n\n    print(f\"[tensor_parallelism] Rank {rank}: forward pass produced activations {summarize_tensor(x)}\", flush=True)\n\n    # Backward pass: homework exercise\n\n    cleanup()\n\n\ndef pipeline_parallelism():\n    image(\"images/pipeline-parallelism.png\", width=300)\n    text(\"Sharding strategy: each rank gets subset of layers, transfer all data/activations\")\n\n    data = generate_sample_data()\n    spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)\n\n\ndef pipeline_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_micro_batches: int):\n    setup(rank, world_size)\n\n    # Use all the data\n    data = data.to(get_device(rank))\n    batch_size = data.size(0)  # @inspect batch_size\n    num_dim = data.size(1)  # @inspect num_dim\n\n    # Split up layers\n    local_num_layers = int_divide(num_layers, world_size)  # @inspect local_num_layers\n\n    # Each rank gets a subset of layers\n    local_params = [get_init_params(num_dim, num_dim, rank) for i in range(local_num_layers)]\n\n    # Forward pass\n\n    # Break up into micro batches to minimize the bubble\n    micro_batch_size = int_divide(batch_size, num_micro_batches)  # @inspect micro_batch_size\n    if rank == 0:\n        # The data\n        micro_batches = data.chunk(chunks=num_micro_batches, dim=0)\n    else:\n        # Allocate memory for activations\n        micro_batches = [torch.empty(micro_batch_size, num_dim, device=get_device(rank)) for _ in range(num_micro_batches)]\n\n    for x in micro_batches:\n        # Get activations from previous rank\n        if rank - 1 >= 0:\n            dist.recv(tensor=x, src=rank - 1)\n\n        # Compute layers assigned to this rank\n        for param in local_params:\n            x = x @ param\n            x = F.gelu(x)\n\n        # Send to the next rank\n        if rank + 1 < world_size:\n            print(f\"[pipeline_parallelism] Rank {rank}: sending {summarize_tensor(x)} to rank {rank + 1}\", flush=True)\n            dist.send(tensor=x, dst=rank + 1)\n\n    text(\"Not handled: overlapping communication/computation to eliminate pipeline bubbles\")\n\n    # Backward pass: homework exercise\n\n    cleanup()\n\n############################################################\n\ndef setup(rank: int, world_size: int):\n    # Specify where master lives (rank 0), used to coordinate (actual data goes through NCCL)\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"15623\"\n\n    if torch.cuda.is_available():\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    else:\n        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n\ndef cleanup():\n    torch.distributed.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "steps": [
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 13,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 14,
          "function_name": "main",
          "code": "text(\"Last week: parallelism within a single GPU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Last week: parallelism within a single GPU",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 15,
          "function_name": "main",
          "code": "text(\"This week: parallelism across multiple GPUs\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This week: parallelism across multiple GPUs",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 16,
          "function_name": "main",
          "code": "image(\"images/gpu-node-overview.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/gpu-node-overview.png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 18,
          "function_name": "main",
          "code": "text(\"In both cases, **compute** (arithmetic logic units) is far from inputs/outputs (**data**).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In both cases, **compute** (arithmetic logic units) is far from inputs/outputs (**data**).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 19,
          "function_name": "main",
          "code": "text(\"Unifying theme: orchestrate computation to avoid data transfer bottlenecks\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Unifying theme: orchestrate computation to avoid data transfer bottlenecks",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 21,
          "function_name": "main",
          "code": "text(\"Last week: reduce memory accesses via fusion/tiling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Last week: reduce memory accesses via fusion/tiling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 22,
          "function_name": "main",
          "code": "text(\"This week: reduce communication across GPUs/nodes via replication/sharding\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This week: reduce communication across GPUs/nodes via replication/sharding",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 24,
          "function_name": "main",
          "code": "text(\"Generalized hierarchy (from small/fast to big/slow):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Generalized hierarchy (from small/fast to big/slow):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 25,
          "function_name": "main",
          "code": "text(\"- Single node, single GPU: L1 cache / shared memory\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Single node, single GPU: L1 cache / shared memory",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 26,
          "function_name": "main",
          "code": "text(\"- Single node, single GPU: HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Single node, single GPU: HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 27,
          "function_name": "main",
          "code": "text(\"- Single node, multi-GPU: NVLink\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Single node, multi-GPU: NVLink",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 28,
          "function_name": "main",
          "code": "text(\"- Multi-node, multi-GPU: NVSwitch\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Multi-node, multi-GPU: NVSwitch",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 30,
          "function_name": "main",
          "code": "text(\"This lecture: concretize the concepts from last lecture in code\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This lecture: concretize the concepts from last lecture in code",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "link(title=\"[stdout for this lecture]\", url=\"var/traces/lecture_08_stdout.txt\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[stdout for this lecture]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "var/traces/lecture_08_stdout.txt",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "text(\"### Part 1: building blocks of distributed communication/computation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Part 1: building blocks of distributed communication/computation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 59,
          "function_name": "collective_operations",
          "code": "def collective_operations():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 60,
          "function_name": "collective_operations",
          "code": "text(\"**Collective operations** are the conceptual primitives used for distributed programming \"), article_link(\"https://en.wikipedia.org/wiki/Collective_operation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Collective operations** are the conceptual primitives used for distributed programming ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Collective_operation",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 61,
          "function_name": "collective_operations",
          "code": "text(\"- Collective means that you specify communication pattern across many (e.g., 256) nodes.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Collective means that you specify communication pattern across many (e.g., 256) nodes.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 62,
          "function_name": "collective_operations",
          "code": "text(\"- These are classic in the parallel programming literature from the 1980s.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- These are classic in the parallel programming literature from the 1980s.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 63,
          "function_name": "collective_operations",
          "code": "text(\"- Better/faster abstraction than managing point-to-point communication yourself.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Better/faster abstraction than managing point-to-point communication yourself.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 65,
          "function_name": "collective_operations",
          "code": "text(\"Terminology:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Terminology:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 66,
          "function_name": "collective_operations",
          "code": "text(\"- **World size**: number of devices (e.g., 4)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **World size**: number of devices (e.g., 4)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 67,
          "function_name": "collective_operations",
          "code": "text(\"- **Rank**: a device (e.g., 0, 1, 2, 3)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **Rank**: a device (e.g., 0, 1, 2, 3)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 69,
          "function_name": "collective_operations",
          "code": "text(\"### Broadcast\"), image(\"https://pytorch.org/tutorials/_images/broadcast.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Broadcast",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-525847c9d4b48933cb231204a2d13e0e-https_pytorch_org_tutorials__images_broadcast_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 71,
          "function_name": "collective_operations",
          "code": "text(\"### Scatter\"), image(\"https://pytorch.org/tutorials/_images/scatter.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Scatter",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-3aa3584628cb0526c8b0e9d02b15d876-https_pytorch_org_tutorials__images_scatter_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 73,
          "function_name": "collective_operations",
          "code": "text(\"### Gather\"), image(\"https://pytorch.org/tutorials/_images/gather.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-7e8670a3b7cdc7848394514ef1da090a-https_pytorch_org_tutorials__images_gather_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 75,
          "function_name": "collective_operations",
          "code": "text(\"### Reduce\"), image(\"https://pytorch.org/tutorials/_images/reduce.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Reduce",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-1c451df4406aea85e640d1ae7df6df31-https_pytorch_org_tutorials__images_reduce_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 77,
          "function_name": "collective_operations",
          "code": "text(\"### All-gather\"), image(\"https://pytorch.org/tutorials/_images/all_gather.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### All-gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-4a48977cd9545f897942a4a4ef1175ac-https_pytorch_org_tutorials__images_all_gather_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 79,
          "function_name": "collective_operations",
          "code": "text(\"### Reduce-scatter\"), image(\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/reducescatter.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Reduce-scatter",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-66ea136cfe7f3e7394fd0b056fd9d949-https_docs_nvidia_com_deeplearning_nccl_user-guide_docs__images_reducescatter_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 81,
          "function_name": "collective_operations",
          "code": "text(\"### All-reduce = reduce-scatter + all-gather\"), image(\"https://pytorch.org/tutorials/_images/all_reduce.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### All-reduce = reduce-scatter + all-gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-0ef9693f0008d5a75aa5ac2b542b83ac-https_pytorch_org_tutorials__images_all_reduce_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 83,
          "function_name": "collective_operations",
          "code": "text(\"Way to remember the terminology:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Way to remember the terminology:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 84,
          "function_name": "collective_operations",
          "code": "text(\"- Reduce: performs some associative/commutative operation (sum, min, max)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Reduce: performs some associative/commutative operation (sum, min, max)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 85,
          "function_name": "collective_operations",
          "code": "text(\"- Broadcast/scatter is inverse of gather\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Broadcast/scatter is inverse of gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 86,
          "function_name": "collective_operations",
          "code": "text(\"- All: means destination is all devices\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- All: means destination is all devices",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 35,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 89,
          "function_name": "torch_distributed",
          "code": "def torch_distributed():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 90,
          "function_name": "torch_distributed",
          "code": "text(\"### Hardware\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Hardware",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 91,
          "function_name": "torch_distributed",
          "code": "text(\"Classic (in the home):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Classic (in the home):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 92,
          "function_name": "torch_distributed",
          "code": "image(\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs42774-021-00098-3/MediaObjects/42774_2021_98_Fig1_HTML.png?as=webp\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-b0641f11a73711b3078acbd257b0c805-https_media_springernature_com_lw685_springer-static_image_art_3A10_1186_2Fs42774-021-00098-3_MediaObjects_42774_2021_98_Fig1_HTML_png_as_webp",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 93,
          "function_name": "torch_distributed",
          "code": "text(\"- GPUs on same node communicate via a PCI(e) bus (v7.0, 16 lanes => 242 GB/s) \"), article_link(\"https://en.wikipedia.org/wiki/PCI_Express\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- GPUs on same node communicate via a PCI(e) bus (v7.0, 16 lanes => 242 GB/s) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/PCI_Express",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 94,
          "function_name": "torch_distributed",
          "code": "text(\"- GPUs on different nodes communicate via Ethernet (~200 MB/s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- GPUs on different nodes communicate via Ethernet (~200 MB/s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 96,
          "function_name": "torch_distributed",
          "code": "text(\"Modern (in the data center):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Modern (in the data center):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 97,
          "function_name": "torch_distributed",
          "code": "image(\"https://www.nextplatform.com/wp-content/uploads/2018/04/nvidia-nvswitch-topology-two.jpg\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-97297f9b05f43069b2bab97dd5a821a8-https_www_nextplatform_com_wp-content_uploads_2018_04_nvidia-nvswitch-topology-two_jpg",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 98,
          "function_name": "torch_distributed",
          "code": "text(\"- Within a node: NVLink connects GPUs directly, bypass CPU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Within a node: NVLink connects GPUs directly, bypass CPU",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 99,
          "function_name": "torch_distributed",
          "code": "text(\"- Across nodes: NVSwitch connects GPUs directly, bypass Ethernet\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Across nodes: NVSwitch connects GPUs directly, bypass Ethernet",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 101,
          "function_name": "torch_distributed",
          "code": "text(\"Each H100 has 18 NVLink 4.0 links, for a total of 900GB/s \"), article_link(\"https://www.nvidia.com/en-us/data-center/nvlink/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Each H100 has 18 NVLink 4.0 links, for a total of 900GB/s ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.nvidia.com/en-us/data-center/nvlink/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 102,
          "function_name": "torch_distributed",
          "code": "text(\"In comparison, memory bandwidth for HBM is 3.9 TB/s \"), article_link(\"https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In comparison, memory bandwidth for HBM is 3.9 TB/s ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 104,
          "function_name": "torch_distributed",
          "code": "text(\"Let's check what our hardware setup is. \"), article_link(\"https://guide.ncloud-docs.com/docs/en/server-baremetal-a100-check-vpc\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's check what our hardware setup is. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://guide.ncloud-docs.com/docs/en/server-baremetal-a100-check-vpc",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 105,
          "function_name": "torch_distributed",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 106,
          "function_name": "torch_distributed",
          "code": "os.system(\"nvidia-smi topo -m\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 107,
          "function_name": "torch_distributed",
          "code": "text(\"Note GPUs are connected via NV18, also connected to NICs (for PCIe)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Note GPUs are connected via NV18, also connected to NICs (for PCIe)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 109,
          "function_name": "torch_distributed",
          "code": "text(\"### NVIDIA Collective Communication Library (NCCL)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### NVIDIA Collective Communication Library (NCCL)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 110,
          "function_name": "torch_distributed",
          "code": "text(\"NCCL translates collective operations into low-level packets that are sent between GPUs. \"), link(title=\"[talk]\", url=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31880/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "NCCL translates collective operations into low-level packets that are sent between GPUs. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[talk]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31880/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 111,
          "function_name": "torch_distributed",
          "code": "text(\"- Detects topology of hardware (e.g., number of nodes, switches, NVLink/PCIe)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Detects topology of hardware (e.g., number of nodes, switches, NVLink/PCIe)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 112,
          "function_name": "torch_distributed",
          "code": "text(\"- Optimizes the path between GPUs\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Optimizes the path between GPUs",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 113,
          "function_name": "torch_distributed",
          "code": "text(\"- Launches CUDA kernels to send/receive data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Launches CUDA kernels to send/receive data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 115,
          "function_name": "torch_distributed",
          "code": "text(\"### PyTorch distributed library (`torch.distributed`)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### PyTorch distributed library (`torch.distributed`)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 116,
          "function_name": "torch_distributed",
          "code": "link(title=\"[Documentation]\", url=\"https://pytorch.org/docs/stable/distributed.html\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Documentation]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://pytorch.org/docs/stable/distributed.html",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 118,
          "function_name": "torch_distributed",
          "code": "text(\"- Provides clean interface for collective operations (e.g., `all_gather_into_tensor`)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Provides clean interface for collective operations (e.g., `all_gather_into_tensor`)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 119,
          "function_name": "torch_distributed",
          "code": "text(\"- Supports multiple backends for different hardware: gloo (CPU), nccl (GPU)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Supports multiple backends for different hardware: gloo (CPU), nccl (GPU)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 120,
          "function_name": "torch_distributed",
          "code": "text(\"- Also supports higher-level algorithms (e.g., `FullyShardedDataParallel`) [not used in this course]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Also supports higher-level algorithms (e.g., `FullyShardedDataParallel`) [not used in this course]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 122,
          "function_name": "torch_distributed",
          "code": "text(\"Let's walk through some examples.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's walk through some examples.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 126,
          "function_name": "collective_operations_main",
          "code": "def collective_operations_main(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 403,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 405,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 406,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 408,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "setup",
          "code": "dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 131,
          "function_name": "collective_operations_main",
          "code": "dist.barrier()  # Waits for all processes to get to this point (in this case, for print statements)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 133,
          "function_name": "collective_operations_main",
          "code": "tensor = torch.tensor([0., 1, 2, 3], device=get_device(rank)) + rank  # Both input and output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 135,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [before all-reduce]: {tensor}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 136,
          "function_name": "collective_operations_main",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)  # Modifies tensor in place"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 137,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [after all-reduce]: {tensor}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 140,
          "function_name": "collective_operations_main",
          "code": "dist.barrier()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 142,
          "function_name": "collective_operations_main",
          "code": "input = torch.arange(world_size, dtype=torch.float32, device=get_device(rank)) + rank  # Input"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 143,
          "function_name": "collective_operations_main",
          "code": "output = torch.empty(1, device=get_device(rank))  # Allocate output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 145,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [before reduce-scatter]: input = {input}, output = {output}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 146,
          "function_name": "collective_operations_main",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 147,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [after reduce-scatter]: input = {input}, output = {output}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 150,
          "function_name": "collective_operations_main",
          "code": "dist.barrier()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 152,
          "function_name": "collective_operations_main",
          "code": "input = output  # Input is the output of reduce-scatter"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 153,
          "function_name": "collective_operations_main",
          "code": "output = torch.empty(world_size, device=get_device(rank))  # Allocate output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 155,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [before all-gather]: input = {input}, output = {output}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 156,
          "function_name": "collective_operations_main",
          "code": "dist.all_gather_into_tensor(output_tensor=output, input_tensor=input, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 157,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [after all-gather]: input = {input}, output = {output}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 159,
          "function_name": "collective_operations_main",
          "code": "text(\"Indeed, all-reduce = reduce-scatter + all-gather!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Indeed, all-reduce = reduce-scatter + all-gather!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 161,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 161,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 161,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 161,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 164,
          "function_name": "benchmarking",
          "code": "def benchmarking():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 165,
          "function_name": "benchmarking",
          "code": "text(\"Let's see how fast communication happens (restrict to one node).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's see how fast communication happens (restrict to one node).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 178,
          "function_name": "all_reduce",
          "code": "def all_reduce(rank: int, world_size: int, num_elements: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 403,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 405,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 406,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 408,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "setup",
          "code": "dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 182,
          "function_name": "all_reduce",
          "code": "tensor = torch.randn(num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 185,
          "function_name": "all_reduce",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 186,
          "function_name": "all_reduce",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 187,
          "function_name": "all_reduce",
          "code": "torch.cuda.synchronize()  # Wait for CUDA kernels to finish"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 188,
          "function_name": "all_reduce",
          "code": "dist.barrier()            # Wait for all the processes to get here"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 191,
          "function_name": "all_reduce",
          "code": "start_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 192,
          "function_name": "all_reduce",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "all_reduce",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "all_reduce",
          "code": "torch.cuda.synchronize()  # Wait for CUDA kernels to finish"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 195,
          "function_name": "all_reduce",
          "code": "dist.barrier()            # Wait for all the processes to get here"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 196,
          "function_name": "all_reduce",
          "code": "end_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "all_reduce",
          "code": "duration = end_time - start_time"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 199,
          "function_name": "all_reduce",
          "code": "print(f\"[all_reduce] Rank {rank}: all_reduce(world_size={world_size}, num_elements={num_elements}) took {render_duration(duration)}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 202,
          "function_name": "all_reduce",
          "code": "dist.barrier()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 203,
          "function_name": "all_reduce",
          "code": "size_bytes = tensor.element_size() * tensor.numel()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "sent_bytes = size_bytes * 2 * (world_size - 1)  # 2x because send input and receive output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 205,
          "function_name": "all_reduce",
          "code": "total_duration = world_size * duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 206,
          "function_name": "all_reduce",
          "code": "bandwidth = sent_bytes / total_duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 207,
          "function_name": "all_reduce",
          "code": "print(f\"[all_reduce] Rank {rank}: all_reduce measured bandwidth = {round(bandwidth / 1024**3)} GB/s\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 209,
          "function_name": "all_reduce",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 209,
          "function_name": "all_reduce",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 209,
          "function_name": "all_reduce",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 209,
          "function_name": "all_reduce",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=100 * 1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 212,
          "function_name": "reduce_scatter",
          "code": "def reduce_scatter(rank: int, world_size: int, num_elements: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 213,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 213,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 403,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 213,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 405,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 213,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 406,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 213,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 408,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 213,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "setup",
          "code": "dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 213,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 216,
          "function_name": "reduce_scatter",
          "code": "input = torch.randn(world_size, num_elements, device=get_device(rank))  # Each rank has a matrix"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 217,
          "function_name": "reduce_scatter",
          "code": "output = torch.empty(num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 220,
          "function_name": "reduce_scatter",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 221,
          "function_name": "reduce_scatter",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 222,
          "function_name": "reduce_scatter",
          "code": "torch.cuda.synchronize()  # Wait for CUDA kerels to finish"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 223,
          "function_name": "reduce_scatter",
          "code": "dist.barrier()            # Wait for all the processes to get here"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 226,
          "function_name": "reduce_scatter",
          "code": "start_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 227,
          "function_name": "reduce_scatter",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 228,
          "function_name": "reduce_scatter",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 229,
          "function_name": "reduce_scatter",
          "code": "torch.cuda.synchronize()  # Wait for CUDA kerels to finish"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 230,
          "function_name": "reduce_scatter",
          "code": "dist.barrier()            # Wait for all the processes to get here"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 231,
          "function_name": "reduce_scatter",
          "code": "end_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "reduce_scatter",
          "code": "duration = end_time - start_time"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 234,
          "function_name": "reduce_scatter",
          "code": "print(f\"[reduce_scatter] Rank {rank}: reduce_scatter(world_size={world_size}, num_elements={num_elements}) took {render_duration(duration)}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "dist.barrier()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 238,
          "function_name": "reduce_scatter",
          "code": "data_bytes = output.element_size() * output.numel()  # How much data in the output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 239,
          "function_name": "reduce_scatter",
          "code": "sent_bytes = data_bytes * (world_size - 1)  # How much needs to be sent (no 2x here)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 240,
          "function_name": "reduce_scatter",
          "code": "total_duration = world_size * duration  # Total time for transmission"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 241,
          "function_name": "reduce_scatter",
          "code": "bandwidth = sent_bytes / total_duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 242,
          "function_name": "reduce_scatter",
          "code": "print(f\"[reduce_scatter] Rank {rank}: reduce_scatter measured bandwidth = {round(bandwidth / 1024**3)} GB/s\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 244,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 244,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 244,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 244,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 171,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=100 * 1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 174,
          "function_name": "benchmarking",
          "code": "link(title=\"How to reason about operations\", url=\"https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#allreduce\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "How to reason about operations",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#allreduce",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 175,
          "function_name": "benchmarking",
          "code": "link(title=\"Sample code\", url=\"https://github.com/stas00/ml-engineering/blob/master/network/benchmarks/all_reduce_bench.py\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Sample code",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stas00/ml-engineering/blob/master/network/benchmarks/all_reduce_bench.py",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "benchmarking()             # Measure actual NCCL bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "text(\"### Part 2: distributed training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Part 2: distributed training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "text(\"Walk through bare-bones implementations of each strategy on deep MLPs.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Walk through bare-bones implementations of each strategy on deep MLPs.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "text(\"Recall that MLPs are the compute bottleneck in Transformers, so this is representative.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall that MLPs are the compute bottleneck in Transformers, so this is representative.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 247,
          "function_name": "data_parallelism",
          "code": "def data_parallelism():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 248,
          "function_name": "data_parallelism",
          "code": "image(\"images/data-parallelism.png\", width=300)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/data-parallelism.png",
          "style": {
            "width": 300
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 249,
          "function_name": "data_parallelism",
          "code": "text(\"Sharding strategy: each rank gets a slice of the data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sharding strategy: each rank gets a slice of the data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "data_parallelism",
          "code": "data = generate_sample_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "data_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 260,
          "function_name": "generate_sample_data",
          "code": "def generate_sample_data():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "data_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 261,
          "function_name": "generate_sample_data",
          "code": "batch_size = 128"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "data_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 262,
          "function_name": "generate_sample_data",
          "code": "num_dim = 1024"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "data_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 263,
          "function_name": "generate_sample_data",
          "code": "data = torch.randn(batch_size, num_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "data_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 264,
          "function_name": "generate_sample_data",
          "code": "return data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "data_parallelism",
          "code": "data = generate_sample_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "data_parallelism_main",
          "code": "def data_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_steps: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 268,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 268,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 403,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 268,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 405,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 268,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 406,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 268,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 408,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 268,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "setup",
          "code": "dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 268,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 271,
          "function_name": "data_parallelism_main",
          "code": "batch_size = data.size(0)  # @inspect batch_size"
        }
      ],
      "env": {
        "batch_size": 128
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 272,
          "function_name": "data_parallelism_main",
          "code": "num_dim = data.size(1)  # @inspect num_dim"
        }
      ],
      "env": {
        "num_dim": 1024
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 273,
          "function_name": "data_parallelism_main",
          "code": "local_batch_size = int_divide(batch_size, world_size)  # @inspect local_batch_size"
        }
      ],
      "env": {
        "local_batch_size": 32
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 274,
          "function_name": "data_parallelism_main",
          "code": "start_index = rank * local_batch_size  # @inspect start_index"
        }
      ],
      "env": {
        "start_index": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 275,
          "function_name": "data_parallelism_main",
          "code": "end_index = start_index + local_batch_size  # @inspect end_index"
        }
      ],
      "env": {
        "end_index": 32
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 276,
          "function_name": "data_parallelism_main",
          "code": "data = data[start_index:end_index].to(get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 279,
          "function_name": "data_parallelism_main",
          "code": "params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 279,
          "function_name": "data_parallelism_main",
          "code": "params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers)]"
        },
        {
          "path": "lecture_08.py",
          "line_number": 279,
          "function_name": "<listcomp>",
          "code": "params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 279,
          "function_name": "data_parallelism_main",
          "code": "params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism_main",
          "code": "optimizer = torch.optim.AdamW(params, lr=1e-3)  # Each rank has own optimizer state"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 282,
          "function_name": "data_parallelism_main",
          "code": "for step in range(num_steps):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 284,
          "function_name": "data_parallelism_main",
          "code": "x = data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 285,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 286,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 287,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 285,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 286,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 287,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 285,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 286,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 287,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 285,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 286,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 287,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 285,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "loss = x.square().mean()  # Loss function is average squared magnitude"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 291,
          "function_name": "data_parallelism_main",
          "code": "loss.backward()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 294,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 295,
          "function_name": "data_parallelism_main",
          "code": "dist.all_reduce(tensor=param.grad, op=dist.ReduceOp.AVG, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 294,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 295,
          "function_name": "data_parallelism_main",
          "code": "dist.all_reduce(tensor=param.grad, op=dist.ReduceOp.AVG, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 294,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 295,
          "function_name": "data_parallelism_main",
          "code": "dist.all_reduce(tensor=param.grad, op=dist.ReduceOp.AVG, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 294,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 295,
          "function_name": "data_parallelism_main",
          "code": "dist.all_reduce(tensor=param.grad, op=dist.ReduceOp.AVG, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 294,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 298,
          "function_name": "data_parallelism_main",
          "code": "optimizer.step()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 300,
          "function_name": "data_parallelism_main",
          "code": "print(f\"[data_parallelism] Rank {rank}: step = {step}, loss = {loss.item()}, params = {[summarize_tensor(params[i]) for i in range(num_layers)]}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 300,
          "function_name": "data_parallelism_main",
          "code": "print(f\"[data_parallelism] Rank {rank}: step = {step}, loss = {loss.item()}, params = {[summarize_tensor(params[i]) for i in range(num_layers)]}\", flush=True)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 300,
          "function_name": "<listcomp>",
          "code": "print(f\"[data_parallelism] Rank {rank}: step = {step}, loss = {loss.item()}, params = {[summarize_tensor(params[i]) for i in range(num_layers)]}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 300,
          "function_name": "data_parallelism_main",
          "code": "print(f\"[data_parallelism] Rank {rank}: step = {step}, loss = {loss.item()}, params = {[summarize_tensor(params[i]) for i in range(num_layers)]}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 282,
          "function_name": "data_parallelism_main",
          "code": "for step in range(num_steps):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 302,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 302,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 302,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 302,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 254,
          "function_name": "data_parallelism",
          "code": "text(\"Notes:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Notes:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 255,
          "function_name": "data_parallelism",
          "code": "text(\"- Losses are different across ranks (computed on local data)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Losses are different across ranks (computed on local data)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 256,
          "function_name": "data_parallelism",
          "code": "text(\"- Gradients are all-reduced to be the same across ranks\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Gradients are all-reduced to be the same across ranks",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 257,
          "function_name": "data_parallelism",
          "code": "text(\"- Therefore, parameters remain the same across ranks\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Therefore, parameters remain the same across ranks",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 42,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 305,
          "function_name": "tensor_parallelism",
          "code": "def tensor_parallelism():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 306,
          "function_name": "tensor_parallelism",
          "code": "image(\"images/tensor-parallelism.png\", width=300)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/tensor-parallelism.png",
          "style": {
            "width": 300
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 307,
          "function_name": "tensor_parallelism",
          "code": "text(\"Sharding strategy: each rank gets part of each layer, transfer all data/activations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sharding strategy: each rank gets part of each layer, transfer all data/activations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 309,
          "function_name": "tensor_parallelism",
          "code": "data = generate_sample_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 309,
          "function_name": "tensor_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 260,
          "function_name": "generate_sample_data",
          "code": "def generate_sample_data():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 309,
          "function_name": "tensor_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 261,
          "function_name": "generate_sample_data",
          "code": "batch_size = 128"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 309,
          "function_name": "tensor_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 262,
          "function_name": "generate_sample_data",
          "code": "num_dim = 1024"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 309,
          "function_name": "tensor_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 263,
          "function_name": "generate_sample_data",
          "code": "data = torch.randn(batch_size, num_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 309,
          "function_name": "tensor_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 264,
          "function_name": "generate_sample_data",
          "code": "return data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 309,
          "function_name": "tensor_parallelism",
          "code": "data = generate_sample_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 313,
          "function_name": "tensor_parallelism_main",
          "code": "def tensor_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 314,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 314,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 403,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 314,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 405,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 314,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 406,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 314,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 408,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 314,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "setup",
          "code": "dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 314,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 316,
          "function_name": "tensor_parallelism_main",
          "code": "data = data.to(get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 317,
          "function_name": "tensor_parallelism_main",
          "code": "batch_size = data.size(0)  # @inspect batch_size"
        }
      ],
      "env": {
        "batch_size": 128
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 318,
          "function_name": "tensor_parallelism_main",
          "code": "num_dim = data.size(1)  # @inspect num_dim"
        }
      ],
      "env": {
        "num_dim": 1024
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 319,
          "function_name": "tensor_parallelism_main",
          "code": "local_num_dim = int_divide(num_dim, world_size)  # Shard `num_dim`  @inspect local_num_dim"
        }
      ],
      "env": {
        "local_num_dim": 256
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "tensor_parallelism_main",
          "code": "params = [get_init_params(num_dim, local_num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "tensor_parallelism_main",
          "code": "params = [get_init_params(num_dim, local_num_dim, rank) for i in range(num_layers)]"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "<listcomp>",
          "code": "params = [get_init_params(num_dim, local_num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "tensor_parallelism_main",
          "code": "params = [get_init_params(num_dim, local_num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 325,
          "function_name": "tensor_parallelism_main",
          "code": "x = data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 326,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 328,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 329,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "<listcomp>",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 335,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 326,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 328,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 329,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "<listcomp>",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 335,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 326,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 328,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 329,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "<listcomp>",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 335,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 326,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 328,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 329,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "<listcomp>",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, local_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 335,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 326,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 340,
          "function_name": "tensor_parallelism_main",
          "code": "print(f\"[tensor_parallelism] Rank {rank}: forward pass produced activations {summarize_tensor(x)}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 344,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 344,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 344,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 344,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 347,
          "function_name": "pipeline_parallelism",
          "code": "def pipeline_parallelism():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 348,
          "function_name": "pipeline_parallelism",
          "code": "image(\"images/pipeline-parallelism.png\", width=300)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/pipeline-parallelism.png",
          "style": {
            "width": 300
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 349,
          "function_name": "pipeline_parallelism",
          "code": "text(\"Sharding strategy: each rank gets subset of layers, transfer all data/activations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sharding strategy: each rank gets subset of layers, transfer all data/activations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "pipeline_parallelism",
          "code": "data = generate_sample_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "pipeline_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 260,
          "function_name": "generate_sample_data",
          "code": "def generate_sample_data():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "pipeline_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 261,
          "function_name": "generate_sample_data",
          "code": "batch_size = 128"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "pipeline_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 262,
          "function_name": "generate_sample_data",
          "code": "num_dim = 1024"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "pipeline_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 263,
          "function_name": "generate_sample_data",
          "code": "data = torch.randn(batch_size, num_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "pipeline_parallelism",
          "code": "data = generate_sample_data()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 264,
          "function_name": "generate_sample_data",
          "code": "return data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "pipeline_parallelism",
          "code": "data = generate_sample_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 355,
          "function_name": "pipeline_parallelism_main",
          "code": "def pipeline_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_micro_batches: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 356,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 356,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 403,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 356,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 405,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 356,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 406,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 356,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 408,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 356,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "setup",
          "code": "dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 356,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 359,
          "function_name": "pipeline_parallelism_main",
          "code": "data = data.to(get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 360,
          "function_name": "pipeline_parallelism_main",
          "code": "batch_size = data.size(0)  # @inspect batch_size"
        }
      ],
      "env": {
        "batch_size": 128
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 361,
          "function_name": "pipeline_parallelism_main",
          "code": "num_dim = data.size(1)  # @inspect num_dim"
        }
      ],
      "env": {
        "num_dim": 1024
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 364,
          "function_name": "pipeline_parallelism_main",
          "code": "local_num_layers = int_divide(num_layers, world_size)  # @inspect local_num_layers"
        }
      ],
      "env": {
        "local_num_layers": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 367,
          "function_name": "pipeline_parallelism_main",
          "code": "local_params = [get_init_params(num_dim, num_dim, rank) for i in range(local_num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 367,
          "function_name": "pipeline_parallelism_main",
          "code": "local_params = [get_init_params(num_dim, num_dim, rank) for i in range(local_num_layers)]"
        },
        {
          "path": "lecture_08.py",
          "line_number": 367,
          "function_name": "<listcomp>",
          "code": "local_params = [get_init_params(num_dim, num_dim, rank) for i in range(local_num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 367,
          "function_name": "pipeline_parallelism_main",
          "code": "local_params = [get_init_params(num_dim, num_dim, rank) for i in range(local_num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 372,
          "function_name": "pipeline_parallelism_main",
          "code": "micro_batch_size = int_divide(batch_size, num_micro_batches)  # @inspect micro_batch_size"
        }
      ],
      "env": {
        "micro_batch_size": 32
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 373,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank == 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 375,
          "function_name": "pipeline_parallelism_main",
          "code": "micro_batches = data.chunk(chunks=num_micro_batches, dim=0)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 380,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 382,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 391,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 392,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline_parallelism] Rank {rank}: sending {summarize_tensor(x)} to rank {rank + 1}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 393,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 380,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 382,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 391,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 392,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline_parallelism] Rank {rank}: sending {summarize_tensor(x)} to rank {rank + 1}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 393,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 380,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 382,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 391,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 392,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline_parallelism] Rank {rank}: sending {summarize_tensor(x)} to rank {rank + 1}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 393,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 380,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 382,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 387,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 386,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in local_params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 391,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 392,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline_parallelism] Rank {rank}: sending {summarize_tensor(x)} to rank {rank + 1}\", flush=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 393,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 380,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 395,
          "function_name": "pipeline_parallelism_main",
          "code": "text(\"Not handled: overlapping communication/computation to eliminate pipeline bubbles\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Not handled: overlapping communication/computation to eliminate pipeline bubbles",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 399,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 399,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 399,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 399,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 352,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 44,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 46,
          "function_name": "main",
          "code": "text(\"What's missing?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What's missing?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 47,
          "function_name": "main",
          "code": "text(\"- More general models (with attention, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- More general models (with attention, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 48,
          "function_name": "main",
          "code": "text(\"- More communication/computation overlap\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- More communication/computation overlap",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 49,
          "function_name": "main",
          "code": "text(\"- This require more complex code with more bookkeeping\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- This require more complex code with more bookkeeping",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 50,
          "function_name": "main",
          "code": "text(\"- Jax/TPUs: just define the model, the sharding strategy, and the Jax compiler handles the rest \"), link(title=\"[levanter]\", url=\"https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Jax/TPUs: just define the model, the sharding strategy, and the Jax compiler handles the rest ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[levanter]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 51,
          "function_name": "main",
          "code": "text(\"- But we're doing PyTorch so you can see how one builds up from the primitives\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- But we're doing PyTorch so you can see how one builds up from the primitives",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 53,
          "function_name": "main",
          "code": "text(\"### Summary\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Summary",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 54,
          "function_name": "main",
          "code": "text(\"- Many ways to parallelize: data (batch), tensor/expert (width), pipeline (depth), sequence (length)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Many ways to parallelize: data (batch), tensor/expert (width), pipeline (depth), sequence (length)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 55,
          "function_name": "main",
          "code": "text(\"- Can **re-compute** or store in **memory** or store in another GPUs memory and **communicate**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Can **re-compute** or store in **memory** or store in another GPUs memory and **communicate**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 56,
          "function_name": "main",
          "code": "text(\"- Hardware is getting faster, but will always want bigger models, so will have this hierarchical structure\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Hardware is getting faster, but will always want bigger models, so will have this hierarchical structure",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    }
  ]
}