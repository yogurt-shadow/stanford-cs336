{
  "files": {
    "lecture_10.py": "from sympy import symbols, oo\nfrom execute_util import text, link, image\nfrom lecture_util import article_link\nfrom references import Reference, llama3, gqa, mla, longformer, sparse_transformer, mistral_7b\n\n# Define symbols corresponding to the shape of the Transformer model\nB, S, T, D, F, N, K, H, L, V = symbols(\"B S T D F N K H L V\", positive=True)\nc = symbols(\"c\", positive=True)  # Just a constant that helps with taking limits\nmemory_bandwidth = symbols(\"memory_bandwidth\", positive=True)\n\nscaling_book_transformers = Reference(title=\"[Scaling book chapter on Transformers]\", url=\"https://jax-ml.github.io/scaling-book/transformers/\")\nscaling_book_inference = Reference(title=\"[Scaling book chapter on Transformers]\", url=\"https://jax-ml.github.io/scaling-book/inference/\")\n\ndef main():\n    text(\"**Inference**: given a **fixed model**, generate responses given prompts\")\n\n    text(\"### Understanding the inference workload\")\n    landscape()\n    review_transformer()\n    review_of_arithmetic_intensity()\n    arithmetic_intensity_of_inference()\n    throughput_and_latency()\n\n    text(\"### Taking shortcuts (lossy)\")\n    reduce_kv_cache_size()\n    alternatives_to_the_transformer()\n    quantization()\n    model_pruning()\n\n    text(\"Summary: reduce inference complexity without hurting accuracy\")\n\n    text(\"From scratch recipe:\")\n    text(\"1. Define faster model architecture\")\n    text(\"2. Train faster model\")\n\n    text(\"Distillation recipe:\")\n    text(\"1. Define faster model architecture\")\n    text(\"2. Initialize weights using original model (which has a different architecture)\")\n    text(\"3. Repair faster model (distillation)\")\n\n    text(\"### Use shortcuts but double check (lossless)\")\n    speculative_sampling()\n\n    text(\"### Handling dynamic workloads\")\n    text(\"Batching over sequences in live traffic is tricky because:\")\n    text(\"1. Requests arrive at different times (waiting for batch is bad for early requests)\")\n    text(\"2. Sequences have shared prefixes (e.g., system prompts, generating multiple samples)\")\n    text(\"3. Sequences have different lengths (padding is inefficient)\")\n\n    continuous_batching()\n    paged_attention()\n\n    text(\"### Summary\")\n    text(\"- Inference is important (actual use, evaluation, reinforcement learning)\")\n    text(\"- Different characteristics compared to training (memory-limited, dynamic)\")\n    text(\"- Techniques: new architectures, quantization, pruning/distillation, speculative decoding\")\n    text(\"- Ideas from systems (speculative execution, paging)\")\n    text(\"- New architectures have huge potential for improvement\")\n\n\ndef landscape():\n    text(\"Inference shows up in many places:\")\n    text(\"- Actual use (chatbots, code completion, batch data processing)\")\n    text(\"- Model evaluation (e.g., on instruction following)\")\n    text(\"- Test-time compute (thinking requires more inference)\")\n    text(\"- Training via reinforcement learning (sample generation, then score)\")\n\n    text(\"Why **efficiency** matters: training is one-time cost, inference is repeated many times\")\n    image(\"images/openai-100b-tokens.png\", width=600); link(title=\" [tweet]\", url=\"https://x.com/sama/status/1756089361609981993\")\n    image(\"images/cursor-1b-lines.png\", width=600); link(title=\" [tweet]\", url=\"https://x.com/amanrsanger/status/1916968123535880684\")\n\n    text(\"Metrics:\")\n    text(\"- Time-to-first-token (TTFT): how long user waits before any generation happens (matters for interactive applications)\")\n    text(\"- Latency (seconds/token): how fast tokens appear for a user (matters for interactive applications)\")\n    text(\"- Throughput (tokens/second): useful for batch processing applications\")\n\n    text(\"Key considerations in efficiency:\")\n    text(\"- Training (supervised): you see all tokens, can parallelize over sequence (matmul in Transformer)\")\n    text(\"- Inference: you have to generate sequentially, can't parallelize, so harder to fully utilize compute\")\n\n    text(\"Companies doing inference (a big deal for anyone who has a product or platform):\")\n    text(\"- Providers serving closed models (OpenAI, Anthropic, Google, etc.)\")\n    text(\"- Providers serving open-weight models (Together, Fireworks, DeepInfra, etc.)\")\n\n    text(\"Open-source packages:\")\n    text(\"- vLLM (Berkeley) \"), link(title=\"[talk]\", url=\"https://www.youtube.com/watch?v=8BaEwoTk8XI\")\n    text(\"- Tensor-RT (NVIDIA) \"), article_link(\"https://nvidia.github.io/TensorRT-LLM/overview.html\")\n    text(\"- TGI (Hugging Face) \"), article_link(\"https://huggingface.co/docs/text-generation-inference/en/index\")\n\n\ndef review_transformer():\n    link(scaling_book_transformers)\n    image(\"https://jax-ml.github.io/scaling-book/assets/img/transformer-diagram.png\", width=800)\n    text(\"Simplifications (following conventions): `F = 4*D, D = N*H, N = K*G, S = T`\")\n    text(\"FLOPs for a feedforward pass: 6 * (B*T) * (num_params + O(T))\")\n\n\ndef review_of_arithmetic_intensity():\n    text(\"Setup: multiply X (B x D) and W (D x F) matrix\")\n    text(\"Intuition: B is batch size, D is hidden dimension, F is up-projection dimension in MLP\")\n\n    text(\"Let's do FLOPs and memory read/write accounting for the matrix multiplication (X * W).\")\n    flops = 0\n    bytes_transferred = 0\n\n    text(\"Steps:\")\n    text(\"1. Read X (B x D) from HBM\")\n    bytes_transferred += 2*B*D\n    text(\"2. Read W (D x F) from HBM\")\n    bytes_transferred += 2*D*F\n    text(\"3. Compute Y = X (B x D) @ W (D x F)\")\n    flops += 2*B*D*F\n    text(\"4. Write Y (B x F) to HBM\")\n    bytes_transferred += 2*B*F\n\n    text(\"Let's take stock of the accounting results.\")\n    assert flops == 2*B*D*F\n    assert bytes_transferred == 2*B*D + 2*D*F + 2*B*F\n    text(\"Recall that **arithmetic intensity** is how much compute we do per byte transferred (want to be high).\")\n    intensity = (flops / bytes_transferred).simplify()  # @inspect intensity\n\n    text(\"Assuming B is much less than D and F, then we can simplify:\")\n    intensity = intensity.subs(D, c*B).subs(F, c*B).limit(c, oo).simplify()  # @inspect intensity\n    assert intensity == B\n\n    text(\"Accelerator intensity of H100:\")\n    flops_per_second = 989e12\n    memory_bandwidth = 3.35e12\n    accelerator_intensity = flops_per_second / memory_bandwidth  # @inspect accelerator_intensity\n    assert round(accelerator_intensity) == 295\n\n    text(\"If computation intensity > accelerator intensity, **compute-limited** (good)\")\n    text(\"If computation intensity < accelerator intensity, **memory-limited** (bad)\")\n    text(\"Conclusion: compute-limited iff B > 295\")\n\n    text(\"Extreme case (B = 1, corresponding to matrix-vector product):\")\n    text(\"- Arithmetic intensity: 1\")\n    text(\"- Memory-limited (read D x F matrix, perform only 2*D*F FLOPs)\")\n    text(\"- This is basically what happens with generation...\")\n\n\ndef arithmetic_intensity_of_inference():\n    link(scaling_book_inference)\n\n    image(\"https://jax-ml.github.io/scaling-book/assets/img/naive-inference-1400.webp\", width=800)\n    text(\"Naive inference: to generate each token, feed history into Transformer\")\n    text(\"Complexity: generating T tokens requires O(T^3) FLOPs (one feedforward pass is O(T^2))\")\n\n    text(\"Observation: a lot of the work can be shared across prefixes\")\n    text(\"Solution: store **KV cache** in HBM\")\n    image(\"https://jax-ml.github.io/scaling-book/assets/img/cached-inference-1400.webp\", width=800)\n    text(\"KV cache: for every sequence (B), token (S), layer (L), head (K), store an H-dimensional vector\")\n\n    text(\"Two stages of inference:\")\n    text(\"1. **Prefill**: given a prompt, encode into vectors (parallelizable like in training)\")\n    text(\"2. **Generation**: generate new response tokens (sequential)\")\n\n    text(\"Let's compute the FLOPs and memory IO for both the MLP and attention layers.\")\n    text(\"S is the number of tokens we're conditioning on, T is the number of tokens we're generating.\")\n    text(\"Later, we'll specialize to prefill (T = S) and generation (T = 1).\")\n\n    text(\"### MLP layers (only looking at the matrix multiplications)\")\n    flops = 0\n    bytes_transferred = 0\n    text(\"Steps:\")\n    text(\"1. Read X (B x T x D) from HBM\")\n    bytes_transferred += 2*B*T*D\n    text(\"2. Read Wup (D x F), Wgate (D x F), Wdown (F x D) from HBM\")\n    bytes_transferred += 3 * 2*D*F\n    text(\"3. Compute U = X (B x T x D) @ Wup (D x F)\")\n    flops += 2*B*T*D*F\n    text(\"4. Write U (B x T x F) to HBM\")\n    bytes_transferred += 2*B*T*F\n    text(\"5. Compute G = X (B x T x F) @ Wgate (D x F)\")\n    flops += 2*B*T*D*F\n    text(\"6. Write G (B x T x F) to HBM\")\n    bytes_transferred += 2*B*T*F\n    text(\"7. Compute Y = GeLU(G)*U (B x T x F) @ Wdown (F x D)\")\n    flops += 2*B*T*D*F\n    text(\"8. Write Y (B x T x D) to HBM\")\n    bytes_transferred += 2*B*T*D\n\n    text(\"Let's take stock of the accounting results.\")\n    assert flops == 6*B*T*D*F\n    assert bytes_transferred == 4*B*T*D + 4*B*T*F + 6*D*F\n    intensity = (flops / bytes_transferred).simplify()  # @inspect intensity\n    text(\"Assume that B*T is much smaller than D and F.\")\n    intensity = intensity.subs(D, c*B*T).subs(F, c*B*T).limit(c, oo).simplify()  # @inspect intensity\n    assert intensity == B*T\n\n    text(\"For the two stages:\")\n    text(\"1. Prefill: easy to make compute-limited (good) by making B T large enough\")\n    text(\"2. Generation:\")\n    text(\"- Generating one token at a time (T = 1)\")\n    text(\"- B is number of concurrent requests, hard to make large enough!\")\n\n    text(\"### Attention layers (focusing on the matrix multiplications with FlashAttention)\")\n    flops = 0\n    bytes_transferred = 0\n    text(\"Steps:\")\n    text(\"1. Read Q (B x T x D), K (B x S x D), V (B x S x D) from HBM\")\n    bytes_transferred += 2*B*T*D + 2*B*S*D + 2*B*S*D\n    text(\"2. Compute A = Q (B x T x D) @ K (B x S x D)\")\n    flops += 2*B*S*T*D\n    text(\"3. Compute Y = softmax(A) (B x S x T x K x G) @ V (B x S x K x H)\")\n    flops += 2*B*S*T*D\n    text(\"4. Write Y (B x T x D) to HBM\")\n    bytes_transferred += 2*B*T*D\n\n    assert flops == 4*B*S*T*D\n    assert bytes_transferred == 4*B*S*D + 4*B*T*D\n    intensity = (flops / bytes_transferred).simplify()  # @inspect intensity\n    assert intensity == S*T / (S + T)\n\n    text(\"For the two stages:\")\n    text(\"1. Prefill: T = S\")\n    prefill_intensity = intensity.subs(T, S).simplify()  # @inspect prefill_intensity\n    assert prefill_intensity == S/2  # Good!\n    text(\"2. Generation: T = 1\")\n    generate_intensity = intensity.subs(T, 1).simplify()  # @inspect generate_intensity\n    assert generate_intensity < 1  # Bad!\n\n    text(\"Unlike MLPs, no dependence on B, so batching doesn't help!\")\n    text(\"Why?\")\n    text(\"- In MLP layers, every sequence hits the same MLP weights (Wup, Wgate, Wdown don't depend on B)\")\n    text(\"- In attention layers, every sequence has its own vectors KV cache (Q, K, V all depend on B)\")\n\n    text(\"Summary\")\n    text(\"- Prefill is compute-limited, generation is memory-limited\")\n    text(\"- MLP intensity is B (requires concurrent requests), attention intensity is 1 (impossible to improve)\")\n\n\ndef compute_transformer_stats(config):  # @inspect config\n    \"\"\"Return symbols corresponding to various statistics of a Transformer.\"\"\"\n    text(\"The memory, throughput, and latency depends on the shape of the Transformer. \"), text(\" \"), link(\"\")\n\n    text(\"Compute the number of parameters in the Transformer:\")\n    num_params = 2*V*D + D*F*3*L + (2*D*N*H + 2*D*K*H)*L\n    text(\"To store parameters, just use bf16 (training requires fp32)\")\n    parameter_size = num_params * 2  # 2 for bf16\n    \n    text(\"We also don't need gradients and optimizer states since we're not training.\")\n    text(\"But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):\")\n    text(\"How much we have to store per sequence:\")\n    kv_cache_size = S * (K*H) * L * 2 * 2  # 2 for key + value, 2 for bf16\n\n    text(\"Total memory usage:\")\n    memory = B * kv_cache_size + parameter_size\n    text(\"Latency is determined by memory IO (read all parameters and KV cache for each step)\")\n    latency = memory / memory_bandwidth\n    text(\"Throughput is the inverse of latency, but we're generating B tokens in parallel\")\n    throughput = B / latency\n\n    # Substitute\n    num_params = num_params.subs(config).simplify()  # @inspect num_params\n    memory = memory.subs(config).simplify()  # @inspect memory\n    latency = latency.subs(config).simplify()  # @inspect latency\n    throughput = throughput.subs(config).simplify()  # @inspect throughput\n\n    return num_params, memory, latency, throughput\n\ndef llama2_13b_config(args={}):\n    return {S: 1024, D: 5120, F: 13824, N: 40, K: 40, H: 128, L: 40, V: 32000, memory_bandwidth: 3.35e12, **args}\n\ndef throughput_and_latency():\n    text(\"So we have shown that inference is memory-limited.\")\n    text(\"Let us now compute the theoretical maximum latency and throughput of a single request.\")\n    text(\"Assumption: can overlap compute and communication perfectly and ignore various types of overhead.\")\n\n    text(\"Instantiate latency and throughput for Llama 2 13B on an H100:\")\n    config = llama2_13b_config()\n    num_params, memory, latency, throughput = compute_transformer_stats(config)\n\n    text(\"If we use a batch size of 1:\")\n    bs1_memory = memory.subs(B, 1).simplify()   # @inspect bs1_memory\n    bs1_latency = latency.subs(B, 1).simplify()   # @inspect bs1_latency\n    bs1_throughput = throughput.subs(B, 1).simplify()   # @inspect bs1_throughput\n\n    text(\"If we use a batch size of 64 (worse latency, better throughput):\")\n    bs64_memory = memory.subs(B, 64).simplify()   # @inspect bs64_memory\n    bs64_latency = latency.subs(B, 64).simplify()   # @inspect bs64_latency\n    bs64_throughput = throughput.subs(B, 64).simplify()   # @inspect bs64_throughput\n\n    text(\"If we use a batch size of 256:\")\n    bs256_memory = memory.subs(B, 256).simplify()   # @inspect bs256_memory\n    bs256_latency = latency.subs(B, 256).simplify()   # @inspect bs256_latency\n    bs256_throughput = throughput.subs(B, 256).simplify()   # @inspect bs256_throughput\n    text(\"Doesn't fit into memory, but throughput gains are diminishing too...\")\n\n    text(\"**Tradeoff** between latency and throughput:\")\n    text(\"1. Smaller batch sizes yields better latency but worse throughput\")\n    text(\"2. Larger batch sizes yields better throughput but worse latency\")\n\n    text(\"Easy parallelism: if you launch M copies of the model, latency is the same, throughput increases by M!\")\n    text(\"Harder parallelism: shard the model and the KV cache \"), link(scaling_book_inference)\n\n    text(\"Note: time-to-first-token (TTFT) is essentially a function of prefill\")\n    text(\"Use smaller batch sizes during prefill for faster TTFT\")\n    text(\"Use larger batch sizes during generation to improve throughput\")\n\n\ndef reduce_kv_cache_size():\n    text(\"Recall that memory is the bottleneck for inference.\")\n    text(\"So let's try to reduce the size of the KV cache\")\n    text(\"...but make sure we don't lose too much accuracy.\")\n\n    text(\"### Grouped-query attention (GQA) \"), link(gqa)\n    image(\"https://jax-ml.github.io/scaling-book/assets/img/gmqa.png\", width=800)\n    text(\"Idea: N query heads, but only K key and value heads, each interacting with N/K query heads\")\n    text(\"Multi-headed attention (MHA): K=N\")\n    text(\"Multi-query attention (MQA): K=1\")\n    text(\"Group-query attention (GQA): K is somewhere in between\")\n\n    text(\"Latency/throughput improvements:\")\n    image(\"images/gqa-speed.png\", width=500); text(\" \"); link(gqa)\n    text(\"Reduce the KV cache by a factor of N/K\")\n    config = llama2_13b_config({K: 40, B: 64})  # Original Llama 2 13B\n    k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput\n\n    config = llama2_13b_config({K: 8, B: 64})  # Use GQA with 1:5 ratio\n    k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput\n\n    text(\"This also means we can use a larger batch size:\")\n    config = llama2_13b_config({K: 8, B: 256})  # Increase batch size\n    k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput\n    text(\"Worse latency, but better throughput (and it fits in memory now!).\")\n\n    text(\"Check that accuracy doesn't drop: \"); link(gqa)\n    image(\"images/gqa-accuracy.png\", width=800)\n\n    text(\"### Multi-head latent attention (MLA) \"), link(mla)\n    image(\"images/mla-schema.png\", width=800)\n    text(\"Key idea: project down each key and value vector from N*H dimensions to C dimensions\")\n    text(\"DeepSeek v2: reduce N*H = 16384 to C = 512\")\n    text(\"Wrinkle: MLA is not compatible with RoPE, so need to add additional 64 dimensions for RoPE, so 512 + 64 = 576 total dimensions\")\n    text(\"Latency/throughput improvements follow similarly from the KV cache reduction as argued earlier\")\n\n    text(\"Let's now check the accuracy.\")\n    text(\"First, MHA is better than GQA (though more expensive) [Table 8] \"); link(mla)\n    image(\"images/mla-accuracy.png\", width=800)\n    text(\"Second, MLA is a bit better than MHA (and much cheaper) [Table 9] \"); link(mla)\n    image(\"images/mla-accuracy2.png\", width=800)\n\n    text(\"### Cross-layer attention (CLA) \"), link(\"https://arxiv.org/abs/2405.12981\")\n    image(\"images/cla-diagram.png\", width=500)\n    text(\"Idea: share KVs across **layers** (just as GQA shares KVs across heads)\")\n    text(\"Empirically improves the pareto frontier of accuracy and KV cache size (latency and throughput)\")\n    image(\"images/cla-results.png\", width=700)\n\n    text(\"### Local attention \"), link(longformer), link(sparse_transformer), link(mistral_7b)\n    image(\"images/longformer-attention.png\", width=800)\n    text(\"Idea: just look at the local context, which is most relevant for modeling\")\n    text(\"Effective context scales linearly with the number of layers\")\n    text(\"KV cache is independent of sequence length!\")\n\n    text(\"Problem: this can still hurt accuracy\")\n    text(\"Solution: interleave local attention with global attention (hybrid layers)\")\n    text(\"Example: character.ai uses 1 global layer every 6 layers (in addition to CLA) \"), article_link(\"https://research.character.ai/optimizing-inference/\")\n    image(\"https://research.character.ai/content/images/2024/06/figure1-2-1.png\", width=800)\n\n    text(\"Summary:\")\n    text(\"- Goal: reduce the KV cache size (since inference is memory-limited) without hurting accuracy\")\n    text(\"- Lower-dimensional KV cache (GQA, MLA, shared KV cache)\")\n    text(\"- Local attention on some of the layers\")\n\n\ndef alternatives_to_the_transformer():\n    text(\"We have shown that tweaking the architecture of the Transformer, we can improve latency and throughput.\")\n    text(\"Attention + autoregression is fundamentally memory-limited (Transformers were not designed with inference in mind).\")\n    text(\"Can we substantially improve things if we go beyond the Transformer?\")\n    text(\"We will discuss two directions: state-space models and diffusion models.\")\n\n    text(\"## State-space models\")\n    link(title=\"[presentation from CS229S]\", url=\"https://docs.google.com/presentation/d/1wrQO4uzwWr73SGj7aFxeVR9Cz0PY-mzJipn12enM39k/edit#slide=id.p\")\n    text(\"- Idea: from signal processing to model long-context sequences in a sub-quadratic time\")\n    text(\"- S4: based on classic state space models, good at synthetic long-context tasks \"), link(\"https://arxiv.org/abs/2111.00396\")\n    image(\"images/s4-summary.png\", width=800)\n    text(\"- Weaknesses: bad at solving associative recall tasks important for language (where Transformers do well)\")\n    image(\"images/based-associative-recall.png\", width=400)\n    text(\"- Mamba: allow SSM parameters to be input-dependent, match Transformers at 1B scale \"), link(\"https://arxiv.org/abs/2312.00752\")\n    text(\"- Jamba: interleave Transformer-Mamba layers (1:7 ratio) with a 52B MoE \"), link(\"https://arxiv.org/abs/2403.19887\")\n    image(\"images/jamba-architecture.png\", width=400)\n    text(\"- BASED: use linear attention + local attention \"), link(\"https://arxiv.org/abs/2402.18668\")\n    image(\"images/based-attention.png\", width=400)\n    text(\"- MiniMax-01: use linear attention + full attention (456B parameter MoE) \"), link(\"https://arxiv.org/pdf/2501.08313\")\n\n    text(\"Takeaways:\")\n    text(\"- Linear + local attention (still need some full attention) yield serious SOTA models\")\n    text(\"- Replace O(T) KV cache with O(1) state => much more efficient for inference\")\n\n    text(\"### Diffusion models\")\n    text(\"- Popular for image generation, but harder to get working for text generation \"), link(\"https://arxiv.org/abs/2205.14217\")\n    image(\"images/diffusion-lm.png\", width=700)\n    text(\"- Idea: generate each token in parallel (not autoregressively), refine multiple time steps\")\n    text(\"- Start with random noise (over entire sequence), iteratively refine it\")\n    text(\"- Results from Inception Labs \"), article_link(\"https://www.inceptionlabs.ai/news\")\n    link(title=\"[demo video]\", url=\"https://x.com/i/status/1894847919624462794\")\n    text(\"Much faster on coding benchmarks:\")\n    image(\"https://framerusercontent.com/images/K2zvhtaTsz5ehDFoWx6KQHOqCyk.jpg\", width=800)\n\n    text(\"Overall, significant gains in inference to be made with more radical architecture changes!\")\n\n\ndef quantization():\n    text(\"Key idea: reduce the precision of numbers\")\n    text(\"Less memory means higher latency/throughput (since inference is memory-limited).\")\n    text(\"Of course we have to worry about accuracy...\")\n\n    image(\"https://www.datocms-assets.com/104802/1709770809-twitter-post-20.png\", width=400), article_link(\"https://www.baseten.co/blog/fp8-efficient-model-inference-with-8-bit-floating-point-numbers/\")\n    text(\"- fp32 (4 bytes): needed for parameters and optimizer states during training\")\n    text(\"- bf16 (2 bytes): default for inference\")\n    text(\"- fp8 (1 byte) [-240, 240] for e4m3 on H100s: can train if you dare \"), link(\"https://arxiv.org/pdf/2310.18313\")\n    text(\"- int8 (1 byte) [-128, 127]: less accurate but cheaper than fp8, but for inference only \"), link(\"https://arxiv.org/pdf/2303.17951\")\n    text(\"- int4 (0.5 bytes) [-8, 7]: cheaper, even less accurate \"), link(\"https://arxiv.org/pdf/2303.17951\")\n\n    text(\"Quantization-aware training (QAT): train with quantization, but doesn't scale up\")\n    text(\"Post-training quantization (PTQ): run on sample data to determine scale and zero point for each layer or tensor\")\n    link(title=\"[Overview of approaches]\", url=\"https://apxml.com/posts/llm-quantization-techniques-explained\")\n\n    text(\"### LLM.int8()\")\n    link(\"https://arxiv.org/abs/2208.07339\"), article_link(\"https://huggingface.co/blog/hf-bitsandbytes-integration\")\n    text(\"Standard quantization (scale by max of absolute values):\")\n    image(\"https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/quant-freeze.png\", width=500)\n    text(\"Problem: outliers (which appear in larger networks) screw everything up\")\n    text(\"Solution: extract outliers and process them in fp16\")\n    image(\"https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/Mixed-int8.gif\", width=600)\n    text(\"It works well (but is 15-23% slower than fp16):\")\n    image(\"images/llm-int8-bloom.png\", width=500)\n\n    text(\"### Activation-aware quantization\")\n    link(\"https://arxiv.org/abs/2306.00978\")\n    text(\"Idea: select which weights (0.1-1%) to keep in high precision based on activations\")\n    text(\"fp16 -> int3 produces 4x lower memory, 3.2x speedup\")\n    image(\"images/awq-schema.png\", width=800)\n\n\ndef model_pruning():\n    text(\"Key idea: just rip out parts of an expensive model to make it cheaper\")\n    text(\"...and then fix it up.\")\n\n    text(\"Paper from NVIDIA \"), link(\"https://arxiv.org/abs/2407.14679\")\n    image(\"images/pruning-kd-loop.png\", width=600)\n    text(\"Algorithm:\")\n    text(\"1. Identify important {layer, head, hidden dimension} on a small calibration dataset (1024 samples)\")\n    text(\"2. Remove unimportant layers to get a smaller model\")\n    text(\"3. Distill the original model into pruned model\")\n\n    text(\"Results:\")\n    image(\"images/pruning-kd.png\", width=500)\n\n\ndef speculative_sampling():\n    text(\"Recall the two stages of inference:\")\n    text(\"- Prefill: given a sequence, encode tokens in parallel (compute-limited) [note: also gives you probabilities]\")\n    text(\"- Generation: generate one token at a time (memory-limited)\")\n    text(\"In other words, checking is faster than generation.\")\n\n    text(\"Speculative sampling \"); link(\"https://arxiv.org/abs/2211.17192\"); link(\"https://arxiv.org/abs/2302.01318\")\n    text(\"- Use a cheaper **draft model** p to guess a few tokens (e.g., 4)\")\n    text(\"- Evaluate with target model q (process tokens in parallel), and accept if it looks good\")\n    link(title=\"[Speculative sampling video]\", url=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeculativeDecoding-1-Illustration.mp4\")\n    article_link(\"https://research.google/blog/looking-back-at-speculative-decoding/\")\n\n    image(\"images/speculative-sampling-algorithm.png\", width=600)\n    text(\"This is modified rejection sampling with proposal p and target q\")\n    text(\"Modification: always generate at least one candidate (rejection sampling will keep looping)\")\n    text(\"Key property: guaranteed to be an **exact sample** from the target model!\")\n\n    text(\"Proof by example: assume two vocabulary elements {A, B}\")\n    text(\"- Target model probabilities: [q(A), q(B)]\")\n    text(\"- Draft model probabilities: [p(A), p(B)]\")\n    text(\"- Assume p(A) > q(A) [draft model oversamples A].\")\n    text(\"- Therefore p(B) < q(B) [draft model undersamples B].\")\n    text(\"- Residual probabilities max(q-p, 0): [0, 1]\")\n    text(\"Compute the probabilities of speculatively sampling a token:\")\n    text(\"- P[sampling A] = p(A) * (q(A) / p(A)) + p(B) * 1 * 0 = q(A)\")\n    text(\"- P[sampling B] = p(B) * 1 + p(A) * (1 - q(A) / p(A)) * 1 = q(B)\")\n\n    image(\"images/speculative-sampling-results.png\", width=600)\n    image(\"images/speculative-sampling-stats.png\", width=600)\n\n    text(\"In practice:\")\n    text(\"- Target model has 70B parameters, draft model has 8B parameters\")\n    text(\"- Target model has 8B parameters, draft model has 1B parameters\")\n    text(\"- Try to make draft model as close to target (distillation)\")\n\n    text(\"Extensions to improve the draft model:\")\n    text(\"- Medusa: draft model generates multiple tokens in parallel \"), link(\"https://arxiv.org/abs/2401.10774\")\n    text(\"- EAGLE: draft model takes high-level features from target model \"), link(\"https://arxiv.org/pdf/2401.15077\")\n    image(\"images/medusa-eagle.png\", width=600)\n\n    text(\"Summary:\")\n    text(\"- Exact sampling from target model (thanks to math)!\")\n    text(\"- Exploits asymmetry between checking and generation\")\n    text(\"- Lots of room for innovation on the draft model (involves training)\")\n\n\ndef continuous_batching():\n    link(title=\"Orca: A Distributed Serving System for Transformer-Based Generative Models\", url=\"https://www.usenix.org/system/files/osdi22-yu.pdf\"), link(title=\"[talk]\", url=\"https://www.youtube.com/watch?v=Ob9PPLxETYU\")\n\n    text(\"Problem:\")\n    text(\"- Training: get a dense block of tokens (batch size x sequence length)\")\n    text(\"- Inference: requests arrive and finish at different times, so you have a ragged array\")\n    image(\"https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png\", width=600)\n\n    text(\"Solution: iteration-level scheduling\")\n    text(\"- Decode step by step\")\n    text(\"- Add new requests to the batch as they arrive (so don't have to wait until generation completes)\")\n\n    text(\"Problem:\")\n    text(\"- Batching only works when all sequences have the same dimensionality (right?)\")\n    text(\"- But each request might have a different length\")\n\n    text(\"Solution: selective batching\")\n    text(\"- Training: when all sequences of the same length, operate on a B x S x H tensor\")\n    text(\"- But we might have different lengths: [3, H], [9, H], [5, H], etc.\")\n    text(\"- Attention computation: process each sequence separately\")\n    text(\"- Non-attention computation: concatenate all the sequences together to [3 + 9 + 5, H]\")\n\n\ndef paged_attention():\n    text(\"Paper that introduced vLLM in addition to PagedAttention \"), link(\"https://arxiv.org/pdf/2309.06180.pdf\")\n\n    text(\"Previous status quo:\")\n    text(\"- Request comes in\")\n    text(\"- Allocate section of KV cache for prompt and response (up to a max length)\")\n    image(\"images/paged-attention-fragmentation.png\", width=800)\n    text(\"Problem: fragmentation (what happens to your hard drive)\")\n    text(\"- But this is wasteful since we might generate much fewer tokens (internal fragmentation)!\")\n    text(\"- Might be extra unused space between sections (external fragmentation)!\")\n\n    text(\"Solution: PagedAttention (remember operating systems)\")\n    text(\"- Divide the KV cache of a sequence into non-contiguous **blocks**\")\n    image(\"images/paged-attention-blocks.png\", width=400)\n\n    text(\"Two requests share the KV caches:\")\n    image(\"images/paged-attention-logical.png\", width=800)\n\n    text(\"In general, multiples types of sharing KV caches across sequences:\")\n    image(\"images/paged-attention-sharing.png\", width=600)\n    text(\"- Sharing the system prompt\")\n    text(\"- Sampling multiple responses per prompt (e.g., for program synthesis)\")\n\n    text(\"Solution: share prefixes, copy-on-write at the block level\")\n    image(\"images/paged-attention-parallel.png\", width=600)\n\n    text(\"Other vLLM optimizations:\")\n    text(\"- Kernel to fuse block read and attention (reduce kernel launch overhead)\")\n    text(\"- Use latest kernels (FlashAttention, FlashDecoding)\")\n    text(\"- Use CUDA graphs to avoid kernel launch overhead\")\n\n    text(\"Summary: use ideas from operating systems (paging) to make use of memory for dynamic workloads\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "steps": [
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 14,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 15,
          "function_name": "main",
          "code": "text(\"**Inference**: given a **fixed model**, generate responses given prompts\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Inference**: given a **fixed model**, generate responses given prompts",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 17,
          "function_name": "main",
          "code": "text(\"### Understanding the inference workload\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Understanding the inference workload",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 61,
          "function_name": "landscape",
          "code": "def landscape():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 62,
          "function_name": "landscape",
          "code": "text(\"Inference shows up in many places:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Inference shows up in many places:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 63,
          "function_name": "landscape",
          "code": "text(\"- Actual use (chatbots, code completion, batch data processing)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Actual use (chatbots, code completion, batch data processing)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 64,
          "function_name": "landscape",
          "code": "text(\"- Model evaluation (e.g., on instruction following)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model evaluation (e.g., on instruction following)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 65,
          "function_name": "landscape",
          "code": "text(\"- Test-time compute (thinking requires more inference)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Test-time compute (thinking requires more inference)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 66,
          "function_name": "landscape",
          "code": "text(\"- Training via reinforcement learning (sample generation, then score)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Training via reinforcement learning (sample generation, then score)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 68,
          "function_name": "landscape",
          "code": "text(\"Why **efficiency** matters: training is one-time cost, inference is repeated many times\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Why **efficiency** matters: training is one-time cost, inference is repeated many times",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 69,
          "function_name": "landscape",
          "code": "image(\"images/openai-100b-tokens.png\", width=600); link(title=\" [tweet]\", url=\"https://x.com/sama/status/1756089361609981993\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/openai-100b-tokens.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [tweet]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://x.com/sama/status/1756089361609981993",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 70,
          "function_name": "landscape",
          "code": "image(\"images/cursor-1b-lines.png\", width=600); link(title=\" [tweet]\", url=\"https://x.com/amanrsanger/status/1916968123535880684\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/cursor-1b-lines.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [tweet]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://x.com/amanrsanger/status/1916968123535880684",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 72,
          "function_name": "landscape",
          "code": "text(\"Metrics:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Metrics:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 73,
          "function_name": "landscape",
          "code": "text(\"- Time-to-first-token (TTFT): how long user waits before any generation happens (matters for interactive applications)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Time-to-first-token (TTFT): how long user waits before any generation happens (matters for interactive applications)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 74,
          "function_name": "landscape",
          "code": "text(\"- Latency (seconds/token): how fast tokens appear for a user (matters for interactive applications)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Latency (seconds/token): how fast tokens appear for a user (matters for interactive applications)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 75,
          "function_name": "landscape",
          "code": "text(\"- Throughput (tokens/second): useful for batch processing applications\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Throughput (tokens/second): useful for batch processing applications",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 77,
          "function_name": "landscape",
          "code": "text(\"Key considerations in efficiency:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key considerations in efficiency:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 78,
          "function_name": "landscape",
          "code": "text(\"- Training (supervised): you see all tokens, can parallelize over sequence (matmul in Transformer)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Training (supervised): you see all tokens, can parallelize over sequence (matmul in Transformer)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 79,
          "function_name": "landscape",
          "code": "text(\"- Inference: you have to generate sequentially, can't parallelize, so harder to fully utilize compute\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Inference: you have to generate sequentially, can't parallelize, so harder to fully utilize compute",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 81,
          "function_name": "landscape",
          "code": "text(\"Companies doing inference (a big deal for anyone who has a product or platform):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Companies doing inference (a big deal for anyone who has a product or platform):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 82,
          "function_name": "landscape",
          "code": "text(\"- Providers serving closed models (OpenAI, Anthropic, Google, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Providers serving closed models (OpenAI, Anthropic, Google, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 83,
          "function_name": "landscape",
          "code": "text(\"- Providers serving open-weight models (Together, Fireworks, DeepInfra, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Providers serving open-weight models (Together, Fireworks, DeepInfra, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 85,
          "function_name": "landscape",
          "code": "text(\"Open-source packages:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Open-source packages:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 86,
          "function_name": "landscape",
          "code": "text(\"- vLLM (Berkeley) \"), link(title=\"[talk]\", url=\"https://www.youtube.com/watch?v=8BaEwoTk8XI\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- vLLM (Berkeley) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[talk]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.youtube.com/watch?v=8BaEwoTk8XI",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 87,
          "function_name": "landscape",
          "code": "text(\"- Tensor-RT (NVIDIA) \"), article_link(\"https://nvidia.github.io/TensorRT-LLM/overview.html\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tensor-RT (NVIDIA) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://nvidia.github.io/TensorRT-LLM/overview.html",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 88,
          "function_name": "landscape",
          "code": "text(\"- TGI (Hugging Face) \"), article_link(\"https://huggingface.co/docs/text-generation-inference/en/index\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- TGI (Hugging Face) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/docs/text-generation-inference/en/index",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 18,
          "function_name": "main",
          "code": "landscape()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 19,
          "function_name": "main",
          "code": "review_transformer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 19,
          "function_name": "main",
          "code": "review_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 91,
          "function_name": "review_transformer",
          "code": "def review_transformer():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 19,
          "function_name": "main",
          "code": "review_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 92,
          "function_name": "review_transformer",
          "code": "link(scaling_book_transformers)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Scaling book chapter on Transformers]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://jax-ml.github.io/scaling-book/transformers/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 19,
          "function_name": "main",
          "code": "review_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 93,
          "function_name": "review_transformer",
          "code": "image(\"https://jax-ml.github.io/scaling-book/assets/img/transformer-diagram.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-2a272a1a4f8f6ddbe5aaa45fcb38ed57-https_jax-ml_github_io_scaling-book_assets_img_transformer-diagram_png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 19,
          "function_name": "main",
          "code": "review_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 94,
          "function_name": "review_transformer",
          "code": "text(\"Simplifications (following conventions): `F = 4*D, D = N*H, N = K*G, S = T`\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Simplifications (following conventions): `F = 4*D, D = N*H, N = K*G, S = T`",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 19,
          "function_name": "main",
          "code": "review_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 95,
          "function_name": "review_transformer",
          "code": "text(\"FLOPs for a feedforward pass: 6 * (B*T) * (num_params + O(T))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "FLOPs for a feedforward pass: 6 * (B*T) * (num_params + O(T))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 19,
          "function_name": "main",
          "code": "review_transformer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 98,
          "function_name": "review_of_arithmetic_intensity",
          "code": "def review_of_arithmetic_intensity():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 99,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Setup: multiply X (B x D) and W (D x F) matrix\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Setup: multiply X (B x D) and W (D x F) matrix",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 100,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Intuition: B is batch size, D is hidden dimension, F is up-projection dimension in MLP\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuition: B is batch size, D is hidden dimension, F is up-projection dimension in MLP",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 102,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Let's do FLOPs and memory read/write accounting for the matrix multiplication (X * W).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's do FLOPs and memory read/write accounting for the matrix multiplication (X * W).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 103,
          "function_name": "review_of_arithmetic_intensity",
          "code": "flops = 0"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 104,
          "function_name": "review_of_arithmetic_intensity",
          "code": "bytes_transferred = 0"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 106,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Steps:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Steps:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 107,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"1. Read X (B x D) from HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Read X (B x D) from HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 108,
          "function_name": "review_of_arithmetic_intensity",
          "code": "bytes_transferred += 2*B*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 109,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"2. Read W (D x F) from HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Read W (D x F) from HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 110,
          "function_name": "review_of_arithmetic_intensity",
          "code": "bytes_transferred += 2*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 111,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"3. Compute Y = X (B x D) @ W (D x F)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Compute Y = X (B x D) @ W (D x F)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 112,
          "function_name": "review_of_arithmetic_intensity",
          "code": "flops += 2*B*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 113,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"4. Write Y (B x F) to HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "4. Write Y (B x F) to HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 114,
          "function_name": "review_of_arithmetic_intensity",
          "code": "bytes_transferred += 2*B*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 116,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Let's take stock of the accounting results.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's take stock of the accounting results.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 117,
          "function_name": "review_of_arithmetic_intensity",
          "code": "assert flops == 2*B*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 118,
          "function_name": "review_of_arithmetic_intensity",
          "code": "assert bytes_transferred == 2*B*D + 2*D*F + 2*B*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 119,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Recall that **arithmetic intensity** is how much compute we do per byte transferred (want to be high).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall that **arithmetic intensity** is how much compute we do per byte transferred (want to be high).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 120,
          "function_name": "review_of_arithmetic_intensity",
          "code": "intensity = (flops / bytes_transferred).simplify()  # @inspect intensity"
        }
      ],
      "env": {
        "intensity": "B*D*F/(B*D + B*F + D*F)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 122,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Assuming B is much less than D and F, then we can simplify:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Assuming B is much less than D and F, then we can simplify:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 123,
          "function_name": "review_of_arithmetic_intensity",
          "code": "intensity = intensity.subs(D, c*B).subs(F, c*B).limit(c, oo).simplify()  # @inspect intensity"
        }
      ],
      "env": {
        "intensity": "B"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 124,
          "function_name": "review_of_arithmetic_intensity",
          "code": "assert intensity == B"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 126,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Accelerator intensity of H100:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Accelerator intensity of H100:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 127,
          "function_name": "review_of_arithmetic_intensity",
          "code": "flops_per_second = 989e12"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 128,
          "function_name": "review_of_arithmetic_intensity",
          "code": "memory_bandwidth = 3.35e12"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 129,
          "function_name": "review_of_arithmetic_intensity",
          "code": "accelerator_intensity = flops_per_second / memory_bandwidth  # @inspect accelerator_intensity"
        }
      ],
      "env": {
        "accelerator_intensity": 295.2238805970149
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 130,
          "function_name": "review_of_arithmetic_intensity",
          "code": "assert round(accelerator_intensity) == 295"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 132,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"If computation intensity > accelerator intensity, **compute-limited** (good)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If computation intensity > accelerator intensity, **compute-limited** (good)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 133,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"If computation intensity < accelerator intensity, **memory-limited** (bad)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If computation intensity < accelerator intensity, **memory-limited** (bad)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 134,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Conclusion: compute-limited iff B > 295\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Conclusion: compute-limited iff B > 295",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 136,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"Extreme case (B = 1, corresponding to matrix-vector product):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Extreme case (B = 1, corresponding to matrix-vector product):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 137,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"- Arithmetic intensity: 1\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Arithmetic intensity: 1",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 138,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"- Memory-limited (read D x F matrix, perform only 2*D*F FLOPs)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Memory-limited (read D x F matrix, perform only 2*D*F FLOPs)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 139,
          "function_name": "review_of_arithmetic_intensity",
          "code": "text(\"- This is basically what happens with generation...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- This is basically what happens with generation...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_of_arithmetic_intensity()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 142,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "def arithmetic_intensity_of_inference():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 143,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "link(scaling_book_inference)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Scaling book chapter on Transformers]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://jax-ml.github.io/scaling-book/inference/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 145,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "image(\"https://jax-ml.github.io/scaling-book/assets/img/naive-inference-1400.webp\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-7ba69ac6feff08da76241c8e1dcf334c-https_jax-ml_github_io_scaling-book_assets_img_naive-inference-1400_webp",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 146,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Naive inference: to generate each token, feed history into Transformer\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Naive inference: to generate each token, feed history into Transformer",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 147,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Complexity: generating T tokens requires O(T^3) FLOPs (one feedforward pass is O(T^2))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Complexity: generating T tokens requires O(T^3) FLOPs (one feedforward pass is O(T^2))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 149,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Observation: a lot of the work can be shared across prefixes\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Observation: a lot of the work can be shared across prefixes",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 150,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Solution: store **KV cache** in HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Solution: store **KV cache** in HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 151,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "image(\"https://jax-ml.github.io/scaling-book/assets/img/cached-inference-1400.webp\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-1a4873c436f3fbe1d863837d3b38a114-https_jax-ml_github_io_scaling-book_assets_img_cached-inference-1400_webp",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 152,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"KV cache: for every sequence (B), token (S), layer (L), head (K), store an H-dimensional vector\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "KV cache: for every sequence (B), token (S), layer (L), head (K), store an H-dimensional vector",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 154,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Two stages of inference:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Two stages of inference:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 155,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"1. **Prefill**: given a prompt, encode into vectors (parallelizable like in training)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. **Prefill**: given a prompt, encode into vectors (parallelizable like in training)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 156,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"2. **Generation**: generate new response tokens (sequential)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. **Generation**: generate new response tokens (sequential)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 158,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Let's compute the FLOPs and memory IO for both the MLP and attention layers.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's compute the FLOPs and memory IO for both the MLP and attention layers.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 159,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"S is the number of tokens we're conditioning on, T is the number of tokens we're generating.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "S is the number of tokens we're conditioning on, T is the number of tokens we're generating.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 160,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Later, we'll specialize to prefill (T = S) and generation (T = 1).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Later, we'll specialize to prefill (T = S) and generation (T = 1).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 162,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"### MLP layers (only looking at the matrix multiplications)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### MLP layers (only looking at the matrix multiplications)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 163,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "flops = 0"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 164,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred = 0"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 165,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Steps:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Steps:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 166,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"1. Read X (B x T x D) from HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Read X (B x T x D) from HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 167,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred += 2*B*T*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 168,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"2. Read Wup (D x F), Wgate (D x F), Wdown (F x D) from HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Read Wup (D x F), Wgate (D x F), Wdown (F x D) from HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 169,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred += 3 * 2*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 170,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"3. Compute U = X (B x T x D) @ Wup (D x F)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Compute U = X (B x T x D) @ Wup (D x F)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 171,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "flops += 2*B*T*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 172,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"4. Write U (B x T x F) to HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "4. Write U (B x T x F) to HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 173,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred += 2*B*T*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 174,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"5. Compute G = X (B x T x F) @ Wgate (D x F)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "5. Compute G = X (B x T x F) @ Wgate (D x F)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 175,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "flops += 2*B*T*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 176,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"6. Write G (B x T x F) to HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "6. Write G (B x T x F) to HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 177,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred += 2*B*T*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 178,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"7. Compute Y = GeLU(G)*U (B x T x F) @ Wdown (F x D)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "7. Compute Y = GeLU(G)*U (B x T x F) @ Wdown (F x D)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 179,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "flops += 2*B*T*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 180,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"8. Write Y (B x T x D) to HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "8. Write Y (B x T x D) to HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 181,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred += 2*B*T*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 183,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Let's take stock of the accounting results.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's take stock of the accounting results.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 184,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert flops == 6*B*T*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 185,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert bytes_transferred == 4*B*T*D + 4*B*T*F + 6*D*F"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 186,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "intensity = (flops / bytes_transferred).simplify()  # @inspect intensity"
        }
      ],
      "env": {
        "intensity": "3*B*D*F*T/(2*B*D*T + 2*B*F*T + 3*D*F)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 187,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Assume that B*T is much smaller than D and F.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Assume that B*T is much smaller than D and F.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 188,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "intensity = intensity.subs(D, c*B*T).subs(F, c*B*T).limit(c, oo).simplify()  # @inspect intensity"
        }
      ],
      "env": {
        "intensity": "B*T"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 189,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert intensity == B*T"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 191,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"For the two stages:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "For the two stages:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 192,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"1. Prefill: easy to make compute-limited (good) by making B T large enough\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Prefill: easy to make compute-limited (good) by making B T large enough",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 193,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"2. Generation:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Generation:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 194,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"- Generating one token at a time (T = 1)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Generating one token at a time (T = 1)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 195,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"- B is number of concurrent requests, hard to make large enough!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- B is number of concurrent requests, hard to make large enough!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 197,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"### Attention layers (focusing on the matrix multiplications with FlashAttention)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Attention layers (focusing on the matrix multiplications with FlashAttention)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 198,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "flops = 0"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 199,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred = 0"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 200,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Steps:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Steps:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 201,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"1. Read Q (B x T x D), K (B x S x D), V (B x S x D) from HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Read Q (B x T x D), K (B x S x D), V (B x S x D) from HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 202,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred += 2*B*T*D + 2*B*S*D + 2*B*S*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 203,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"2. Compute A = Q (B x T x D) @ K (B x S x D)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Compute A = Q (B x T x D) @ K (B x S x D)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 204,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "flops += 2*B*S*T*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 205,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"3. Compute Y = softmax(A) (B x S x T x K x G) @ V (B x S x K x H)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Compute Y = softmax(A) (B x S x T x K x G) @ V (B x S x K x H)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 206,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "flops += 2*B*S*T*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 207,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"4. Write Y (B x T x D) to HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "4. Write Y (B x T x D) to HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 208,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "bytes_transferred += 2*B*T*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 210,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert flops == 4*B*S*T*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 211,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert bytes_transferred == 4*B*S*D + 4*B*T*D"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 212,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "intensity = (flops / bytes_transferred).simplify()  # @inspect intensity"
        }
      ],
      "env": {
        "intensity": "S*T/(S + T)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 213,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert intensity == S*T / (S + T)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 215,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"For the two stages:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "For the two stages:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 216,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"1. Prefill: T = S\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Prefill: T = S",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 217,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "prefill_intensity = intensity.subs(T, S).simplify()  # @inspect prefill_intensity"
        }
      ],
      "env": {
        "prefill_intensity": "S/2"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 218,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert prefill_intensity == S/2  # Good!"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 219,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"2. Generation: T = 1\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Generation: T = 1",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 220,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "generate_intensity = intensity.subs(T, 1).simplify()  # @inspect generate_intensity"
        }
      ],
      "env": {
        "generate_intensity": "S/(S + 1)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 221,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "assert generate_intensity < 1  # Bad!"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 223,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Unlike MLPs, no dependence on B, so batching doesn't help!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Unlike MLPs, no dependence on B, so batching doesn't help!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 224,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Why?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Why?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 225,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"- In MLP layers, every sequence hits the same MLP weights (Wup, Wgate, Wdown don't depend on B)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- In MLP layers, every sequence hits the same MLP weights (Wup, Wgate, Wdown don't depend on B)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 226,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"- In attention layers, every sequence has its own vectors KV cache (Q, K, V all depend on B)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- In attention layers, every sequence has its own vectors KV cache (Q, K, V all depend on B)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 228,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"Summary\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 229,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"- Prefill is compute-limited, generation is memory-limited\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Prefill is compute-limited, generation is memory-limited",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 230,
          "function_name": "arithmetic_intensity_of_inference",
          "code": "text(\"- MLP intensity is B (requires concurrent requests), attention intensity is 1 (impossible to improve)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- MLP intensity is B (requires concurrent requests), attention intensity is 1 (impossible to improve)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 21,
          "function_name": "main",
          "code": "arithmetic_intensity_of_inference()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 265,
          "function_name": "throughput_and_latency",
          "code": "def throughput_and_latency():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 266,
          "function_name": "throughput_and_latency",
          "code": "text(\"So we have shown that inference is memory-limited.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "So we have shown that inference is memory-limited.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 267,
          "function_name": "throughput_and_latency",
          "code": "text(\"Let us now compute the theoretical maximum latency and throughput of a single request.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let us now compute the theoretical maximum latency and throughput of a single request.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 268,
          "function_name": "throughput_and_latency",
          "code": "text(\"Assumption: can overlap compute and communication perfectly and ignore various types of overhead.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Assumption: can overlap compute and communication perfectly and ignore various types of overhead.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 270,
          "function_name": "throughput_and_latency",
          "code": "text(\"Instantiate latency and throughput for Llama 2 13B on an H100:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Instantiate latency and throughput for Llama 2 13B on an H100:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 271,
          "function_name": "throughput_and_latency",
          "code": "config = llama2_13b_config()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 271,
          "function_name": "throughput_and_latency",
          "code": "config = llama2_13b_config()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 262,
          "function_name": "llama2_13b_config",
          "code": "def llama2_13b_config(args={}):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 271,
          "function_name": "throughput_and_latency",
          "code": "config = llama2_13b_config()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 263,
          "function_name": "llama2_13b_config",
          "code": "return {S: 1024, D: 5120, F: 13824, N: 40, K: 40, H: 128, L: 40, V: 32000, memory_bandwidth: 3.35e12, **args}"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 271,
          "function_name": "throughput_and_latency",
          "code": "config = llama2_13b_config()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 233,
          "function_name": "compute_transformer_stats",
          "code": "def compute_transformer_stats(config):  # @inspect config"
        }
      ],
      "env": {
        "config": {
          "S": 1024,
          "D": 5120,
          "F": 13824,
          "N": 40,
          "K": 40,
          "H": 128,
          "L": 40,
          "V": 32000,
          "memory_bandwidth": 3350000000000.0
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 235,
          "function_name": "compute_transformer_stats",
          "code": "text(\"The memory, throughput, and latency depends on the shape of the Transformer. \"), text(\" \"), link(\"\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The memory, throughput, and latency depends on the shape of the Transformer. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": " ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 237,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Compute the number of parameters in the Transformer:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute the number of parameters in the Transformer:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 238,
          "function_name": "compute_transformer_stats",
          "code": "num_params = 2*V*D + D*F*3*L + (2*D*N*H + 2*D*K*H)*L"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 239,
          "function_name": "compute_transformer_stats",
          "code": "text(\"To store parameters, just use bf16 (training requires fp32)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "To store parameters, just use bf16 (training requires fp32)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 240,
          "function_name": "compute_transformer_stats",
          "code": "parameter_size = num_params * 2  # 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 242,
          "function_name": "compute_transformer_stats",
          "code": "text(\"We also don't need gradients and optimizer states since we're not training.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We also don't need gradients and optimizer states since we're not training.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 243,
          "function_name": "compute_transformer_stats",
          "code": "text(\"But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 244,
          "function_name": "compute_transformer_stats",
          "code": "text(\"How much we have to store per sequence:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How much we have to store per sequence:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 245,
          "function_name": "compute_transformer_stats",
          "code": "kv_cache_size = S * (K*H) * L * 2 * 2  # 2 for key + value, 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 247,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Total memory usage:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Total memory usage:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 248,
          "function_name": "compute_transformer_stats",
          "code": "memory = B * kv_cache_size + parameter_size"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 249,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Latency is determined by memory IO (read all parameters and KV cache for each step)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Latency is determined by memory IO (read all parameters and KV cache for each step)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 250,
          "function_name": "compute_transformer_stats",
          "code": "latency = memory / memory_bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 251,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Throughput is the inverse of latency, but we're generating B tokens in parallel\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Throughput is the inverse of latency, but we're generating B tokens in parallel",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 252,
          "function_name": "compute_transformer_stats",
          "code": "throughput = B / latency"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 255,
          "function_name": "compute_transformer_stats",
          "code": "num_params = num_params.subs(config).simplify()  # @inspect num_params"
        }
      ],
      "env": {
        "num_params": 13015449600
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 256,
          "function_name": "compute_transformer_stats",
          "code": "memory = memory.subs(config).simplify()  # @inspect memory"
        }
      ],
      "env": {
        "memory": "838860800*B + 26030899200"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 257,
          "function_name": "compute_transformer_stats",
          "code": "latency = latency.subs(config).simplify()  # @inspect latency"
        }
      ],
      "env": {
        "latency": "0.000250406208955224*B + 0.00777041767164179"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 258,
          "function_name": "compute_transformer_stats",
          "code": "throughput = throughput.subs(config).simplify()  # @inspect throughput"
        }
      ],
      "env": {
        "throughput": "127792.358398438*B/(32*B + 993)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        },
        {
          "path": "lecture_10.py",
          "line_number": 260,
          "function_name": "compute_transformer_stats",
          "code": "return num_params, memory, latency, throughput"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 272,
          "function_name": "throughput_and_latency",
          "code": "num_params, memory, latency, throughput = compute_transformer_stats(config)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 274,
          "function_name": "throughput_and_latency",
          "code": "text(\"If we use a batch size of 1:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If we use a batch size of 1:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 275,
          "function_name": "throughput_and_latency",
          "code": "bs1_memory = memory.subs(B, 1).simplify()   # @inspect bs1_memory"
        }
      ],
      "env": {
        "bs1_memory": 26869760000
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 276,
          "function_name": "throughput_and_latency",
          "code": "bs1_latency = latency.subs(B, 1).simplify()   # @inspect bs1_latency"
        }
      ],
      "env": {
        "bs1_latency": 0.008020823880597015
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 277,
          "function_name": "throughput_and_latency",
          "code": "bs1_throughput = throughput.subs(B, 1).simplify()   # @inspect bs1_throughput"
        }
      ],
      "env": {
        "bs1_throughput": 124.6754716082317
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 279,
          "function_name": "throughput_and_latency",
          "code": "text(\"If we use a batch size of 64 (worse latency, better throughput):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If we use a batch size of 64 (worse latency, better throughput):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 280,
          "function_name": "throughput_and_latency",
          "code": "bs64_memory = memory.subs(B, 64).simplify()   # @inspect bs64_memory"
        }
      ],
      "env": {
        "bs64_memory": 79717990400
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 281,
          "function_name": "throughput_and_latency",
          "code": "bs64_latency = latency.subs(B, 64).simplify()   # @inspect bs64_latency"
        }
      ],
      "env": {
        "bs64_latency": 0.02379641504477612
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 282,
          "function_name": "throughput_and_latency",
          "code": "bs64_throughput = throughput.subs(B, 64).simplify()   # @inspect bs64_throughput"
        }
      ],
      "env": {
        "bs64_throughput": 2689.480742354489
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 284,
          "function_name": "throughput_and_latency",
          "code": "text(\"If we use a batch size of 256:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If we use a batch size of 256:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 285,
          "function_name": "throughput_and_latency",
          "code": "bs256_memory = memory.subs(B, 256).simplify()   # @inspect bs256_memory"
        }
      ],
      "env": {
        "bs256_memory": 240779264000
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 286,
          "function_name": "throughput_and_latency",
          "code": "bs256_latency = latency.subs(B, 256).simplify()   # @inspect bs256_latency"
        }
      ],
      "env": {
        "bs256_latency": 0.0718744071641791
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 287,
          "function_name": "throughput_and_latency",
          "code": "bs256_throughput = throughput.subs(B, 256).simplify()   # @inspect bs256_throughput"
        }
      ],
      "env": {
        "bs256_throughput": 3561.7685084376703
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 288,
          "function_name": "throughput_and_latency",
          "code": "text(\"Doesn't fit into memory, but throughput gains are diminishing too...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Doesn't fit into memory, but throughput gains are diminishing too...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 290,
          "function_name": "throughput_and_latency",
          "code": "text(\"**Tradeoff** between latency and throughput:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Tradeoff** between latency and throughput:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 291,
          "function_name": "throughput_and_latency",
          "code": "text(\"1. Smaller batch sizes yields better latency but worse throughput\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Smaller batch sizes yields better latency but worse throughput",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 292,
          "function_name": "throughput_and_latency",
          "code": "text(\"2. Larger batch sizes yields better throughput but worse latency\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Larger batch sizes yields better throughput but worse latency",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 294,
          "function_name": "throughput_and_latency",
          "code": "text(\"Easy parallelism: if you launch M copies of the model, latency is the same, throughput increases by M!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Easy parallelism: if you launch M copies of the model, latency is the same, throughput increases by M!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 295,
          "function_name": "throughput_and_latency",
          "code": "text(\"Harder parallelism: shard the model and the KV cache \"), link(scaling_book_inference)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Harder parallelism: shard the model and the KV cache ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Scaling book chapter on Transformers]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://jax-ml.github.io/scaling-book/inference/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 297,
          "function_name": "throughput_and_latency",
          "code": "text(\"Note: time-to-first-token (TTFT) is essentially a function of prefill\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Note: time-to-first-token (TTFT) is essentially a function of prefill",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 298,
          "function_name": "throughput_and_latency",
          "code": "text(\"Use smaller batch sizes during prefill for faster TTFT\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Use smaller batch sizes during prefill for faster TTFT",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 299,
          "function_name": "throughput_and_latency",
          "code": "text(\"Use larger batch sizes during generation to improve throughput\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Use larger batch sizes during generation to improve throughput",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 22,
          "function_name": "main",
          "code": "throughput_and_latency()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 24,
          "function_name": "main",
          "code": "text(\"### Taking shortcuts (lossy)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Taking shortcuts (lossy)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 302,
          "function_name": "reduce_kv_cache_size",
          "code": "def reduce_kv_cache_size():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 303,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Recall that memory is the bottleneck for inference.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall that memory is the bottleneck for inference.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 304,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"So let's try to reduce the size of the KV cache\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "So let's try to reduce the size of the KV cache",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 305,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"...but make sure we don't lose too much accuracy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...but make sure we don't lose too much accuracy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 307,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"### Grouped-query attention (GQA) \"), link(gqa)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Grouped-query attention (GQA) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
            "authors": [
              "Joshua Ainslie",
              "James Lee-Thorp",
              "Michiel de Jong",
              "Yury Zemlyanskiy",
              "Federico Lebr\u00f3n",
              "Sumit Sanghai"
            ],
            "organization": "Google",
            "date": "2023-05-22T17:16:38Z",
            "url": "https://arxiv.org/pdf/2305.13245.pdf",
            "description": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
            "notes": "Multi-query attention (MQA) speeds up, but less expressive\nGQA: use an intermediate (more than one, less than number of heads) number of key-value heads\nExperiments on T5"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 308,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"https://jax-ml.github.io/scaling-book/assets/img/gmqa.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-45c47d328aa951d9cbe28b4ae0074615-https_jax-ml_github_io_scaling-book_assets_img_gmqa_png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 309,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Idea: N query heads, but only K key and value heads, each interacting with N/K query heads\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Idea: N query heads, but only K key and value heads, each interacting with N/K query heads",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 310,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Multi-headed attention (MHA): K=N\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Multi-headed attention (MHA): K=N",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 311,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Multi-query attention (MQA): K=1\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Multi-query attention (MQA): K=1",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 312,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Group-query attention (GQA): K is somewhere in between\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Group-query attention (GQA): K is somewhere in between",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 314,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Latency/throughput improvements:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Latency/throughput improvements:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 315,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/gqa-speed.png\", width=500); text(\" \"); link(gqa)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/gqa-speed.png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": " ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
            "authors": [
              "Joshua Ainslie",
              "James Lee-Thorp",
              "Michiel de Jong",
              "Yury Zemlyanskiy",
              "Federico Lebr\u00f3n",
              "Sumit Sanghai"
            ],
            "organization": "Google",
            "date": "2023-05-22T17:16:38Z",
            "url": "https://arxiv.org/pdf/2305.13245.pdf",
            "description": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
            "notes": "Multi-query attention (MQA) speeds up, but less expressive\nGQA: use an intermediate (more than one, less than number of heads) number of key-value heads\nExperiments on T5"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 316,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Reduce the KV cache by a factor of N/K\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reduce the KV cache by a factor of N/K",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 317,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 40, B: 64})  # Original Llama 2 13B"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 317,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 40, B: 64})  # Original Llama 2 13B"
        },
        {
          "path": "lecture_10.py",
          "line_number": 262,
          "function_name": "llama2_13b_config",
          "code": "def llama2_13b_config(args={}):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 317,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 40, B: 64})  # Original Llama 2 13B"
        },
        {
          "path": "lecture_10.py",
          "line_number": 263,
          "function_name": "llama2_13b_config",
          "code": "return {S: 1024, D: 5120, F: 13824, N: 40, K: 40, H: 128, L: 40, V: 32000, memory_bandwidth: 3.35e12, **args}"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 317,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 40, B: 64})  # Original Llama 2 13B"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 233,
          "function_name": "compute_transformer_stats",
          "code": "def compute_transformer_stats(config):  # @inspect config"
        }
      ],
      "env": {
        "config": {
          "S": 1024,
          "D": 5120,
          "F": 13824,
          "N": 40,
          "K": 40,
          "H": 128,
          "L": 40,
          "V": 32000,
          "memory_bandwidth": 3350000000000.0,
          "B": 64
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 235,
          "function_name": "compute_transformer_stats",
          "code": "text(\"The memory, throughput, and latency depends on the shape of the Transformer. \"), text(\" \"), link(\"\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The memory, throughput, and latency depends on the shape of the Transformer. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": " ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 237,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Compute the number of parameters in the Transformer:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute the number of parameters in the Transformer:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 238,
          "function_name": "compute_transformer_stats",
          "code": "num_params = 2*V*D + D*F*3*L + (2*D*N*H + 2*D*K*H)*L"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 239,
          "function_name": "compute_transformer_stats",
          "code": "text(\"To store parameters, just use bf16 (training requires fp32)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "To store parameters, just use bf16 (training requires fp32)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 240,
          "function_name": "compute_transformer_stats",
          "code": "parameter_size = num_params * 2  # 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 242,
          "function_name": "compute_transformer_stats",
          "code": "text(\"We also don't need gradients and optimizer states since we're not training.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We also don't need gradients and optimizer states since we're not training.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 243,
          "function_name": "compute_transformer_stats",
          "code": "text(\"But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 244,
          "function_name": "compute_transformer_stats",
          "code": "text(\"How much we have to store per sequence:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How much we have to store per sequence:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 245,
          "function_name": "compute_transformer_stats",
          "code": "kv_cache_size = S * (K*H) * L * 2 * 2  # 2 for key + value, 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 247,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Total memory usage:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Total memory usage:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 248,
          "function_name": "compute_transformer_stats",
          "code": "memory = B * kv_cache_size + parameter_size"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 249,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Latency is determined by memory IO (read all parameters and KV cache for each step)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Latency is determined by memory IO (read all parameters and KV cache for each step)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 250,
          "function_name": "compute_transformer_stats",
          "code": "latency = memory / memory_bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 251,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Throughput is the inverse of latency, but we're generating B tokens in parallel\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Throughput is the inverse of latency, but we're generating B tokens in parallel",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 252,
          "function_name": "compute_transformer_stats",
          "code": "throughput = B / latency"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 255,
          "function_name": "compute_transformer_stats",
          "code": "num_params = num_params.subs(config).simplify()  # @inspect num_params"
        }
      ],
      "env": {
        "num_params": 13015449600
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 256,
          "function_name": "compute_transformer_stats",
          "code": "memory = memory.subs(config).simplify()  # @inspect memory"
        }
      ],
      "env": {
        "memory": 79717990400
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 257,
          "function_name": "compute_transformer_stats",
          "code": "latency = latency.subs(config).simplify()  # @inspect latency"
        }
      ],
      "env": {
        "latency": 0.023796415044776118
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 258,
          "function_name": "compute_transformer_stats",
          "code": "throughput = throughput.subs(config).simplify()  # @inspect throughput"
        }
      ],
      "env": {
        "throughput": 2689.480742354489
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 260,
          "function_name": "compute_transformer_stats",
          "code": "return num_params, memory, latency, throughput"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 318,
          "function_name": "reduce_kv_cache_size",
          "code": "k40_num_params, k40_memory, k40_latency, k40_throughput = compute_transformer_stats(config)  # @inspect k40_memory, @inspect k40_latency, @inspect k40_throughput"
        }
      ],
      "env": {
        "k40_memory": 79717990400,
        "k40_latency": 0.023796415044776118,
        "k40_throughput": 2689.480742354489
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 320,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 64})  # Use GQA with 1:5 ratio"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 320,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 64})  # Use GQA with 1:5 ratio"
        },
        {
          "path": "lecture_10.py",
          "line_number": 262,
          "function_name": "llama2_13b_config",
          "code": "def llama2_13b_config(args={}):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 320,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 64})  # Use GQA with 1:5 ratio"
        },
        {
          "path": "lecture_10.py",
          "line_number": 263,
          "function_name": "llama2_13b_config",
          "code": "return {S: 1024, D: 5120, F: 13824, N: 40, K: 40, H: 128, L: 40, V: 32000, memory_bandwidth: 3.35e12, **args}"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 320,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 64})  # Use GQA with 1:5 ratio"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 233,
          "function_name": "compute_transformer_stats",
          "code": "def compute_transformer_stats(config):  # @inspect config"
        }
      ],
      "env": {
        "config": {
          "S": 1024,
          "D": 5120,
          "F": 13824,
          "N": 40,
          "K": 8,
          "H": 128,
          "L": 40,
          "V": 32000,
          "memory_bandwidth": 3350000000000.0,
          "B": 64
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 235,
          "function_name": "compute_transformer_stats",
          "code": "text(\"The memory, throughput, and latency depends on the shape of the Transformer. \"), text(\" \"), link(\"\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The memory, throughput, and latency depends on the shape of the Transformer. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": " ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 237,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Compute the number of parameters in the Transformer:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute the number of parameters in the Transformer:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 238,
          "function_name": "compute_transformer_stats",
          "code": "num_params = 2*V*D + D*F*3*L + (2*D*N*H + 2*D*K*H)*L"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 239,
          "function_name": "compute_transformer_stats",
          "code": "text(\"To store parameters, just use bf16 (training requires fp32)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "To store parameters, just use bf16 (training requires fp32)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 240,
          "function_name": "compute_transformer_stats",
          "code": "parameter_size = num_params * 2  # 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 242,
          "function_name": "compute_transformer_stats",
          "code": "text(\"We also don't need gradients and optimizer states since we're not training.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We also don't need gradients and optimizer states since we're not training.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 243,
          "function_name": "compute_transformer_stats",
          "code": "text(\"But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 244,
          "function_name": "compute_transformer_stats",
          "code": "text(\"How much we have to store per sequence:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How much we have to store per sequence:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 245,
          "function_name": "compute_transformer_stats",
          "code": "kv_cache_size = S * (K*H) * L * 2 * 2  # 2 for key + value, 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 247,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Total memory usage:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Total memory usage:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 248,
          "function_name": "compute_transformer_stats",
          "code": "memory = B * kv_cache_size + parameter_size"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 249,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Latency is determined by memory IO (read all parameters and KV cache for each step)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Latency is determined by memory IO (read all parameters and KV cache for each step)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 250,
          "function_name": "compute_transformer_stats",
          "code": "latency = memory / memory_bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 251,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Throughput is the inverse of latency, but we're generating B tokens in parallel\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Throughput is the inverse of latency, but we're generating B tokens in parallel",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 252,
          "function_name": "compute_transformer_stats",
          "code": "throughput = B / latency"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 255,
          "function_name": "compute_transformer_stats",
          "code": "num_params = num_params.subs(config).simplify()  # @inspect num_params"
        }
      ],
      "env": {
        "num_params": 11337728000
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 256,
          "function_name": "compute_transformer_stats",
          "code": "memory = memory.subs(config).simplify()  # @inspect memory"
        }
      ],
      "env": {
        "memory": 33412874240
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 257,
          "function_name": "compute_transformer_stats",
          "code": "latency = latency.subs(config).simplify()  # @inspect latency"
        }
      ],
      "env": {
        "latency": 0.00997399231044776
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 258,
          "function_name": "compute_transformer_stats",
          "code": "throughput = throughput.subs(config).simplify()  # @inspect throughput"
        }
      ],
      "env": {
        "throughput": 6416.688323787855
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 260,
          "function_name": "compute_transformer_stats",
          "code": "return num_params, memory, latency, throughput"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 321,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_num_params, k8_memory, k8_latency, k8_throughput = compute_transformer_stats(config)  # @inspect k8_memory, @inspect k8_latency, @inspect k8_throughput"
        }
      ],
      "env": {
        "k8_memory": 33412874240,
        "k8_latency": 0.00997399231044776,
        "k8_throughput": 6416.688323787855
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 323,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"This also means we can use a larger batch size:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This also means we can use a larger batch size:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 324,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 256})  # Increase batch size"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 324,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 256})  # Increase batch size"
        },
        {
          "path": "lecture_10.py",
          "line_number": 262,
          "function_name": "llama2_13b_config",
          "code": "def llama2_13b_config(args={}):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 324,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 256})  # Increase batch size"
        },
        {
          "path": "lecture_10.py",
          "line_number": 263,
          "function_name": "llama2_13b_config",
          "code": "return {S: 1024, D: 5120, F: 13824, N: 40, K: 40, H: 128, L: 40, V: 32000, memory_bandwidth: 3.35e12, **args}"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 324,
          "function_name": "reduce_kv_cache_size",
          "code": "config = llama2_13b_config({K: 8, B: 256})  # Increase batch size"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 233,
          "function_name": "compute_transformer_stats",
          "code": "def compute_transformer_stats(config):  # @inspect config"
        }
      ],
      "env": {
        "config": {
          "S": 1024,
          "D": 5120,
          "F": 13824,
          "N": 40,
          "K": 8,
          "H": 128,
          "L": 40,
          "V": 32000,
          "memory_bandwidth": 3350000000000.0,
          "B": 256
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 235,
          "function_name": "compute_transformer_stats",
          "code": "text(\"The memory, throughput, and latency depends on the shape of the Transformer. \"), text(\" \"), link(\"\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The memory, throughput, and latency depends on the shape of the Transformer. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": " ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 237,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Compute the number of parameters in the Transformer:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute the number of parameters in the Transformer:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 238,
          "function_name": "compute_transformer_stats",
          "code": "num_params = 2*V*D + D*F*3*L + (2*D*N*H + 2*D*K*H)*L"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 239,
          "function_name": "compute_transformer_stats",
          "code": "text(\"To store parameters, just use bf16 (training requires fp32)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "To store parameters, just use bf16 (training requires fp32)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 240,
          "function_name": "compute_transformer_stats",
          "code": "parameter_size = num_params * 2  # 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 242,
          "function_name": "compute_transformer_stats",
          "code": "text(\"We also don't need gradients and optimizer states since we're not training.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We also don't need gradients and optimizer states since we're not training.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 243,
          "function_name": "compute_transformer_stats",
          "code": "text(\"But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But we do have to store the KV cache (which are some of the activations) for each sequence (of length S):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 244,
          "function_name": "compute_transformer_stats",
          "code": "text(\"How much we have to store per sequence:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How much we have to store per sequence:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 245,
          "function_name": "compute_transformer_stats",
          "code": "kv_cache_size = S * (K*H) * L * 2 * 2  # 2 for key + value, 2 for bf16"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 247,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Total memory usage:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Total memory usage:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 248,
          "function_name": "compute_transformer_stats",
          "code": "memory = B * kv_cache_size + parameter_size"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 249,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Latency is determined by memory IO (read all parameters and KV cache for each step)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Latency is determined by memory IO (read all parameters and KV cache for each step)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 250,
          "function_name": "compute_transformer_stats",
          "code": "latency = memory / memory_bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 251,
          "function_name": "compute_transformer_stats",
          "code": "text(\"Throughput is the inverse of latency, but we're generating B tokens in parallel\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Throughput is the inverse of latency, but we're generating B tokens in parallel",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 252,
          "function_name": "compute_transformer_stats",
          "code": "throughput = B / latency"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 255,
          "function_name": "compute_transformer_stats",
          "code": "num_params = num_params.subs(config).simplify()  # @inspect num_params"
        }
      ],
      "env": {
        "num_params": 11337728000
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 256,
          "function_name": "compute_transformer_stats",
          "code": "memory = memory.subs(config).simplify()  # @inspect memory"
        }
      ],
      "env": {
        "memory": 65625128960
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 257,
          "function_name": "compute_transformer_stats",
          "code": "latency = latency.subs(config).simplify()  # @inspect latency"
        }
      ],
      "env": {
        "latency": 0.01958959073432836
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 258,
          "function_name": "compute_transformer_stats",
          "code": "throughput = throughput.subs(config).simplify()  # @inspect throughput"
        }
      ],
      "env": {
        "throughput": 13068.164795877605
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        },
        {
          "path": "lecture_10.py",
          "line_number": 260,
          "function_name": "compute_transformer_stats",
          "code": "return num_params, memory, latency, throughput"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 325,
          "function_name": "reduce_kv_cache_size",
          "code": "k8_bs_num_params, k8_bs_memory, k8_bs_latency, k8_bs_throughput = compute_transformer_stats(config)  # @inspect k8_bs_memory, @inspect k8_bs_latency, @inspect k8_bs_throughput"
        }
      ],
      "env": {
        "k8_bs_memory": 65625128960,
        "k8_bs_latency": 0.01958959073432836,
        "k8_bs_throughput": 13068.164795877605
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 326,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Worse latency, but better throughput (and it fits in memory now!).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Worse latency, but better throughput (and it fits in memory now!).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 328,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Check that accuracy doesn't drop: \"); link(gqa)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Check that accuracy doesn't drop: ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
            "authors": [
              "Joshua Ainslie",
              "James Lee-Thorp",
              "Michiel de Jong",
              "Yury Zemlyanskiy",
              "Federico Lebr\u00f3n",
              "Sumit Sanghai"
            ],
            "organization": "Google",
            "date": "2023-05-22T17:16:38Z",
            "url": "https://arxiv.org/pdf/2305.13245.pdf",
            "description": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
            "notes": "Multi-query attention (MQA) speeds up, but less expressive\nGQA: use an intermediate (more than one, less than number of heads) number of key-value heads\nExperiments on T5"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 329,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/gqa-accuracy.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/gqa-accuracy.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 331,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"### Multi-head latent attention (MLA) \"), link(mla)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Multi-head latent attention (MLA) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
            "authors": [
              "DeepSeek-AI",
              "Aixin Liu",
              "Bei Feng",
              "Bin Wang",
              "Bingxuan Wang",
              "Bo Liu",
              "Chenggang Zhao",
              "Chengqi Dengr",
              "Chong Ruan",
              "Damai Dai",
              "Daya Guo",
              "Dejian Yang",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Hanwei Xu",
              "Hao Yang",
              "Haowei Zhang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Li",
              "Hui Qu",
              "J. L. Cai",
              "Jian Liang",
              "Jianzhong Guo",
              "Jiaqi Ni",
              "Jiashi Li",
              "Jin Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junxiao Song",
              "Kai Dong",
              "Kaige Gao",
              "Kang Guan",
              "Lean Wang",
              "Lecong Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Liang Zhao",
              "Liyue Zhang",
              "Meng Li",
              "Miaojun Wang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peiyi Wang",
              "Peng Zhang",
              "Qihao Zhu",
              "Qinyu Chen",
              "Qiushi Du",
              "R. J. Chen",
              "R. L. Jin",
              "Ruiqi Ge",
              "Ruizhe Pan",
              "Runxin Xu",
              "Ruyi Chen",
              "S. S. Li",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Shirong Ma",
              "Shiyu Wang",
              "Shuang Zhou",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Size Zheng",
              "T. Wang",
              "Tian Pei",
              "Tian Yuan",
              "Tianyu Sun",
              "W. L. Xiao",
              "Wangding Zeng",
              "Wei An",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wentao Zhang",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xianzu Wang",
              "Xiao Bi",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaojin Shen",
              "Xiaokang Chen",
              "Xiaosha Chen",
              "Xiaotao Nie",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xin Liu",
              "Xin Xie",
              "Xingkai Yu",
              "Xinnan Song",
              "Xinyi Zhou",
              "Xinyu Yang",
              "Xuan Lu",
              "Xuecheng Su",
              "Y. Wu",
              "Y. K. Li",
              "Y. X. Wei",
              "Y. X. Zhu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Li",
              "Yaohui Wang",
              "Yi Zheng",
              "Yichao Zhang",
              "Yiliang Xiong",
              "Yilong Zhao",
              "Ying He",
              "Ying Tang",
              "Yishi Piao",
              "Yixin Dong",
              "Yixuan Tan",
              "Yiyuan Liu",
              "Yongji Wang",
              "Yongqiang Guo",
              "Yuchen Zhu",
              "Yuduan Wang",
              "Yuheng Zou",
              "Yukun Zha",
              "Yunxian Ma",
              "Yuting Yan",
              "Yuxiang You",
              "Yuxuan Liu",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhen Huang",
              "Zhen Zhang",
              "Zhenda Xie",
              "Zhewen Hao",
              "Zhihong Shao",
              "Zhiniu Wen",
              "Zhipeng Xu",
              "Zhongyu Zhang",
              "Zhuoshu Li",
              "Zihan Wang",
              "Zihui Gu",
              "Zilin Li",
              "Ziwei Xie"
            ],
            "organization": null,
            "date": "2024-05-07T15:56:43Z",
            "url": "https://arxiv.org/abs/2405.04434",
            "description": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 332,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/mla-schema.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/mla-schema.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 333,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Key idea: project down each key and value vector from N*H dimensions to C dimensions\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key idea: project down each key and value vector from N*H dimensions to C dimensions",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 334,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"DeepSeek v2: reduce N*H = 16384 to C = 512\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "DeepSeek v2: reduce N*H = 16384 to C = 512",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 335,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Wrinkle: MLA is not compatible with RoPE, so need to add additional 64 dimensions for RoPE, so 512 + 64 = 576 total dimensions\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Wrinkle: MLA is not compatible with RoPE, so need to add additional 64 dimensions for RoPE, so 512 + 64 = 576 total dimensions",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 336,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Latency/throughput improvements follow similarly from the KV cache reduction as argued earlier\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Latency/throughput improvements follow similarly from the KV cache reduction as argued earlier",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 338,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Let's now check the accuracy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's now check the accuracy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 339,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"First, MHA is better than GQA (though more expensive) [Table 8] \"); link(mla)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "First, MHA is better than GQA (though more expensive) [Table 8] ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
            "authors": [
              "DeepSeek-AI",
              "Aixin Liu",
              "Bei Feng",
              "Bin Wang",
              "Bingxuan Wang",
              "Bo Liu",
              "Chenggang Zhao",
              "Chengqi Dengr",
              "Chong Ruan",
              "Damai Dai",
              "Daya Guo",
              "Dejian Yang",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Hanwei Xu",
              "Hao Yang",
              "Haowei Zhang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Li",
              "Hui Qu",
              "J. L. Cai",
              "Jian Liang",
              "Jianzhong Guo",
              "Jiaqi Ni",
              "Jiashi Li",
              "Jin Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junxiao Song",
              "Kai Dong",
              "Kaige Gao",
              "Kang Guan",
              "Lean Wang",
              "Lecong Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Liang Zhao",
              "Liyue Zhang",
              "Meng Li",
              "Miaojun Wang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peiyi Wang",
              "Peng Zhang",
              "Qihao Zhu",
              "Qinyu Chen",
              "Qiushi Du",
              "R. J. Chen",
              "R. L. Jin",
              "Ruiqi Ge",
              "Ruizhe Pan",
              "Runxin Xu",
              "Ruyi Chen",
              "S. S. Li",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Shirong Ma",
              "Shiyu Wang",
              "Shuang Zhou",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Size Zheng",
              "T. Wang",
              "Tian Pei",
              "Tian Yuan",
              "Tianyu Sun",
              "W. L. Xiao",
              "Wangding Zeng",
              "Wei An",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wentao Zhang",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xianzu Wang",
              "Xiao Bi",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaojin Shen",
              "Xiaokang Chen",
              "Xiaosha Chen",
              "Xiaotao Nie",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xin Liu",
              "Xin Xie",
              "Xingkai Yu",
              "Xinnan Song",
              "Xinyi Zhou",
              "Xinyu Yang",
              "Xuan Lu",
              "Xuecheng Su",
              "Y. Wu",
              "Y. K. Li",
              "Y. X. Wei",
              "Y. X. Zhu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Li",
              "Yaohui Wang",
              "Yi Zheng",
              "Yichao Zhang",
              "Yiliang Xiong",
              "Yilong Zhao",
              "Ying He",
              "Ying Tang",
              "Yishi Piao",
              "Yixin Dong",
              "Yixuan Tan",
              "Yiyuan Liu",
              "Yongji Wang",
              "Yongqiang Guo",
              "Yuchen Zhu",
              "Yuduan Wang",
              "Yuheng Zou",
              "Yukun Zha",
              "Yunxian Ma",
              "Yuting Yan",
              "Yuxiang You",
              "Yuxuan Liu",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhen Huang",
              "Zhen Zhang",
              "Zhenda Xie",
              "Zhewen Hao",
              "Zhihong Shao",
              "Zhiniu Wen",
              "Zhipeng Xu",
              "Zhongyu Zhang",
              "Zhuoshu Li",
              "Zihan Wang",
              "Zihui Gu",
              "Zilin Li",
              "Ziwei Xie"
            ],
            "organization": null,
            "date": "2024-05-07T15:56:43Z",
            "url": "https://arxiv.org/abs/2405.04434",
            "description": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 340,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/mla-accuracy.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/mla-accuracy.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 341,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Second, MLA is a bit better than MHA (and much cheaper) [Table 9] \"); link(mla)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Second, MLA is a bit better than MHA (and much cheaper) [Table 9] ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
            "authors": [
              "DeepSeek-AI",
              "Aixin Liu",
              "Bei Feng",
              "Bin Wang",
              "Bingxuan Wang",
              "Bo Liu",
              "Chenggang Zhao",
              "Chengqi Dengr",
              "Chong Ruan",
              "Damai Dai",
              "Daya Guo",
              "Dejian Yang",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Hanwei Xu",
              "Hao Yang",
              "Haowei Zhang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Li",
              "Hui Qu",
              "J. L. Cai",
              "Jian Liang",
              "Jianzhong Guo",
              "Jiaqi Ni",
              "Jiashi Li",
              "Jin Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junxiao Song",
              "Kai Dong",
              "Kaige Gao",
              "Kang Guan",
              "Lean Wang",
              "Lecong Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Liang Zhao",
              "Liyue Zhang",
              "Meng Li",
              "Miaojun Wang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peiyi Wang",
              "Peng Zhang",
              "Qihao Zhu",
              "Qinyu Chen",
              "Qiushi Du",
              "R. J. Chen",
              "R. L. Jin",
              "Ruiqi Ge",
              "Ruizhe Pan",
              "Runxin Xu",
              "Ruyi Chen",
              "S. S. Li",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Shirong Ma",
              "Shiyu Wang",
              "Shuang Zhou",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Size Zheng",
              "T. Wang",
              "Tian Pei",
              "Tian Yuan",
              "Tianyu Sun",
              "W. L. Xiao",
              "Wangding Zeng",
              "Wei An",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wentao Zhang",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xianzu Wang",
              "Xiao Bi",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaojin Shen",
              "Xiaokang Chen",
              "Xiaosha Chen",
              "Xiaotao Nie",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xin Liu",
              "Xin Xie",
              "Xingkai Yu",
              "Xinnan Song",
              "Xinyi Zhou",
              "Xinyu Yang",
              "Xuan Lu",
              "Xuecheng Su",
              "Y. Wu",
              "Y. K. Li",
              "Y. X. Wei",
              "Y. X. Zhu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Li",
              "Yaohui Wang",
              "Yi Zheng",
              "Yichao Zhang",
              "Yiliang Xiong",
              "Yilong Zhao",
              "Ying He",
              "Ying Tang",
              "Yishi Piao",
              "Yixin Dong",
              "Yixuan Tan",
              "Yiyuan Liu",
              "Yongji Wang",
              "Yongqiang Guo",
              "Yuchen Zhu",
              "Yuduan Wang",
              "Yuheng Zou",
              "Yukun Zha",
              "Yunxian Ma",
              "Yuting Yan",
              "Yuxiang You",
              "Yuxuan Liu",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhen Huang",
              "Zhen Zhang",
              "Zhenda Xie",
              "Zhewen Hao",
              "Zhihong Shao",
              "Zhiniu Wen",
              "Zhipeng Xu",
              "Zhongyu Zhang",
              "Zhuoshu Li",
              "Zihan Wang",
              "Zihui Gu",
              "Zilin Li",
              "Ziwei Xie"
            ],
            "organization": null,
            "date": "2024-05-07T15:56:43Z",
            "url": "https://arxiv.org/abs/2405.04434",
            "description": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 342,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/mla-accuracy2.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/mla-accuracy2.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 344,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"### Cross-layer attention (CLA) \"), link(\"https://arxiv.org/abs/2405.12981\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Cross-layer attention (CLA) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
            "authors": [
              "William Brandon",
              "Mayank Mishra",
              "Aniruddha Nrusimha",
              "Rameswar Panda",
              "Jonathan Ragan Kelly"
            ],
            "organization": null,
            "date": "2024-05-21T17:59:29Z",
            "url": "https://arxiv.org/abs/2405.12981",
            "description": "Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 345,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/cla-diagram.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/cla-diagram.png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 346,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Idea: share KVs across **layers** (just as GQA shares KVs across heads)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Idea: share KVs across **layers** (just as GQA shares KVs across heads)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 347,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Empirically improves the pareto frontier of accuracy and KV cache size (latency and throughput)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Empirically improves the pareto frontier of accuracy and KV cache size (latency and throughput)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 348,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/cla-results.png\", width=700)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/cla-results.png",
          "style": {
            "width": 700
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 350,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"### Local attention \"), link(longformer), link(sparse_transformer), link(mistral_7b)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Local attention ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Longformer: The Long-Document Transformer",
            "authors": [
              "Iz Beltagy",
              "Matthew E. Peters",
              "Arman Cohan"
            ],
            "organization": "AllenAI",
            "date": "2020-04-10T17:54:09Z",
            "url": "https://arxiv.org/pdf/2004.05150.pdf",
            "description": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
            "notes": "Sliding window (local) attention\nGlobal attention to capture task-specific information"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Generating Long Sequences with Sparse Transformers",
            "authors": [
              "Rewon Child",
              "Scott Gray",
              "Alec Radford",
              "Ilya Sutskever"
            ],
            "organization": "OpenAI",
            "date": "2019-04-23T19:29:47Z",
            "url": "https://arxiv.org/pdf/1904.10509.pdf",
            "description": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
            "notes": "Local attention"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Mistral 7B",
            "authors": [
              "Albert Q. Jiang",
              "Alexandre Sablayrolles",
              "Arthur Mensch",
              "Chris Bamford",
              "Devendra Singh Chaplot",
              "Diego de las Casas",
              "Florian Bressand",
              "Gianna Lengyel",
              "Guillaume Lample",
              "Lucile Saulnier",
              "L\u00e9lio Renard Lavaud",
              "Marie-Anne Lachaux",
              "Pierre Stock",
              "Teven Le Scao",
              "Thibaut Lavril",
              "Thomas Wang",
              "Timoth\u00e9e Lacroix",
              "William El Sayed"
            ],
            "organization": "Mistral",
            "date": "2023-10-10T17:54:58Z",
            "url": "https://arxiv.org/pdf/2310.06825.pdf",
            "description": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",
            "notes": "GQA, sliding window attention"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 351,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"images/longformer-attention.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/longformer-attention.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 352,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Idea: just look at the local context, which is most relevant for modeling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Idea: just look at the local context, which is most relevant for modeling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 353,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Effective context scales linearly with the number of layers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Effective context scales linearly with the number of layers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 354,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"KV cache is independent of sequence length!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "KV cache is independent of sequence length!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 356,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Problem: this can still hurt accuracy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem: this can still hurt accuracy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 357,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Solution: interleave local attention with global attention (hybrid layers)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Solution: interleave local attention with global attention (hybrid layers)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 358,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Example: character.ai uses 1 global layer every 6 layers (in addition to CLA) \"), article_link(\"https://research.character.ai/optimizing-inference/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example: character.ai uses 1 global layer every 6 layers (in addition to CLA) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://research.character.ai/optimizing-inference/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 359,
          "function_name": "reduce_kv_cache_size",
          "code": "image(\"https://research.character.ai/content/images/2024/06/figure1-2-1.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-fe65751fdb3e1a45a354527093281a1e-https_research_character_ai_content_images_2024_06_figure1-2-1_png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 361,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 362,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"- Goal: reduce the KV cache size (since inference is memory-limited) without hurting accuracy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Goal: reduce the KV cache size (since inference is memory-limited) without hurting accuracy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 363,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"- Lower-dimensional KV cache (GQA, MLA, shared KV cache)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Lower-dimensional KV cache (GQA, MLA, shared KV cache)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 364,
          "function_name": "reduce_kv_cache_size",
          "code": "text(\"- Local attention on some of the layers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Local attention on some of the layers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 25,
          "function_name": "main",
          "code": "reduce_kv_cache_size()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 367,
          "function_name": "alternatives_to_the_transformer",
          "code": "def alternatives_to_the_transformer():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 368,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"We have shown that tweaking the architecture of the Transformer, we can improve latency and throughput.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We have shown that tweaking the architecture of the Transformer, we can improve latency and throughput.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 369,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"Attention + autoregression is fundamentally memory-limited (Transformers were not designed with inference in mind).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Attention + autoregression is fundamentally memory-limited (Transformers were not designed with inference in mind).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 370,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"Can we substantially improve things if we go beyond the Transformer?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Can we substantially improve things if we go beyond the Transformer?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 371,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"We will discuss two directions: state-space models and diffusion models.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We will discuss two directions: state-space models and diffusion models.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 373,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"## State-space models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## State-space models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 374,
          "function_name": "alternatives_to_the_transformer",
          "code": "link(title=\"[presentation from CS229S]\", url=\"https://docs.google.com/presentation/d/1wrQO4uzwWr73SGj7aFxeVR9Cz0PY-mzJipn12enM39k/edit#slide=id.p\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[presentation from CS229S]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://docs.google.com/presentation/d/1wrQO4uzwWr73SGj7aFxeVR9Cz0PY-mzJipn12enM39k/edit#slide=id.p",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 375,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Idea: from signal processing to model long-context sequences in a sub-quadratic time\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Idea: from signal processing to model long-context sequences in a sub-quadratic time",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 376,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- S4: based on classic state space models, good at synthetic long-context tasks \"), link(\"https://arxiv.org/abs/2111.00396\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- S4: based on classic state space models, good at synthetic long-context tasks ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
            "authors": [
              "Albert Gu",
              "Karan Goel",
              "Christopher R\u00e9"
            ],
            "organization": null,
            "date": "2021-10-31T03:32:18Z",
            "url": "https://arxiv.org/abs/2111.00396",
            "description": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 377,
          "function_name": "alternatives_to_the_transformer",
          "code": "image(\"images/s4-summary.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/s4-summary.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 378,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Weaknesses: bad at solving associative recall tasks important for language (where Transformers do well)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Weaknesses: bad at solving associative recall tasks important for language (where Transformers do well)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 379,
          "function_name": "alternatives_to_the_transformer",
          "code": "image(\"images/based-associative-recall.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/based-associative-recall.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 380,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Mamba: allow SSM parameters to be input-dependent, match Transformers at 1B scale \"), link(\"https://arxiv.org/abs/2312.00752\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Mamba: allow SSM parameters to be input-dependent, match Transformers at 1B scale ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
            "authors": [
              "Albert Gu",
              "Tri Dao"
            ],
            "organization": null,
            "date": "2023-12-01T18:01:34Z",
            "url": "https://arxiv.org/abs/2312.00752",
            "description": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 381,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Jamba: interleave Transformer-Mamba layers (1:7 ratio) with a 52B MoE \"), link(\"https://arxiv.org/abs/2403.19887\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Jamba: interleave Transformer-Mamba layers (1:7 ratio) with a 52B MoE ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Jamba: A Hybrid Transformer-Mamba Language Model",
            "authors": [
              "Opher Lieber",
              "Barak Lenz",
              "Hofit Bata",
              "Gal Cohen",
              "Jhonathan Osin",
              "Itay Dalmedigos",
              "Erez Safahi",
              "Shaked Meirom",
              "Yonatan Belinkov",
              "Shai Shalev-Shwartz",
              "Omri Abend",
              "Raz Alon",
              "Tomer Asida",
              "Amir Bergman",
              "Roman Glozman",
              "Michael Gokhman",
              "Avashalom Manevich",
              "Nir Ratner",
              "Noam Rozen",
              "Erez Shwartz",
              "Mor Zusman",
              "Yoav Shoham"
            ],
            "organization": null,
            "date": "2024-03-28T23:55:06Z",
            "url": "https://arxiv.org/abs/2403.19887",
            "description": "We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 382,
          "function_name": "alternatives_to_the_transformer",
          "code": "image(\"images/jamba-architecture.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/jamba-architecture.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 383,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- BASED: use linear attention + local attention \"), link(\"https://arxiv.org/abs/2402.18668\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- BASED: use linear attention + local attention ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Simple linear attention language models balance the recall-throughput tradeoff",
            "authors": [
              "Simran Arora",
              "Sabri Eyuboglu",
              "Michael Zhang",
              "Aman Timalsina",
              "Silas Alberti",
              "Dylan Zinsley",
              "James Zou",
              "Atri Rudra",
              "Christopher R\u00e9"
            ],
            "organization": null,
            "date": "2024-02-28T19:28:27Z",
            "url": "https://arxiv.org/abs/2402.18668",
            "description": "Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 384,
          "function_name": "alternatives_to_the_transformer",
          "code": "image(\"images/based-attention.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/based-attention.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 385,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- MiniMax-01: use linear attention + full attention (456B parameter MoE) \"), link(\"https://arxiv.org/pdf/2501.08313\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- MiniMax-01: use linear attention + full attention (456B parameter MoE) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
            "authors": [
              "MiniMax",
              "Aonian Li",
              "Bangwei Gong",
              "Bo Yang",
              "Boji Shan",
              "Chang Liu",
              "Cheng Zhu",
              "Chunhao Zhang",
              "Congchao Guo",
              "Da Chen",
              "Dong Li",
              "Enwei Jiao",
              "Gengxin Li",
              "Guojun Zhang",
              "Haohai Sun",
              "Houze Dong",
              "Jiadai Zhu",
              "Jiaqi Zhuang",
              "Jiayuan Song",
              "Jin Zhu",
              "Jingtao Han",
              "Jingyang Li",
              "Junbin Xie",
              "Junhao Xu",
              "Junjie Yan",
              "Kaishun Zhang",
              "Kecheng Xiao",
              "Kexi Kang",
              "Le Han",
              "Leyang Wang",
              "Lianfei Yu",
              "Liheng Feng",
              "Lin Zheng",
              "Linbo Chai",
              "Long Xing",
              "Meizhi Ju",
              "Mingyuan Chi",
              "Mozhi Zhang",
              "Peikai Huang",
              "Pengcheng Niu",
              "Pengfei Li",
              "Pengyu Zhao",
              "Qi Yang",
              "Qidi Xu",
              "Qiexiang Wang",
              "Qin Wang",
              "Qiuhui Li",
              "Ruitao Leng",
              "Shengmin Shi",
              "Shuqi Yu",
              "Sichen Li",
              "Songquan Zhu",
              "Tao Huang",
              "Tianrun Liang",
              "Weigao Sun",
              "Weixuan Sun",
              "Weiyu Cheng",
              "Wenkai Li",
              "Xiangjun Song",
              "Xiao Su",
              "Xiaodong Han",
              "Xinjie Zhang",
              "Xinzhu Hou",
              "Xu Min",
              "Xun Zou",
              "Xuyang Shen",
              "Yan Gong",
              "Yingjie Zhu",
              "Yipeng Zhou",
              "Yiran Zhong",
              "Yongyi Hu",
              "Yuanxiang Fan",
              "Yue Yu",
              "Yufeng Yang",
              "Yuhao Li",
              "Yunan Huang",
              "Yunji Li",
              "Yunpeng Huang",
              "Yunzhi Xu",
              "Yuxin Mao",
              "Zehan Li",
              "Zekang Li",
              "Zewei Tao",
              "Zewen Ying",
              "Zhaoyang Cong",
              "Zhen Qin",
              "Zhenhua Fan",
              "Zhihang Yu",
              "Zhuo Jiang",
              "Zijia Wu"
            ],
            "organization": null,
            "date": "2025-01-14T18:50:05Z",
            "url": "https://arxiv.org/pdf/2501.08313",
            "description": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 387,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"Takeaways:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Takeaways:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 388,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Linear + local attention (still need some full attention) yield serious SOTA models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Linear + local attention (still need some full attention) yield serious SOTA models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 389,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Replace O(T) KV cache with O(1) state => much more efficient for inference\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Replace O(T) KV cache with O(1) state => much more efficient for inference",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 391,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"### Diffusion models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Diffusion models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 392,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Popular for image generation, but harder to get working for text generation \"), link(\"https://arxiv.org/abs/2205.14217\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Popular for image generation, but harder to get working for text generation ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Diffusion-LM Improves Controllable Text Generation",
            "authors": [
              "Xiang Lisa Li",
              "John Thickstun",
              "Ishaan Gulrajani",
              "Percy Liang",
              "Tatsunori B. Hashimoto"
            ],
            "organization": null,
            "date": "2022-05-27T20:12:09Z",
            "url": "https://arxiv.org/abs/2205.14217",
            "description": "Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 393,
          "function_name": "alternatives_to_the_transformer",
          "code": "image(\"images/diffusion-lm.png\", width=700)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/diffusion-lm.png",
          "style": {
            "width": 700
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 394,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Idea: generate each token in parallel (not autoregressively), refine multiple time steps\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Idea: generate each token in parallel (not autoregressively), refine multiple time steps",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 395,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Start with random noise (over entire sequence), iteratively refine it\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Start with random noise (over entire sequence), iteratively refine it",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 396,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"- Results from Inception Labs \"), article_link(\"https://www.inceptionlabs.ai/news\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Results from Inception Labs ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.inceptionlabs.ai/news",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 397,
          "function_name": "alternatives_to_the_transformer",
          "code": "link(title=\"[demo video]\", url=\"https://x.com/i/status/1894847919624462794\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[demo video]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://x.com/i/status/1894847919624462794",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 398,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"Much faster on coding benchmarks:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Much faster on coding benchmarks:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 399,
          "function_name": "alternatives_to_the_transformer",
          "code": "image(\"https://framerusercontent.com/images/K2zvhtaTsz5ehDFoWx6KQHOqCyk.jpg\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-0b0fd02c1b0c6f65c13940b7fc260ab4-https_framerusercontent_com_images_K2zvhtaTsz5ehDFoWx6KQHOqCyk_jpg",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 401,
          "function_name": "alternatives_to_the_transformer",
          "code": "text(\"Overall, significant gains in inference to be made with more radical architecture changes!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Overall, significant gains in inference to be made with more radical architecture changes!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 26,
          "function_name": "main",
          "code": "alternatives_to_the_transformer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 404,
          "function_name": "quantization",
          "code": "def quantization():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 405,
          "function_name": "quantization",
          "code": "text(\"Key idea: reduce the precision of numbers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key idea: reduce the precision of numbers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 406,
          "function_name": "quantization",
          "code": "text(\"Less memory means higher latency/throughput (since inference is memory-limited).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Less memory means higher latency/throughput (since inference is memory-limited).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 407,
          "function_name": "quantization",
          "code": "text(\"Of course we have to worry about accuracy...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Of course we have to worry about accuracy...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 409,
          "function_name": "quantization",
          "code": "image(\"https://www.datocms-assets.com/104802/1709770809-twitter-post-20.png\", width=400), article_link(\"https://www.baseten.co/blog/fp8-efficient-model-inference-with-8-bit-floating-point-numbers/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-6e6964259cf3d5a080f18d21764d2bec-https_www_datocms-assets_com_104802_1709770809-twitter-post-20_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.baseten.co/blog/fp8-efficient-model-inference-with-8-bit-floating-point-numbers/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 410,
          "function_name": "quantization",
          "code": "text(\"- fp32 (4 bytes): needed for parameters and optimizer states during training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- fp32 (4 bytes): needed for parameters and optimizer states during training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 411,
          "function_name": "quantization",
          "code": "text(\"- bf16 (2 bytes): default for inference\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- bf16 (2 bytes): default for inference",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 412,
          "function_name": "quantization",
          "code": "text(\"- fp8 (1 byte) [-240, 240] for e4m3 on H100s: can train if you dare \"), link(\"https://arxiv.org/pdf/2310.18313\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- fp8 (1 byte) [-240, 240] for e4m3 on H100s: can train if you dare ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "FP8-LM: Training FP8 Large Language Models",
            "authors": [
              "Houwen Peng",
              "Kan Wu",
              "Yixuan Wei",
              "Guoshuai Zhao",
              "Yuxiang Yang",
              "Ze Liu",
              "Yifan Xiong",
              "Ziyue Yang",
              "Bolin Ni",
              "Jingcheng Hu",
              "Ruihang Li",
              "Miaosen Zhang",
              "Chen Li",
              "Jia Ning",
              "Ruizhe Wang",
              "Zheng Zhang",
              "Shuguang Liu",
              "Joe Chau",
              "Han Hu",
              "Peng Cheng"
            ],
            "organization": null,
            "date": "2023-10-27T17:59:51Z",
            "url": "https://arxiv.org/pdf/2310.18313",
            "description": "In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 39% reduction in real memory usage but also ran 75% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 37%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 413,
          "function_name": "quantization",
          "code": "text(\"- int8 (1 byte) [-128, 127]: less accurate but cheaper than fp8, but for inference only \"), link(\"https://arxiv.org/pdf/2303.17951\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- int8 (1 byte) [-128, 127]: less accurate but cheaper than fp8, but for inference only ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "FP8 versus INT8 for efficient deep learning inference",
            "authors": [
              "Mart van Baalen",
              "Andrey Kuzmin",
              "Suparna S Nair",
              "Yuwei Ren",
              "Eric Mahurin",
              "Chirag Patel",
              "Sundar Subramanian",
              "Sanghyuk Lee",
              "Markus Nagel",
              "Joseph Soriaga",
              "Tijmen Blankevoort"
            ],
            "organization": null,
            "date": "2023-03-31T10:29:17Z",
            "url": "https://arxiv.org/pdf/2303.17951",
            "description": "Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how this theory translates to practice. We also provide a hardware analysis showing that the FP formats are somewhere between 50-180% less efficient in terms of compute in dedicated hardware than the INT format. Based on our research and a read of the research field, we conclude that although the proposed FP8 format could be good for training, the results for inference do not warrant a dedicated implementation of FP8 in favor of INT8 for efficient inference. We show that our results are mostly consistent with previous findings but that important comparisons between the formats have thus far been lacking. Finally, we discuss what happens when FP8-trained networks are converted to INT8 and conclude with a brief discussion on the most efficient way for on-device deployment and an extensive suite of INT8 results for many models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 414,
          "function_name": "quantization",
          "code": "text(\"- int4 (0.5 bytes) [-8, 7]: cheaper, even less accurate \"), link(\"https://arxiv.org/pdf/2303.17951\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- int4 (0.5 bytes) [-8, 7]: cheaper, even less accurate ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "FP8 versus INT8 for efficient deep learning inference",
            "authors": [
              "Mart van Baalen",
              "Andrey Kuzmin",
              "Suparna S Nair",
              "Yuwei Ren",
              "Eric Mahurin",
              "Chirag Patel",
              "Sundar Subramanian",
              "Sanghyuk Lee",
              "Markus Nagel",
              "Joseph Soriaga",
              "Tijmen Blankevoort"
            ],
            "organization": null,
            "date": "2023-03-31T10:29:17Z",
            "url": "https://arxiv.org/pdf/2303.17951",
            "description": "Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how this theory translates to practice. We also provide a hardware analysis showing that the FP formats are somewhere between 50-180% less efficient in terms of compute in dedicated hardware than the INT format. Based on our research and a read of the research field, we conclude that although the proposed FP8 format could be good for training, the results for inference do not warrant a dedicated implementation of FP8 in favor of INT8 for efficient inference. We show that our results are mostly consistent with previous findings but that important comparisons between the formats have thus far been lacking. Finally, we discuss what happens when FP8-trained networks are converted to INT8 and conclude with a brief discussion on the most efficient way for on-device deployment and an extensive suite of INT8 results for many models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 416,
          "function_name": "quantization",
          "code": "text(\"Quantization-aware training (QAT): train with quantization, but doesn't scale up\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Quantization-aware training (QAT): train with quantization, but doesn't scale up",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 417,
          "function_name": "quantization",
          "code": "text(\"Post-training quantization (PTQ): run on sample data to determine scale and zero point for each layer or tensor\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Post-training quantization (PTQ): run on sample data to determine scale and zero point for each layer or tensor",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 418,
          "function_name": "quantization",
          "code": "link(title=\"[Overview of approaches]\", url=\"https://apxml.com/posts/llm-quantization-techniques-explained\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Overview of approaches]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://apxml.com/posts/llm-quantization-techniques-explained",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 420,
          "function_name": "quantization",
          "code": "text(\"### LLM.int8()\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### LLM.int8()",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 421,
          "function_name": "quantization",
          "code": "link(\"https://arxiv.org/abs/2208.07339\"), article_link(\"https://huggingface.co/blog/hf-bitsandbytes-integration\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "authors": [
              "Tim Dettmers",
              "Mike Lewis",
              "Younes Belkada",
              "Luke Zettlemoyer"
            ],
            "organization": null,
            "date": "2022-08-15T17:08:50Z",
            "url": "https://arxiv.org/abs/2208.07339",
            "description": "Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/blog/hf-bitsandbytes-integration",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 422,
          "function_name": "quantization",
          "code": "text(\"Standard quantization (scale by max of absolute values):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Standard quantization (scale by max of absolute values):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 423,
          "function_name": "quantization",
          "code": "image(\"https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/quant-freeze.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-8f165bbc7b78ab8ca356e2866bbdd118-https_huggingface_co_blog_assets_96_hf_bitsandbytes_integration_quant-freeze_png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 424,
          "function_name": "quantization",
          "code": "text(\"Problem: outliers (which appear in larger networks) screw everything up\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem: outliers (which appear in larger networks) screw everything up",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 425,
          "function_name": "quantization",
          "code": "text(\"Solution: extract outliers and process them in fp16\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Solution: extract outliers and process them in fp16",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 426,
          "function_name": "quantization",
          "code": "image(\"https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/Mixed-int8.gif\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-5e056ed09ad2b0888540d9e4e4795efa-https_huggingface_co_blog_assets_96_hf_bitsandbytes_integration_Mixed-int8_gif",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 427,
          "function_name": "quantization",
          "code": "text(\"It works well (but is 15-23% slower than fp16):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "It works well (but is 15-23% slower than fp16):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 428,
          "function_name": "quantization",
          "code": "image(\"images/llm-int8-bloom.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/llm-int8-bloom.png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 430,
          "function_name": "quantization",
          "code": "text(\"### Activation-aware quantization\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Activation-aware quantization",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 431,
          "function_name": "quantization",
          "code": "link(\"https://arxiv.org/abs/2306.00978\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
            "authors": [
              "Ji Lin",
              "Jiaming Tang",
              "Haotian Tang",
              "Shang Yang",
              "Wei-Ming Chen",
              "Wei-Chen Wang",
              "Guangxuan Xiao",
              "Xingyu Dang",
              "Chuang Gan",
              "Song Han"
            ],
            "organization": null,
            "date": "2023-06-01T17:59:10Z",
            "url": "https://arxiv.org/abs/2306.00978",
            "description": "Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 432,
          "function_name": "quantization",
          "code": "text(\"Idea: select which weights (0.1-1%) to keep in high precision based on activations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Idea: select which weights (0.1-1%) to keep in high precision based on activations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 433,
          "function_name": "quantization",
          "code": "text(\"fp16 -> int3 produces 4x lower memory, 3.2x speedup\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "fp16 -> int3 produces 4x lower memory, 3.2x speedup",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 434,
          "function_name": "quantization",
          "code": "image(\"images/awq-schema.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/awq-schema.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 27,
          "function_name": "main",
          "code": "quantization()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 437,
          "function_name": "model_pruning",
          "code": "def model_pruning():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 438,
          "function_name": "model_pruning",
          "code": "text(\"Key idea: just rip out parts of an expensive model to make it cheaper\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key idea: just rip out parts of an expensive model to make it cheaper",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 439,
          "function_name": "model_pruning",
          "code": "text(\"...and then fix it up.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...and then fix it up.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 441,
          "function_name": "model_pruning",
          "code": "text(\"Paper from NVIDIA \"), link(\"https://arxiv.org/abs/2407.14679\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Paper from NVIDIA ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Compact Language Models via Pruning and Knowledge Distillation",
            "authors": [
              "Saurav Muralidharan",
              "Sharath Turuvekere Sreenivas",
              "Raviraj Joshi",
              "Marcin Chochowski",
              "Mostofa Patwary",
              "Mohammad Shoeybi",
              "Bryan Catanzaro",
              "Jan Kautz",
              "Pavlo Molchanov"
            ],
            "organization": null,
            "date": "2024-07-19T21:47:57Z",
            "url": "https://arxiv.org/abs/2407.14679",
            "description": "Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 442,
          "function_name": "model_pruning",
          "code": "image(\"images/pruning-kd-loop.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/pruning-kd-loop.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 443,
          "function_name": "model_pruning",
          "code": "text(\"Algorithm:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Algorithm:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 444,
          "function_name": "model_pruning",
          "code": "text(\"1. Identify important {layer, head, hidden dimension} on a small calibration dataset (1024 samples)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Identify important {layer, head, hidden dimension} on a small calibration dataset (1024 samples)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 445,
          "function_name": "model_pruning",
          "code": "text(\"2. Remove unimportant layers to get a smaller model\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Remove unimportant layers to get a smaller model",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 446,
          "function_name": "model_pruning",
          "code": "text(\"3. Distill the original model into pruned model\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Distill the original model into pruned model",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 448,
          "function_name": "model_pruning",
          "code": "text(\"Results:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Results:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 449,
          "function_name": "model_pruning",
          "code": "image(\"images/pruning-kd.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/pruning-kd.png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 28,
          "function_name": "main",
          "code": "model_pruning()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 30,
          "function_name": "main",
          "code": "text(\"Summary: reduce inference complexity without hurting accuracy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary: reduce inference complexity without hurting accuracy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 32,
          "function_name": "main",
          "code": "text(\"From scratch recipe:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "From scratch recipe:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 33,
          "function_name": "main",
          "code": "text(\"1. Define faster model architecture\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Define faster model architecture",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 34,
          "function_name": "main",
          "code": "text(\"2. Train faster model\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Train faster model",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 36,
          "function_name": "main",
          "code": "text(\"Distillation recipe:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Distillation recipe:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 37,
          "function_name": "main",
          "code": "text(\"1. Define faster model architecture\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Define faster model architecture",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 38,
          "function_name": "main",
          "code": "text(\"2. Initialize weights using original model (which has a different architecture)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Initialize weights using original model (which has a different architecture)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 39,
          "function_name": "main",
          "code": "text(\"3. Repair faster model (distillation)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Repair faster model (distillation)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 41,
          "function_name": "main",
          "code": "text(\"### Use shortcuts but double check (lossless)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Use shortcuts but double check (lossless)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 452,
          "function_name": "speculative_sampling",
          "code": "def speculative_sampling():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 453,
          "function_name": "speculative_sampling",
          "code": "text(\"Recall the two stages of inference:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall the two stages of inference:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 454,
          "function_name": "speculative_sampling",
          "code": "text(\"- Prefill: given a sequence, encode tokens in parallel (compute-limited) [note: also gives you probabilities]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Prefill: given a sequence, encode tokens in parallel (compute-limited) [note: also gives you probabilities]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 455,
          "function_name": "speculative_sampling",
          "code": "text(\"- Generation: generate one token at a time (memory-limited)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Generation: generate one token at a time (memory-limited)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 456,
          "function_name": "speculative_sampling",
          "code": "text(\"In other words, checking is faster than generation.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In other words, checking is faster than generation.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 458,
          "function_name": "speculative_sampling",
          "code": "text(\"Speculative sampling \"); link(\"https://arxiv.org/abs/2211.17192\"); link(\"https://arxiv.org/abs/2302.01318\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Speculative sampling ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Fast Inference from Transformers via Speculative Decoding",
            "authors": [
              "Yaniv Leviathan",
              "Matan Kalman",
              "Yossi Matias"
            ],
            "organization": null,
            "date": "2022-11-30T17:33:28Z",
            "url": "https://arxiv.org/abs/2211.17192",
            "description": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
            "authors": [
              "Charlie Chen",
              "Sebastian Borgeaud",
              "Geoffrey Irving",
              "Jean-Baptiste Lespiau",
              "Laurent Sifre",
              "John Jumper"
            ],
            "organization": null,
            "date": "2023-02-02T18:44:11Z",
            "url": "https://arxiv.org/abs/2302.01318",
            "description": "We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 459,
          "function_name": "speculative_sampling",
          "code": "text(\"- Use a cheaper **draft model** p to guess a few tokens (e.g., 4)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use a cheaper **draft model** p to guess a few tokens (e.g., 4)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 460,
          "function_name": "speculative_sampling",
          "code": "text(\"- Evaluate with target model q (process tokens in parallel), and accept if it looks good\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Evaluate with target model q (process tokens in parallel), and accept if it looks good",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 461,
          "function_name": "speculative_sampling",
          "code": "link(title=\"[Speculative sampling video]\", url=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeculativeDecoding-1-Illustration.mp4\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Speculative sampling video]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://storage.googleapis.com/gweb-research2023-media/media/SpeculativeDecoding-1-Illustration.mp4",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 462,
          "function_name": "speculative_sampling",
          "code": "article_link(\"https://research.google/blog/looking-back-at-speculative-decoding/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://research.google/blog/looking-back-at-speculative-decoding/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 464,
          "function_name": "speculative_sampling",
          "code": "image(\"images/speculative-sampling-algorithm.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/speculative-sampling-algorithm.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 465,
          "function_name": "speculative_sampling",
          "code": "text(\"This is modified rejection sampling with proposal p and target q\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is modified rejection sampling with proposal p and target q",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 466,
          "function_name": "speculative_sampling",
          "code": "text(\"Modification: always generate at least one candidate (rejection sampling will keep looping)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Modification: always generate at least one candidate (rejection sampling will keep looping)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 467,
          "function_name": "speculative_sampling",
          "code": "text(\"Key property: guaranteed to be an **exact sample** from the target model!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key property: guaranteed to be an **exact sample** from the target model!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 469,
          "function_name": "speculative_sampling",
          "code": "text(\"Proof by example: assume two vocabulary elements {A, B}\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Proof by example: assume two vocabulary elements {A, B}",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 470,
          "function_name": "speculative_sampling",
          "code": "text(\"- Target model probabilities: [q(A), q(B)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Target model probabilities: [q(A), q(B)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 471,
          "function_name": "speculative_sampling",
          "code": "text(\"- Draft model probabilities: [p(A), p(B)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Draft model probabilities: [p(A), p(B)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 472,
          "function_name": "speculative_sampling",
          "code": "text(\"- Assume p(A) > q(A) [draft model oversamples A].\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Assume p(A) > q(A) [draft model oversamples A].",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 473,
          "function_name": "speculative_sampling",
          "code": "text(\"- Therefore p(B) < q(B) [draft model undersamples B].\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Therefore p(B) < q(B) [draft model undersamples B].",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 474,
          "function_name": "speculative_sampling",
          "code": "text(\"- Residual probabilities max(q-p, 0): [0, 1]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Residual probabilities max(q-p, 0): [0, 1]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 475,
          "function_name": "speculative_sampling",
          "code": "text(\"Compute the probabilities of speculatively sampling a token:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute the probabilities of speculatively sampling a token:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 476,
          "function_name": "speculative_sampling",
          "code": "text(\"- P[sampling A] = p(A) * (q(A) / p(A)) + p(B) * 1 * 0 = q(A)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- P[sampling A] = p(A) * (q(A) / p(A)) + p(B) * 1 * 0 = q(A)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 477,
          "function_name": "speculative_sampling",
          "code": "text(\"- P[sampling B] = p(B) * 1 + p(A) * (1 - q(A) / p(A)) * 1 = q(B)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- P[sampling B] = p(B) * 1 + p(A) * (1 - q(A) / p(A)) * 1 = q(B)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 479,
          "function_name": "speculative_sampling",
          "code": "image(\"images/speculative-sampling-results.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/speculative-sampling-results.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 480,
          "function_name": "speculative_sampling",
          "code": "image(\"images/speculative-sampling-stats.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/speculative-sampling-stats.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 482,
          "function_name": "speculative_sampling",
          "code": "text(\"In practice:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In practice:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 483,
          "function_name": "speculative_sampling",
          "code": "text(\"- Target model has 70B parameters, draft model has 8B parameters\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Target model has 70B parameters, draft model has 8B parameters",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 484,
          "function_name": "speculative_sampling",
          "code": "text(\"- Target model has 8B parameters, draft model has 1B parameters\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Target model has 8B parameters, draft model has 1B parameters",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 485,
          "function_name": "speculative_sampling",
          "code": "text(\"- Try to make draft model as close to target (distillation)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Try to make draft model as close to target (distillation)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 487,
          "function_name": "speculative_sampling",
          "code": "text(\"Extensions to improve the draft model:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Extensions to improve the draft model:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 488,
          "function_name": "speculative_sampling",
          "code": "text(\"- Medusa: draft model generates multiple tokens in parallel \"), link(\"https://arxiv.org/abs/2401.10774\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Medusa: draft model generates multiple tokens in parallel ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
            "authors": [
              "Tianle Cai",
              "Yuhong Li",
              "Zhengyang Geng",
              "Hongwu Peng",
              "Jason D. Lee",
              "Deming Chen",
              "Tri Dao"
            ],
            "organization": null,
            "date": "2024-01-19T15:48:40Z",
            "url": "https://arxiv.org/abs/2401.10774",
            "description": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities. Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 489,
          "function_name": "speculative_sampling",
          "code": "text(\"- EAGLE: draft model takes high-level features from target model \"), link(\"https://arxiv.org/pdf/2401.15077\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- EAGLE: draft model takes high-level features from target model ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
            "authors": [
              "Yuhui Li",
              "Fangyun Wei",
              "Chao Zhang",
              "Hongyang Zhang"
            ],
            "organization": null,
            "date": "2024-01-26T18:59:01Z",
            "url": "https://arxiv.org/pdf/2401.15077",
            "description": "Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 490,
          "function_name": "speculative_sampling",
          "code": "image(\"images/medusa-eagle.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/medusa-eagle.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 492,
          "function_name": "speculative_sampling",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 493,
          "function_name": "speculative_sampling",
          "code": "text(\"- Exact sampling from target model (thanks to math)!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Exact sampling from target model (thanks to math)!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 494,
          "function_name": "speculative_sampling",
          "code": "text(\"- Exploits asymmetry between checking and generation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Exploits asymmetry between checking and generation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 495,
          "function_name": "speculative_sampling",
          "code": "text(\"- Lots of room for innovation on the draft model (involves training)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Lots of room for innovation on the draft model (involves training)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 42,
          "function_name": "main",
          "code": "speculative_sampling()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 44,
          "function_name": "main",
          "code": "text(\"### Handling dynamic workloads\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Handling dynamic workloads",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 45,
          "function_name": "main",
          "code": "text(\"Batching over sequences in live traffic is tricky because:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Batching over sequences in live traffic is tricky because:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 46,
          "function_name": "main",
          "code": "text(\"1. Requests arrive at different times (waiting for batch is bad for early requests)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Requests arrive at different times (waiting for batch is bad for early requests)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 47,
          "function_name": "main",
          "code": "text(\"2. Sequences have shared prefixes (e.g., system prompts, generating multiple samples)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Sequences have shared prefixes (e.g., system prompts, generating multiple samples)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 48,
          "function_name": "main",
          "code": "text(\"3. Sequences have different lengths (padding is inefficient)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Sequences have different lengths (padding is inefficient)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 498,
          "function_name": "continuous_batching",
          "code": "def continuous_batching():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 499,
          "function_name": "continuous_batching",
          "code": "link(title=\"Orca: A Distributed Serving System for Transformer-Based Generative Models\", url=\"https://www.usenix.org/system/files/osdi22-yu.pdf\"), link(title=\"[talk]\", url=\"https://www.youtube.com/watch?v=Ob9PPLxETYU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Orca: A Distributed Serving System for Transformer-Based Generative Models",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.usenix.org/system/files/osdi22-yu.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[talk]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.youtube.com/watch?v=Ob9PPLxETYU",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 501,
          "function_name": "continuous_batching",
          "code": "text(\"Problem:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 502,
          "function_name": "continuous_batching",
          "code": "text(\"- Training: get a dense block of tokens (batch size x sequence length)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Training: get a dense block of tokens (batch size x sequence length)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 503,
          "function_name": "continuous_batching",
          "code": "text(\"- Inference: requests arrive and finish at different times, so you have a ragged array\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Inference: requests arrive and finish at different times, so you have a ragged array",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 504,
          "function_name": "continuous_batching",
          "code": "image(\"https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-1d1ec88764cc9f6eee8cc00bfce35f75-https_images_ctfassets_net_xjan103pcp94_1LJioEsEdQQpDCxYNWirU6_82b9fbfc5b78b10c1d4508b60e72fdcf_cb_02_diagram-static-batching_png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 506,
          "function_name": "continuous_batching",
          "code": "text(\"Solution: iteration-level scheduling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Solution: iteration-level scheduling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 507,
          "function_name": "continuous_batching",
          "code": "text(\"- Decode step by step\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Decode step by step",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 508,
          "function_name": "continuous_batching",
          "code": "text(\"- Add new requests to the batch as they arrive (so don't have to wait until generation completes)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Add new requests to the batch as they arrive (so don't have to wait until generation completes)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 510,
          "function_name": "continuous_batching",
          "code": "text(\"Problem:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 511,
          "function_name": "continuous_batching",
          "code": "text(\"- Batching only works when all sequences have the same dimensionality (right?)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Batching only works when all sequences have the same dimensionality (right?)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 512,
          "function_name": "continuous_batching",
          "code": "text(\"- But each request might have a different length\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- But each request might have a different length",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 514,
          "function_name": "continuous_batching",
          "code": "text(\"Solution: selective batching\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Solution: selective batching",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 515,
          "function_name": "continuous_batching",
          "code": "text(\"- Training: when all sequences of the same length, operate on a B x S x H tensor\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Training: when all sequences of the same length, operate on a B x S x H tensor",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 516,
          "function_name": "continuous_batching",
          "code": "text(\"- But we might have different lengths: [3, H], [9, H], [5, H], etc.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- But we might have different lengths: [3, H], [9, H], [5, H], etc.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 517,
          "function_name": "continuous_batching",
          "code": "text(\"- Attention computation: process each sequence separately\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Attention computation: process each sequence separately",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 518,
          "function_name": "continuous_batching",
          "code": "text(\"- Non-attention computation: concatenate all the sequences together to [3 + 9 + 5, H]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Non-attention computation: concatenate all the sequences together to [3 + 9 + 5, H]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 50,
          "function_name": "main",
          "code": "continuous_batching()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 521,
          "function_name": "paged_attention",
          "code": "def paged_attention():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 522,
          "function_name": "paged_attention",
          "code": "text(\"Paper that introduced vLLM in addition to PagedAttention \"), link(\"https://arxiv.org/pdf/2309.06180.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Paper that introduced vLLM in addition to PagedAttention ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
            "authors": [
              "Woosuk Kwon",
              "Zhuohan Li",
              "Siyuan Zhuang",
              "Ying Sheng",
              "Lianmin Zheng",
              "Cody Hao Yu",
              "Joseph E. Gonzalez",
              "Hao Zhang",
              "Ion Stoica"
            ],
            "organization": null,
            "date": "2023-09-12T12:50:04Z",
            "url": "https://arxiv.org/pdf/2309.06180.pdf",
            "description": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 524,
          "function_name": "paged_attention",
          "code": "text(\"Previous status quo:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Previous status quo:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 525,
          "function_name": "paged_attention",
          "code": "text(\"- Request comes in\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Request comes in",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 526,
          "function_name": "paged_attention",
          "code": "text(\"- Allocate section of KV cache for prompt and response (up to a max length)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Allocate section of KV cache for prompt and response (up to a max length)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 527,
          "function_name": "paged_attention",
          "code": "image(\"images/paged-attention-fragmentation.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/paged-attention-fragmentation.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 528,
          "function_name": "paged_attention",
          "code": "text(\"Problem: fragmentation (what happens to your hard drive)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem: fragmentation (what happens to your hard drive)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 529,
          "function_name": "paged_attention",
          "code": "text(\"- But this is wasteful since we might generate much fewer tokens (internal fragmentation)!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- But this is wasteful since we might generate much fewer tokens (internal fragmentation)!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 530,
          "function_name": "paged_attention",
          "code": "text(\"- Might be extra unused space between sections (external fragmentation)!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Might be extra unused space between sections (external fragmentation)!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 532,
          "function_name": "paged_attention",
          "code": "text(\"Solution: PagedAttention (remember operating systems)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Solution: PagedAttention (remember operating systems)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 533,
          "function_name": "paged_attention",
          "code": "text(\"- Divide the KV cache of a sequence into non-contiguous **blocks**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Divide the KV cache of a sequence into non-contiguous **blocks**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 534,
          "function_name": "paged_attention",
          "code": "image(\"images/paged-attention-blocks.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/paged-attention-blocks.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 536,
          "function_name": "paged_attention",
          "code": "text(\"Two requests share the KV caches:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Two requests share the KV caches:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 537,
          "function_name": "paged_attention",
          "code": "image(\"images/paged-attention-logical.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/paged-attention-logical.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 539,
          "function_name": "paged_attention",
          "code": "text(\"In general, multiples types of sharing KV caches across sequences:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In general, multiples types of sharing KV caches across sequences:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 540,
          "function_name": "paged_attention",
          "code": "image(\"images/paged-attention-sharing.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/paged-attention-sharing.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 541,
          "function_name": "paged_attention",
          "code": "text(\"- Sharing the system prompt\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Sharing the system prompt",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 542,
          "function_name": "paged_attention",
          "code": "text(\"- Sampling multiple responses per prompt (e.g., for program synthesis)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Sampling multiple responses per prompt (e.g., for program synthesis)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 544,
          "function_name": "paged_attention",
          "code": "text(\"Solution: share prefixes, copy-on-write at the block level\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Solution: share prefixes, copy-on-write at the block level",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 545,
          "function_name": "paged_attention",
          "code": "image(\"images/paged-attention-parallel.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/paged-attention-parallel.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 547,
          "function_name": "paged_attention",
          "code": "text(\"Other vLLM optimizations:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Other vLLM optimizations:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 548,
          "function_name": "paged_attention",
          "code": "text(\"- Kernel to fuse block read and attention (reduce kernel launch overhead)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Kernel to fuse block read and attention (reduce kernel launch overhead)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 549,
          "function_name": "paged_attention",
          "code": "text(\"- Use latest kernels (FlashAttention, FlashDecoding)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use latest kernels (FlashAttention, FlashDecoding)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 550,
          "function_name": "paged_attention",
          "code": "text(\"- Use CUDA graphs to avoid kernel launch overhead\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use CUDA graphs to avoid kernel launch overhead",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        },
        {
          "path": "lecture_10.py",
          "line_number": 552,
          "function_name": "paged_attention",
          "code": "text(\"Summary: use ideas from operating systems (paging) to make use of memory for dynamic workloads\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary: use ideas from operating systems (paging) to make use of memory for dynamic workloads",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 51,
          "function_name": "main",
          "code": "paged_attention()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 53,
          "function_name": "main",
          "code": "text(\"### Summary\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Summary",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 54,
          "function_name": "main",
          "code": "text(\"- Inference is important (actual use, evaluation, reinforcement learning)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Inference is important (actual use, evaluation, reinforcement learning)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 55,
          "function_name": "main",
          "code": "text(\"- Different characteristics compared to training (memory-limited, dynamic)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Different characteristics compared to training (memory-limited, dynamic)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 56,
          "function_name": "main",
          "code": "text(\"- Techniques: new architectures, quantization, pruning/distillation, speculative decoding\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Techniques: new architectures, quantization, pruning/distillation, speculative decoding",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 57,
          "function_name": "main",
          "code": "text(\"- Ideas from systems (speculative execution, paging)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Ideas from systems (speculative execution, paging)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_10.py",
          "line_number": 58,
          "function_name": "main",
          "code": "text(\"- New architectures have huge potential for improvement\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- New architectures have huge potential for improvement",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    }
  ]
}