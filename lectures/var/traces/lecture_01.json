{
  "files": {
    "lecture_01.py": "import regex\nfrom abc import ABC\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport random\n\nfrom execute_util import link, image, text\nfrom lecture_util import article_link, x_link, youtube_link\nfrom references import gpt_3, gpt4, shannon1950, bengio2003, susketver2014, \\\n    bahdanau2015_attention, transformer_2017, gpt2, t5, kaplan_scaling_laws_2020, \\\n    the_pile, gpt_j, opt_175b, bloom, palm, chinchilla, llama, mistral_7b, \\\n    instruct_gpt, dpo, adamw2017, lima, deepseek_v3, adam2014, grpo, ppo2017, muon, \\\n    large_batch_training_2018, wsd_2024, cosine_learning_rate_2017, olmo_7b, moe_2017, \\\n    megatron_lm_2019, shazeer_2020, elmo, bert, qwen_2_5, deepseek_r1, moe_2017, \\\n    rms_norm_2019, rope_2021, soap, gqa, mla, deepseek_67b, deepseek_v2, brants2007, \\\n    layernorm_2016, pre_post_norm_2020, llama2, llama3, olmo2, \\\n    megabyte, byt5, blt, tfree, sennrich_2016, zero_2019, gpipe_2018\nfrom data import get_common_crawl_urls, read_common_crawl, write_documents, markdownify_documents\nfrom model_util import query_gpt4o\n\nimport tiktoken\n\ndef main():\n    welcome()\n    why_this_course_exists()\n    current_landscape()\n\n    what_is_this_program()\n\n    course_logistics()\n    course_components()\n\n    tokenization()\n\n    text(\"Next time: PyTorch building blocks, resource accounting\")\n\n\ndef welcome():\n    text(\"## CS336: Language Models From Scratch (Spring 2025)\"),\n\n    image(\"images/course-staff.png\", width=600)\n\n    text(\"This is the second offering of CS336.\")\n    text(\"Stanford edition has grown by 50%.\")\n    text(\"Lectures will be posted on YouTube and be made available to the whole world.\")\n\n\ndef why_this_course_exists():\n    text(\"## Why did we make this course?\")\n\n    text(\"Let's ask GPT-4 \"), link(gpt4)\n    response = query_gpt4o(prompt=\"Why teach a course on building language models from scratch? Answer in one sentence.\")  # @inspect response\n    \n    text(\"Problem: researchers are becoming **disconnected** from the underlying technology.\")\n    text(\"8 years ago, researchers would implement and train their own models.\")\n    text(\"6 years ago, researchers would download a model (e.g., BERT) and fine-tune it.\")\n    text(\"Today, researchers just prompt a proprietary model (e.g., GPT-4/Claude/Gemini).\")\n\n    text(\"Moving up levels of abstractions boosts productivity, but\")\n    text(\"- These abstractions are leaky (in contrast to programming languages or operating systems).\")\n    text(\"- There is still fundamental research to be done that require tearing up the stack.\")\n\n    text(\"**Full understanding** of this technology is necessary for **fundamental research**.\")\n\n    text(\"This course: **understanding via building**\")\n    text(\"But there's one small problem...\")\n\n    text(\"## The industrialization of language models\")\n    image(\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Industrialisation.jpg/440px-Industrialisation.jpg\", width=400)\n\n    text(\"GPT-4 supposedly has 1.8T parameters. \"), article_link(\"https://www.hpcwire.com/2024/03/19/the-generative-ai-future-is-now-nvidias-huang-says\")\n    text(\"GPT-4 supposedly cost $100M to train. \"), article_link(\"https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/\")\n    text(\"xAI builds cluster with 200,000 H100s to train Grok. \"), article_link(\"https://www.tomshardware.com/pc-components/gpus/elon-musk-is-doubling-the-worlds-largest-ai-gpu-cluster-expanding-colossus-gpu-cluster-to-200-000-soon-has-floated-300-000-in-the-past\")\n    text(\"Stargate (OpenAI, NVIDIA, Oracle) invests $500B over 4 years. \"), article_link(\"https://openai.com/index/announcing-the-stargate-project/\")\n\n    text(\"Also, there are no public details on how frontier models are built.\")\n    text(\"From the GPT-4 technical report \"), link(gpt4), text(\":\")\n    image(\"images/gpt4-no-details.png\", width=600)\n\n    text(\"## More is different\")\n    text(\"Frontier models are out of reach for us.\")\n    text(\"But building small language models (<1B parameters in this class) might not be representative of large language models.\")\n\n    text(\"Exmaple 1: fraction of FLOPs spent in attention versus MLP changes with scale. \"), x_link(\"https://x.com/stephenroller/status/1579993017234382849\")\n    image(\"images/roller-flops.png\", width=400)\n    text(\"Example 2: emergence of behavior with scale \"), link(\"https://arxiv.org/pdf/2206.07682\")\n    image(\"images/wei-emergence-plot.png\", width=600)\n\n    text(\"## What can we learn in this class that transfers to frontier models?\")\n    text(\"There are three types of knowledge:\")\n    text(\"- **Mechanics**: how things work (what a Transformer is, how model parallelism leverages GPUs)\")\n    text(\"- **Mindset**: squeezing the most out of the hardware, taking scale seriously (scaling laws)\")\n    text(\"- **Intuitions**: which data and modeling decisions yield good accuracy\")\n\n    text(\"We can teach mechanics and mindset (these do transfer).\")\n    text(\"We can only partially teach intuitions (do not necessarily transfer across scales).\")\n\n    text(\"## Intuitions? \ud83e\udd37\")\n    text(\"Some design decisions are simply not (yet) justifiable and just come from experimentation.\")\n    text(\"Example: Noam Shazeer paper that introduced SwiGLU \"), link(shazeer_2020)\n    image(\"images/divine-benevolence.png\", width=600)\n\n    text(\"## The bitter lesson\")\n    text(\"Wrong interpretation: scale is all that matters, algorithms don't matter.\")\n    text(\"Right interpretation: algorithms that scale is what matters.\")\n    text(\"### accuracy = efficiency x resources\")\n    text(\"In fact, efficiency is way more important at larger scale (can't afford to be wasteful).\")\n    link(\"https://arxiv.org/abs/2005.04305\"), text(\" showed 44x algorithmic efficiency on ImageNet between 2012 and 2019\")\n\n    text(\"Framing: what is the best model one can build given a certain compute and data budget?\")\n    text(\"In other words, **maximize efficiency**!\")\n\n\ndef current_landscape():\n    text(\"## Pre-neural (before 2010s)\")\n    text(\"- Language model to measure the entropy of English \"), link(shannon1950)\n    text(\"- Lots of work on n-gram language models (for machine translation, speech recognition) \"), link(brants2007)\n\n    text(\"## Neural ingredients (2010s)\")\n    text(\"- First neural language model \"), link(bengio2003)\n    text(\"- Sequence-to-sequence modeling (for machine translation) \"), link(susketver2014)\n    text(\"- Adam optimizer \"), link(adam2014)\n    text(\"- Attention mechanism (for machine translation) \"), link(bahdanau2015_attention)\n    text(\"- Transformer architecture (for machine translation) \"), link(transformer_2017)\n    text(\"- Mixture of experts \"), link(moe_2017)\n    text(\"- Model parallelism \"), link(gpipe_2018), link(zero_2019), link(megatron_lm_2019)\n\n    text(\"## Early foundation models (late 2010s)\")\n    text(\"- ELMo: pretraining with LSTMs, fine-tuning helps tasks \"), link(elmo)\n    text(\"- BERT: pretraining with Transformer, fine-tuning helps tasks \"), link(bert)\n    text(\"- Google's T5 (11B): cast everything as text-to-text \"), link(t5)\n\n    text(\"## Embracing scaling, more closed\")\n    text(\"- OpenAI's GPT-2 (1.5B): fluent text, first signs of zero-shot, staged release \"), link(gpt2)\n    text(\"- Scaling laws: provide hope / predictability for scaling \"), link(kaplan_scaling_laws_2020)\n    text(\"- OpenAI's GPT-3 (175B): in-context learning, closed \"), link(gpt_3)\n    text(\"- Google's PaLM (540B): massive scale, undertrained \"), link(palm)\n    text(\"- DeepMind's Chinchilla (70B): compute-optimal scaling laws \"), link(chinchilla)\n\n    text(\"## Open models\")\n    text(\"- EleutherAI's open datasets (The Pile) and models (GPT-J) \"), link(the_pile), link(gpt_j)\n    text(\"- Meta's OPT (175B): GPT-3 replication, lots of hardware issues \"), link(opt_175b)\n    text(\"- Hugging Face / BigScience's BLOOM: focused on data sourcing \"), link(bloom)\n    text(\"- Meta's Llama models \"), link(llama), link(llama2), link(llama3)\n    text(\"- Alibaba\\'s Qwen models \"), link(qwen_2_5)\n    text(\"- DeepSeek\\'s models \"), link(deepseek_67b), link(deepseek_v2), link(deepseek_v3)\n    text(\"- AI2's OLMo 2 \"), link(olmo_7b), link(olmo2),\n\n    text(\"## Levels of openness\")\n    text(\"- Closed models (e.g., GPT-4o): API access only \"), link(gpt4)\n    text(\"- Open-weight models (e.g., DeepSeek): weights available, paper with architecture details, some training details, no data details \"), link(deepseek_v3)\n    text(\"- Open-source models (e.g., OLMo): weights and data available, paper with most details (but not necessarily the rationale, failed experiments) \"), link(olmo_7b)\n\n    text(\"## Today's frontier models\")\n    text(\"- OpenAI's o3 \"), link(\"https://openai.com/index/openai-o3-mini/\")\n    text(\"- Anthropic's Claude Sonnet 3.7 \"), link(\"https://www.anthropic.com/news/claude-3-7-sonnet\")\n    text(\"- xAI's Grok 3 \"), link(\"https://x.ai/news/grok-3\")\n    text(\"- Google's Gemini 2.5 \"), link(\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\")\n    text(\"- Meta's Llama 3.3 \"), link(\"https://ai.meta.com/blog/meta-llama-3/\")\n    text(\"- DeepSeek's r1 \"), link(deepseek_r1)\n    text(\"- Alibaba's Qwen 2.5 Max \"), link(\"https://qwenlm.github.io/blog/qwen2.5-max/\")\n    text(\"- Tencent's Hunyuan-T1 \"), link(\"https://tencent.github.io/llm.hunyuan.T1/README_EN.html\")\n\n\ndef what_is_this_program():\n    text(\"This is an *executable lecture*, a program whose execution delivers the content of a lecture.\")\n    text(\"Executable lectures make it possible to:\")\n    text(\"- view and run code (since everything is code!),\")\n    total = 0  # @inspect total\n    for x in [1, 2, 3]:  # @inspect x\n        total += x  # @inspect total\n    text(\"- see the hierarchical structure of the lecture, and\")\n    text(\"- jump to definitions and concepts: \"), link(supervised_finetuning)\n\n\ndef course_logistics():\n    text(\"All information online: \"), link(\"https://stanford-cs336.github.io/spring2025/\")\n\n    text(\"This is a 5-unit class.\")\n    text(\"Comment from Spring 2024 course evaluation: *The entire assignment was approximately the same amount of work as all 5 assignments from CS 224n plus the final project. And that's just the first homework assignment.*\")\n\n    text(\"## Why you should take this course\")\n    text(\"- You have an obsessive need to understand how things work.\")\n    text(\"- You want to build up your research engineering muscles.\")\n\n    text(\"## Why you should not take this course\")\n    text(\"- You actually want to get research done this quarter.<br>(Talk to your advisor.)\")\n    text(\"- You are interested in learning about the hottest new techniques in AI (e.g., multimodality, RAG, etc.).<br>(You should take a seminar class for that.)\")\n    text(\"- You want to get good results on your own application domain.<br>(You should just prompt or fine-tune an existing model.)\")\n\n    text(\"## How you can follow along at home\")\n    text(\"- All lecture materials and assignments will be posted online, so feel free to follow on your own.\")\n    text(\"- Lectures are recorded via [CGOE, formally SCPD](https://cgoe.stanford.edu/) and be made available on YouTube (with some lag).\")\n    text(\"- We plan to offer this class again next year.\")\n\n    text(\"## Assignments\")\n    text(\"- 5 assignments (basics, systems, scaling laws, data, alignment).\")\n    text(\"- No scaffolding code, but we provide unit tests and adapter interfaces to help you check correctness.\")\n    text(\"- Implement locally to test for correctness, then run on cluster for benchmarking (accuracy and speed).\")\n    text(\"- Leaderboard for some assignments (minimize perplexity given training budget).\")\n    text(\"- AI tools (e.g., CoPilot, Cursor) can take away from learning, so use at your own risk.\")\n\n    text(\"## Cluster\")\n    text(\"- Thanks to Together AI for providing a compute cluster. \ud83d\ude4f\")\n    text(\"- Please read [the guide](https://docs.google.com/document/d/1BSSig7zInyjDKcbNGftVxubiHlwJ-ZqahQewIzBmBOo/edit) on how to use the cluster.\")\n    text(\"- Start your assignments early, since the cluster will fill up close to the deadline!\")\n\n\ndef course_components():\n    text(\"## It's all about efficiency\")\n    text(\"Resources: data + hardware (compute, memory, communication bandwidth)\")\n    text(\"How do you train the best model given a fixed set of resources?\")\n    text(\"Example: given a Common Crawl dump and 32 H100s for 2 weeks, what should you do?\")\n\n    text(\"Design decisions:\")\n    image(\"images/design-decisions.png\", width=800)\n\n    text(\"## Overview of the course\")\n    basics()\n    systems()\n    scaling_laws()\n    data()\n    alignment()\n\n    text(\"## Efficiency drives design decisions\")\n\n    text(\"Today, we are compute-constrained, so design decisions will reflect squeezing the most out of given hardware.\")\n    text(\"- Data processing: avoid wasting precious compute updating on bad / irrelevant data\")\n    text(\"- Tokenization: working with raw bytes is elegant, but compute-inefficient with today's model architectures.\")\n    text(\"- Model architecture: many changes motivated by reducing memory or FLOPs (e.g., sharing KV caches, sliding window attention)\")\n    text(\"- Training: we can get away with a single epoch!\")\n    text(\"- Scaling laws: use less compute on smaller models to do hyperparameter tuning\")\n    text(\"- Alignment: if tune model more to desired use cases, require smaller base models\")\n\n    text(\"Tomorrow, we will become data-constrained...\")\n\n\nclass Tokenizer(ABC):\n    \"\"\"Abstract interface for a tokenizer.\"\"\"\n    def encode(self, string: str) -> list[int]:\n        raise NotImplementedError\n\n    def decode(self, indices: list[int]) -> str:\n        raise NotImplementedError\n\n\ndef basics():\n    text(\"Goal: get a basic version of the full pipeline working\")\n    text(\"Components: tokenization, model architecture, training\")\n\n    text(\"## Tokenization\")\n    text(\"Tokenizers convert between strings and sequences of integers (tokens)\")\n    image(\"images/tokenized-example.png\", width=600) \n    text(\"Intuition: break up string into popular segments\")\n\n    text(\"This course: Byte-Pair Encoding (BPE) tokenizer \"), link(sennrich_2016)\n\n    text(\"Tokenizer-free approaches: \"), link(byt5), link(megabyte), link(blt), link(tfree)\n    text(\"Use bytes directly, promising, but have not yet been scaled up to the frontier.\")\n    \n    text(\"## Architecture\")\n    text(\"Starting point: original Transformer \"), link(transformer_2017)\n    image(\"images/transformer-architecture.png\", width=500)\n\n    text(\"Variants:\")\n    text(\"- Activation functions: ReLU, SwiGLU \"), link(shazeer_2020)\n    text(\"- Positional encodings: sinusoidal, RoPE \"), link(rope_2021)\n    text(\"- Normalization: LayerNorm, RMSNorm \"), link(layernorm_2016), link(rms_norm_2019)\n    text(\"- Placement of normalization: pre-norm versus post-norm \"), link(pre_post_norm_2020)\n    text(\"- MLP: dense, mixture of experts \"), link(moe_2017)\n    text(\"- Attention: full, sliding window, linear \"), link(mistral_7b), link(\"https://arxiv.org/abs/2006.16236\")\n    text(\"- Lower-dimensional attention: group-query attention (GQA), multi-head latent attention (MLA) \"), link(gqa), link(mla)\n    text(\"- State-space models: Hyena \"), link(\"https://arxiv.org/abs/2302.10866\")\n\n    text(\"## Training\")\n    text(\"- Optimizer (e.g., AdamW, Muon, SOAP) \"), link(adam2014), link(adamw2017), link(muon), link(soap)\n    text(\"- Learning rate schedule (e.g., cosine, WSD) \"), link(cosine_learning_rate_2017), link(wsd_2024)\n    text(\"- Batch size (e..g, critical batch size) \"), link(large_batch_training_2018)\n    text(\"- Regularization (e.g., dropout, weight decay)\")\n    text(\"- Hyperparameters (number of heads, hidden dimension): grid search\")\n\n    text(\"## Assignment 1\")\n    link(title=\"[GitHub]\", url=\"https://github.com/stanford-cs336/assignment1-basics\"), link(title=\"[PDF]\", url=\"https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf\")\n    text(\"- Implement BPE tokenizer\")\n    text(\"- Implement Transformer, cross-entropy loss, AdamW optimizer, training loop\")\n    text(\"- Train on TinyStories and OpenWebText\")\n    text(\"- Leaderboard: minimize OpenWebText perplexity given 90 minutes on a H100 \"), link(title=\"[last year's leaderboard]\", url=\"https://github.com/stanford-cs336/spring2024-assignment1-basics-leaderboard\")\n\n\ndef systems():\n    text(\"Goal: squeeze the most out of the hardware\")\n    text(\"Components: kernels, parallelism, inference\")\n\n    text(\"## Kernels\")\n    text(\"What a GPU (A100) looks like:\")\n    image(\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6xoBKi5kL2dZpivFe1-zgw.jpeg\", width=800)\n    text(\"Analogy: warehouse : DRAM :: factory : SRAM\")\n    image(\"https://horace.io/img/perf_intro/factory_bandwidth.png\", width=400)\n    text(\"Trick: organize computation to maximize utilization of GPUs by minimizing data movement\")\n    text(\"Write kernels in CUDA/**Triton**/CUTLASS/ThunderKittens\")\n\n    text(\"## Parallelism\")\n    text(\"What if we have multiple GPUs (8 A100s)?\")\n    image(\"https://www.fibermall.com/blog/wp-content/uploads/2024/09/the-hardware-topology-of-a-typical-8xA100-GPU-host.png\", width=500)\n    text(\"Data movement between GPUs is even slower, but same 'minimize data movement' principle holds\")\n    text(\"Use collective operations (e.g., gather, reduce, all-reduce)\")\n    text(\"Shard (parameters, activations, gradients, optimizer states) across GPUs\")\n    text(\"How to split computation: {data,tensor,pipeline,sequence} parallelism\")\n    \n    text(\"## Inference\")\n    text(\"Goal: generate tokens given a prompt (needed to actually use models!)\")\n    text(\"Inference is also needed for reinforcement learning, test-time compute, evaluation\")\n    text(\"Globally, inference compute (every use) exceeds training compute (one-time cost)\")\n    text(\"Two phases: prefill and decode\")\n    image(\"images/prefill-decode.png\", width=500)\n    text(\"Prefill (similar to training): tokens are given, can process all at once (compute-bound)\")\n    text(\"Decode: need to generate one token at a time (memory-bound)\")\n    text(\"Methods to speed up decoding:\")\n    text(\"- Use cheaper model (via model pruning, quantization, distillation)\")\n    text(\"- Speculative decoding: use a cheaper \\\"draft\\\" model to generate multiple tokens, then use the full model to score in parallel (exact decoding!)\")\n    text(\"- Systems optimizations: KV caching, batching\")\n\n    text(\"## Assignment 2\")\n    link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment2-systems\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment2-systems/blob/master/cs336_spring2024_assignment2_systems.pdf\")\n    text(\"- Implement a fused RMSNorm kernel in Triton\")\n    text(\"- Implement distributed data parallel training\")\n    text(\"- Implement optimizer state sharding\")\n    text(\"- Benchmark and profile the implementations\")\n\n\ndef scaling_laws():\n    text(\"Goal: do experiments at small scale, predict hyperparameters/loss at large scale\")\n    text(\"Question: given a FLOPs budget ($C$), use a bigger model ($N$) or train on more tokens ($D$)?\")\n    text(\"Compute-optimal scaling laws: \"), link(kaplan_scaling_laws_2020), link(chinchilla)\n    image(\"images/chinchilla-isoflop.png\", width=800)\n    text(\"TL;DR: $D^* = 20 N^*$ (e.g., 1.4B parameter model should be trained on 28B tokens)\")\n    text(\"But this doesn't take into account inference costs!\")\n\n    text(\"## Assignment 3\")\n    link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment3-scaling\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment3-scaling/blob/master/cs336_spring2024_assignment3_scaling.pdf\")\n    text(\"- We define a training API (hyperparameters -> loss) based on previous runs\")\n    text(\"- Submit \\\"training jobs\\\" (under a FLOPs budget) and gather data points\")\n    text(\"- Fit a scaling law to the data points\")\n    text(\"- Submit predictions for scaled up hyperparameters\")\n    text(\"- Leaderboard: minimize loss given FLOPs budget\")\n\n\ndef data():\n    text(\"Question: What capabilities do we want the model to have?\")\n    text(\"Multilingual? Code? Math?\")\n    image(\"https://ar5iv.labs.arxiv.org/html/2101.00027/assets/pile_chart2.png\", width=600)\n\n    text(\"## Evaluation\")\n    text(\"- Perplexity: textbook evaluation for language models\")\n    text(\"- Standardized testing (e.g., MMLU, HellaSwag, GSM8K)\")\n    text(\"- Instruction following (e.g., AlpacaEval, IFEval, WildBench)\")\n    text(\"- Scaling test-time compute: chain-of-thought, ensembling\")\n    text(\"- LM-as-a-judge: evaluate generative tasks\")\n    text(\"- Full system: RAG, agents\")\n\n    text(\"## Data curation\")\n    text(\"- Data does not just fall from the sky.\")\n    look_at_web_data()\n    text(\"- Sources: webpages crawled from the Internet, books, arXiv papers, GitHub code, etc.\")\n    text(\"- Appeal to fair use to train on copyright data? \"), link(\"https://arxiv.org/pdf/2303.15715.pdf\")\n    text(\"- Might have to license data (e.g., Google with Reddit data) \"), article_link(\"https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/\")\n    text(\"- Formats: HTML, PDF, directories (not text!)\")\n\n    text(\"## Data processing\")\n    text(\"- Transformation: convert HTML/PDF to text (preserve content, some structure, rewriting)\")\n    text(\"- Filtering: keep high quality data, remove harmful content (via classifiers)\")\n    text(\"- Deduplication: save compute, avoid memorization; use Bloom filters or MinHash\")\n\n    text(\"## Assignment 4\")\n    link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment4-data\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment4-data/blob/master/cs336_spring2024_assignment4_data.pdf\")\n    text(\"- Convert Common Crawl HTML to text\")\n    text(\"- Train classifiers to filter for quality and harmful content\")\n    text(\"- Deduplication using MinHash\")\n    text(\"- Leaderboard: minimize perplexity given token budget\")\n\n\ndef look_at_web_data():\n    urls = get_common_crawl_urls()[:3]  # @inspect urls\n    documents = list(read_common_crawl(urls[1], limit=300))\n    random.seed(40)\n    random.shuffle(documents)\n    documents = markdownify_documents(documents[:10])\n    write_documents(documents, \"var/sample-documents.txt\")\n    link(title=\"[sample documents]\", url=\"var/sample-documents.txt\")\n    text(\"It's a wasteland out there!  Need to really process the data.\")\n\n\ndef alignment():\n    text(\"So far, a **base model** is raw potential, very good at completing the next token.\")\n    text(\"Alignment makes the model actually useful.\")\n\n    text(\"Goals of alignment:\")\n    text(\"- Get the language model to follow instructions\")\n    text(\"- Tune the style (format, length, tone, etc.)\")\n    text(\"- Incorporate safety (e.g., refusals to answer harmful questions)\")\n\n    text(\"Two phases:\")\n    supervised_finetuning()\n    learning_from_feedback()\n\n    text(\"## Assignment 5\")\n    link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment5-alignment\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment5-alignment/blob/master/cs336_spring2024_assignment5_alignment.pdf\")\n    text(\"- Implement supervised fine-tuning\")\n    text(\"- Implement Direct Preference Optimization (DPO)\")\n    text(\"- Implement Group Relative Preference Optimization (GRPO)\")\n\n\n@dataclass(frozen=True)\nclass Turn:\n    role: str\n    content: str\n\n\n@dataclass(frozen=True)\nclass ChatExample:\n    turns: list[Turn]\n\n\n@dataclass(frozen=True)\nclass PreferenceExample:\n    history: list[Turn]\n    response_a: str\n    response_b: str\n    chosen: str\n\n\ndef supervised_finetuning():\n    text(\"## Supervised finetuning (SFT)\")\n\n    text(\"Instruction data: (prompt, response) pairs\")\n    sft_data: list[ChatExample] = [\n        ChatExample(\n            turns=[\n                Turn(role=\"system\", content=\"You are a helpful assistant.\"),\n                Turn(role=\"user\", content=\"What is 1 + 1?\"),\n                Turn(role=\"assistant\", content=\"The answer is 2.\"),\n            ],\n        ),\n    ]\n    text(\"Data often involves human annotation.\")\n    text(\"Intuition: base model already has the skills, just need few examples to surface them. \"), link(lima)\n    text(\"Supervised learning: fine-tune model to maximize p(response | prompt).\")\n\n\ndef learning_from_feedback():\n    text(\"Now we have a preliminary instruction following model.\")\n    text(\"Let's make it better without expensive annotation.\")\n    \n    text(\"## Preference data\")\n    text(\"Data: generate multiple responses using model (e.g., [A, B]) to a given prompt.\")\n    text(\"User provides preferences (e.g., A < B or A > B).\")\n    preference_data: list[PreferenceExample] = [\n        PreferenceExample(\n            history=[\n                Turn(role=\"system\", content=\"You are a helpful assistant.\"),\n                Turn(role=\"user\", content=\"What is the best way to train a language model?\"),\n            ],\n            response_a=\"You should use a large dataset and train for a long time.\",\n            response_b=\"You should use a small dataset and train for a short time.\",\n            chosen=\"a\",\n        )\n    ]\n\n    text(\"## Verifiers\")\n    text(\"- Formal verifiers (e.g., for code, math)\")\n    text(\"- Learned verifiers: train against an LM-as-a-judge\")\n\n    text(\"## Algorithms\")\n    text(\"- Proximal Policy Optimization (PPO) from reinforcement learning \"), link(ppo2017), link(instruct_gpt)\n    text(\"- Direct Policy Optimization (DPO): for preference data, simpler \"), link(dpo)\n    text(\"- Group Relative Preference Optimization (GRPO): remove value function \"), link(grpo)\n\n\n############################################################\n# Tokenization\n\n# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L23\nGPT2_TOKENIZER_REGEX = \\\n    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n\n\ndef tokenization():\n    text(\"This unit was inspired by Andrej Karpathy's video on tokenization; check it out! \"), youtube_link(\"https://www.youtube.com/watch?v=zduSFxRajkE\")\n\n    intro_to_tokenization()\n    tokenization_examples()\n    character_tokenizer()\n    byte_tokenizer()\n    word_tokenizer()\n    bpe_tokenizer()\n\n    text(\"## Summary\")\n    text(\"- Tokenizer: strings <-> tokens (indices)\")\n    text(\"- Character-based, byte-based, word-based tokenization highly suboptimal\")\n    text(\"- BPE is an effective heuristic that looks at corpus statistics\")\n    text(\"- Tokenization is a necessary evil, maybe one day we'll just do it from bytes...\")\n\n@dataclass(frozen=True)\nclass BPETokenizerParams:\n    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n    vocab: dict[int, bytes]     # index -> bytes\n    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n\n\n\nclass CharacterTokenizer(Tokenizer):\n    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n    def encode(self, string: str) -> list[int]:\n        return list(map(ord, string))\n\n    def decode(self, indices: list[int]) -> str:\n        return \"\".join(map(chr, indices))\n\n\nclass ByteTokenizer(Tokenizer):\n    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n    def encode(self, string: str) -> list[int]:\n        string_bytes = string.encode(\"utf-8\")  # @inspect string_bytes\n        indices = list(map(int, string_bytes))  # @inspect indices\n        return indices\n\n    def decode(self, indices: list[int]) -> str:\n        string_bytes = bytes(indices)  # @inspect string_bytes\n        string = string_bytes.decode(\"utf-8\")  # @inspect string\n        return string\n\n\ndef merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index\n    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n    new_indices = []  # @inspect new_indices\n    i = 0  # @inspect i\n    while i < len(indices):\n        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n            new_indices.append(new_index)\n            i += 2\n        else:\n            new_indices.append(indices[i])\n            i += 1\n    return new_indices\n\n\nclass BPETokenizer(Tokenizer):\n    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n    def __init__(self, params: BPETokenizerParams):\n        self.params = params\n\n    def encode(self, string: str) -> list[int]:\n        indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n        # Note: this is a very slow implementation\n        for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index\n            indices = merge(indices, pair, new_index)\n        return indices\n\n    def decode(self, indices: list[int]) -> str:\n        bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list\n        string = b\"\".join(bytes_list).decode(\"utf-8\")  # @inspect string\n        return string\n\n\ndef get_compression_ratio(string: str, indices: list[int]) -> float:\n    \"\"\"Given `string` that has been tokenized into `indices`, .\"\"\"\n    num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes\n    num_tokens = len(indices)                       # @inspect num_tokens\n    return num_bytes / num_tokens\n\n\ndef get_gpt2_tokenizer():\n    # Code: https://github.com/openai/tiktoken\n    # You can use cl100k_base for the gpt3.5-turbo or gpt4 tokenizer\n    return tiktoken.get_encoding(\"gpt2\")\n\n\ndef intro_to_tokenization():\n    text(\"Raw text is generally represented as Unicode strings.\")\n    string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\"\n\n    text(\"A language model places a probability distribution over sequences of tokens (usually represented by integer indices).\")\n    indices = [15496, 11, 995, 0]\n\n    text(\"So we need a procedure that *encodes* strings into tokens.\")\n    text(\"We also need a procedure that *decodes* tokens back into strings.\")\n    text(\"A \"), link(Tokenizer), text(\" is a class that implements the encode and decode methods.\")\n    text(\"The **vocabulary size** is number of possible tokens (integers).\")\n\n\ndef tokenization_examples():\n    text(\"To get a feel for how tokenizers work, play with this \"), link(title=\"interactive site\", url=\"https://tiktokenizer.vercel.app/?encoder=gpt2\")\n\n    text(\"## Observations\")\n    text(\"- A word and its preceding space are part of the same token (e.g., \\\" world\\\").\")\n    text(\"- A word at the beginning and in the middle are represented differently (e.g., \\\"hello hello\\\").\")\n    text(\"- Numbers are tokenized into every few digits.\")\n\n    text(\"Here's the GPT-2 tokenizer from OpenAI (tiktoken) in action.\")\n    tokenizer = get_gpt2_tokenizer()\n    string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\"  # @inspect string\n\n    text(\"Check that encode() and decode() roundtrip:\")\n    indices = tokenizer.encode(string)  # @inspect indices\n    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n    assert string == reconstructed_string\n    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio\n\n\ndef character_tokenizer():\n    text(\"## Character-based tokenization\")\n\n    text(\"A Unicode string is a sequence of Unicode characters.\")\n    text(\"Each character can be converted into a code point (integer) via `ord`.\")\n    assert ord(\"a\") == 97\n    assert ord(\"\ud83c\udf0d\") == 127757\n    text(\"It can be converted back via `chr`.\")\n    assert chr(97) == \"a\"\n    assert chr(127757) == \"\ud83c\udf0d\"\n\n    text(\"Now let's build a `Tokenizer` and make sure it round-trips:\")\n    tokenizer = CharacterTokenizer()\n    string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\"  # @inspect string\n    indices = tokenizer.encode(string)  # @inspect indices\n    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n    assert string == reconstructed_string\n\n    text(\"There are approximately 150K Unicode characters. \"), link(title=\"[Wikipedia]\", url=\"https://en.wikipedia.org/wiki/List_of_Unicode_characters\")\n    vocabulary_size = max(indices) + 1  # This is a lower bound @inspect vocabulary_size\n    text(\"Problem 1: this is a very large vocabulary.\")\n    text(\"Problem 2: many characters are quite rare (e.g., \ud83c\udf0d), which is inefficient use of the vocabulary.\")\n    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio\n\n\ndef byte_tokenizer():\n    text(\"## Byte-based tokenization\")\n\n    text(\"Unicode strings can be represented as a sequence of bytes, which can be represented by integers between 0 and 255.\")\n    text(\"The most common Unicode encoding is \"), link(title=\"UTF-8\", url=\"https://en.wikipedia.org/wiki/UTF-8\")\n\n    text(\"Some Unicode characters are represented by one byte:\")\n    assert bytes(\"a\", encoding=\"utf-8\") == b\"a\"\n    text(\"Others take multiple bytes:\")\n    assert bytes(\"\ud83c\udf0d\", encoding=\"utf-8\") == b\"\\xf0\\x9f\\x8c\\x8d\"\n\n    text(\"Now let's build a `Tokenizer` and make sure it round-trips:\")\n    tokenizer = ByteTokenizer()\n    string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\"  # @inspect string\n    indices = tokenizer.encode(string)  # @inspect indices\n    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n    assert string == reconstructed_string\n\n    text(\"The vocabulary is nice and small: a byte can represent 256 values.\")\n    vocabulary_size = 256  # @inspect vocabulary_size\n    text(\"What about the compression rate?\")\n    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio\n    assert compression_ratio == 1\n    text(\"The compression ratio is terrible, which means the sequences will be too long.\")\n    text(\"Given that the context length of a Transformer is limited (since attention is quadratic), this is not looking great...\")\n\n\ndef word_tokenizer():\n    text(\"## Word-based tokenization\")\n\n    text(\"Another approach (closer to what was done classically in NLP) is to split strings into words.\")\n    string = \"I'll say supercalifragilisticexpialidocious!\"\n\n    segments = regex.findall(r\"\\w+|.\", string)  # @inspect segments\n    text(\"This regular expression keeps all alphanumeric characters together (words).\")\n\n    text(\"Here is a fancier version:\")\n    pattern = GPT2_TOKENIZER_REGEX  # @inspect pattern\n    segments = regex.findall(pattern, string)  # @inspect segments\n\n    text(\"To turn this into a `Tokenizer`, we need to map these segments into integers.\")\n    text(\"Then, we can build a mapping from each segment into an integer.\")\n\n    text(\"But there are problems:\")\n    text(\"- The number of words is huge (like for Unicode characters).\")\n    text(\"- Many words are rare and the model won't learn much about them.\")\n    text(\"- This doesn't obviously provide a fixed vocabulary size.\")\n\n    text(\"New words we haven't seen during training get a special UNK token, which is ugly and can mess up perplexity calculations.\")\n\n    vocabulary_size = \"Number of distinct segments in the training data\"\n    compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio\n\n\ndef bpe_tokenizer():\n    text(\"## Byte Pair Encoding (BPE)\")\n    link(title=\"[Wikipedia]\", url=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\")\n    text(\"The BPE algorithm was introduced by Philip Gage in 1994 for data compression. \"), article_link(\"http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM\")\n    text(\"It was adapted to NLP for neural machine translation. \"), link(sennrich_2016)\n    text(\"(Previously, papers had been using word-based tokenization.)\")\n    text(\"BPE was then used by GPT-2. \"), link(gpt2)\n\n    text(\"Basic idea: *train* the tokenizer on raw text to automatically determine the vocabulary.\")\n    text(\"Intuition: common sequences of characters are represented by a single token, rare sequences are represented by many tokens.\")\n\n    text(\"The GPT-2 paper used word-based tokenization to break up the text into inital segments and run the original BPE algorithm on each segment.\")\n    text(\"Sketch: start with each byte as a token, and successively merge the most common pair of adjacent tokens.\")\n\n    text(\"## Training the tokenizer\")\n    string = \"the cat in the hat\"  # @inspect string\n    params = train_bpe(string, num_merges=3)\n\n    text(\"## Using the tokenizer\")\n    text(\"Now, given a new text, we can encode it.\")\n    tokenizer = BPETokenizer(params)\n    string = \"the quick brown fox\"  # @inspect string\n    indices = tokenizer.encode(string)  # @inspect indices\n    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n    assert string == reconstructed_string\n\n    text(\"In Assignment 1, you will go beyond this in the following ways:\")\n    text(\"- encode() currently loops over all merges. Only loop over merges that matter.\")\n    text(\"- Detect and preserve special tokens (e.g., <|endoftext|>).\")\n    text(\"- Use pre-tokenization (e.g., the GPT-2 tokenizer regex).\")\n    text(\"- Try to make the implementation as fast as possible.\")\n\n\ndef train_bpe(string: str, num_merges: int) -> BPETokenizerParams:  # @inspect string, @inspect num_merges\n    text(\"Start with the list of bytes of `string`.\")\n    indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n\n    for i in range(num_merges):\n        text(\"Count the number of occurrences of each pair of tokens\")\n        counts = defaultdict(int)\n        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n            counts[(index1, index2)] += 1  # @inspect counts\n\n        text(\"Find the most common pair.\")\n        pair = max(counts, key=counts.get)  # @inspect pair\n        index1, index2 = pair\n\n        text(\"Merge that pair.\")\n        new_index = 256 + i  # @inspect new_index\n        merges[pair] = new_index  # @inspect merges\n        vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab\n        indices = merge(indices, pair, new_index)  # @inspect indices\n\n    return BPETokenizerParams(vocab=vocab, merges=merges)\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "steps": [
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 23,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 38,
          "function_name": "welcome",
          "code": "def welcome():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 39,
          "function_name": "welcome",
          "code": "text(\"## CS336: Language Models From Scratch (Spring 2025)\"),"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## CS336: Language Models From Scratch (Spring 2025)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 41,
          "function_name": "welcome",
          "code": "image(\"images/course-staff.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/course-staff.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 43,
          "function_name": "welcome",
          "code": "text(\"This is the second offering of CS336.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is the second offering of CS336.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 44,
          "function_name": "welcome",
          "code": "text(\"Stanford edition has grown by 50%.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Stanford edition has grown by 50%.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 45,
          "function_name": "welcome",
          "code": "text(\"Lectures will be posted on YouTube and be made available to the whole world.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Lectures will be posted on YouTube and be made available to the whole world.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 24,
          "function_name": "main",
          "code": "welcome()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 48,
          "function_name": "why_this_course_exists",
          "code": "def why_this_course_exists():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 49,
          "function_name": "why_this_course_exists",
          "code": "text(\"## Why did we make this course?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Why did we make this course?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 51,
          "function_name": "why_this_course_exists",
          "code": "text(\"Let's ask GPT-4 \"), link(gpt4)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's ask GPT-4 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GPT-4 Technical Report",
            "authors": [
              "OpenAI",
              "Josh Achiam",
              "Steven Adler",
              "Sandhini Agarwal",
              "Lama Ahmad",
              "Ilge Akkaya",
              "Florencia Leoni Aleman",
              "Diogo Almeida",
              "Janko Altenschmidt",
              "Sam Altman",
              "Shyamal Anadkat",
              "Red Avila",
              "Igor Babuschkin",
              "Suchir Balaji",
              "Valerie Balcom",
              "Paul Baltescu",
              "Haiming Bao",
              "Mohammad Bavarian",
              "Jeff Belgum",
              "Irwan Bello",
              "Jake Berdine",
              "Gabriel Bernadett-Shapiro",
              "Christopher Berner",
              "Lenny Bogdonoff",
              "Oleg Boiko",
              "Madelaine Boyd",
              "Anna-Luisa Brakman",
              "Greg Brockman",
              "Tim Brooks",
              "Miles Brundage",
              "Kevin Button",
              "Trevor Cai",
              "Rosie Campbell",
              "Andrew Cann",
              "Brittany Carey",
              "Chelsea Carlson",
              "Rory Carmichael",
              "Brooke Chan",
              "Che Chang",
              "Fotis Chantzis",
              "Derek Chen",
              "Sully Chen",
              "Ruby Chen",
              "Jason Chen",
              "Mark Chen",
              "Ben Chess",
              "Chester Cho",
              "Casey Chu",
              "Hyung Won Chung",
              "Dave Cummings",
              "Jeremiah Currier",
              "Yunxing Dai",
              "Cory Decareaux",
              "Thomas Degry",
              "Noah Deutsch",
              "Damien Deville",
              "Arka Dhar",
              "David Dohan",
              "Steve Dowling",
              "Sheila Dunning",
              "Adrien Ecoffet",
              "Atty Eleti",
              "Tyna Eloundou",
              "David Farhi",
              "Liam Fedus",
              "Niko Felix",
              "Sim\u00f3n Posada Fishman",
              "Juston Forte",
              "Isabella Fulford",
              "Leo Gao",
              "Elie Georges",
              "Christian Gibson",
              "Vik Goel",
              "Tarun Gogineni",
              "Gabriel Goh",
              "Rapha Gontijo-Lopes",
              "Jonathan Gordon",
              "Morgan Grafstein",
              "Scott Gray",
              "Ryan Greene",
              "Joshua Gross",
              "Shixiang Shane Gu",
              "Yufei Guo",
              "Chris Hallacy",
              "Jesse Han",
              "Jeff Harris",
              "Yuchen He",
              "Mike Heaton",
              "Johannes Heidecke",
              "Chris Hesse",
              "Alan Hickey",
              "Wade Hickey",
              "Peter Hoeschele",
              "Brandon Houghton",
              "Kenny Hsu",
              "Shengli Hu",
              "Xin Hu",
              "Joost Huizinga",
              "Shantanu Jain",
              "Shawn Jain",
              "Joanne Jang",
              "Angela Jiang",
              "Roger Jiang",
              "Haozhun Jin",
              "Denny Jin",
              "Shino Jomoto",
              "Billie Jonn",
              "Heewoo Jun",
              "Tomer Kaftan",
              "\u0141ukasz Kaiser",
              "Ali Kamali",
              "Ingmar Kanitscheider",
              "Nitish Shirish Keskar",
              "Tabarak Khan",
              "Logan Kilpatrick",
              "Jong Wook Kim",
              "Christina Kim",
              "Yongjik Kim",
              "Jan Hendrik Kirchner",
              "Jamie Kiros",
              "Matt Knight",
              "Daniel Kokotajlo",
              "\u0141ukasz Kondraciuk",
              "Andrew Kondrich",
              "Aris Konstantinidis",
              "Kyle Kosic",
              "Gretchen Krueger",
              "Vishal Kuo",
              "Michael Lampe",
              "Ikai Lan",
              "Teddy Lee",
              "Jan Leike",
              "Jade Leung",
              "Daniel Levy",
              "Chak Ming Li",
              "Rachel Lim",
              "Molly Lin",
              "Stephanie Lin",
              "Mateusz Litwin",
              "Theresa Lopez",
              "Ryan Lowe",
              "Patricia Lue",
              "Anna Makanju",
              "Kim Malfacini",
              "Sam Manning",
              "Todor Markov",
              "Yaniv Markovski",
              "Bianca Martin",
              "Katie Mayer",
              "Andrew Mayne",
              "Bob McGrew",
              "Scott Mayer McKinney",
              "Christine McLeavey",
              "Paul McMillan",
              "Jake McNeil",
              "David Medina",
              "Aalok Mehta",
              "Jacob Menick",
              "Luke Metz",
              "Andrey Mishchenko",
              "Pamela Mishkin",
              "Vinnie Monaco",
              "Evan Morikawa",
              "Daniel Mossing",
              "Tong Mu",
              "Mira Murati",
              "Oleg Murk",
              "David M\u00e9ly",
              "Ashvin Nair",
              "Reiichiro Nakano",
              "Rajeev Nayak",
              "Arvind Neelakantan",
              "Richard Ngo",
              "Hyeonwoo Noh",
              "Long Ouyang",
              "Cullen O'Keefe",
              "Jakub Pachocki",
              "Alex Paino",
              "Joe Palermo",
              "Ashley Pantuliano",
              "Giambattista Parascandolo",
              "Joel Parish",
              "Emy Parparita",
              "Alex Passos",
              "Mikhail Pavlov",
              "Andrew Peng",
              "Adam Perelman",
              "Filipe de Avila Belbute Peres",
              "Michael Petrov",
              "Henrique Ponde de Oliveira Pinto",
              "Michael",
              "Pokorny",
              "Michelle Pokrass",
              "Vitchyr H. Pong",
              "Tolly Powell",
              "Alethea Power",
              "Boris Power",
              "Elizabeth Proehl",
              "Raul Puri",
              "Alec Radford",
              "Jack Rae",
              "Aditya Ramesh",
              "Cameron Raymond",
              "Francis Real",
              "Kendra Rimbach",
              "Carl Ross",
              "Bob Rotsted",
              "Henri Roussez",
              "Nick Ryder",
              "Mario Saltarelli",
              "Ted Sanders",
              "Shibani Santurkar",
              "Girish Sastry",
              "Heather Schmidt",
              "David Schnurr",
              "John Schulman",
              "Daniel Selsam",
              "Kyla Sheppard",
              "Toki Sherbakov",
              "Jessica Shieh",
              "Sarah Shoker",
              "Pranav Shyam",
              "Szymon Sidor",
              "Eric Sigler",
              "Maddie Simens",
              "Jordan Sitkin",
              "Katarina Slama",
              "Ian Sohl",
              "Benjamin Sokolowsky",
              "Yang Song",
              "Natalie Staudacher",
              "Felipe Petroski Such",
              "Natalie Summers",
              "Ilya Sutskever",
              "Jie Tang",
              "Nikolas Tezak",
              "Madeleine B. Thompson",
              "Phil Tillet",
              "Amin Tootoonchian",
              "Elizabeth Tseng",
              "Preston Tuggle",
              "Nick Turley",
              "Jerry Tworek",
              "Juan Felipe Cer\u00f3n Uribe",
              "Andrea Vallone",
              "Arun Vijayvergiya",
              "Chelsea Voss",
              "Carroll Wainwright",
              "Justin Jay Wang",
              "Alvin Wang",
              "Ben Wang",
              "Jonathan Ward",
              "Jason Wei",
              "CJ Weinmann",
              "Akila Welihinda",
              "Peter Welinder",
              "Jiayi Weng",
              "Lilian Weng",
              "Matt Wiethoff",
              "Dave Willner",
              "Clemens Winter",
              "Samuel Wolrich",
              "Hannah Wong",
              "Lauren Workman",
              "Sherwin Wu",
              "Jeff Wu",
              "Michael Wu",
              "Kai Xiao",
              "Tao Xu",
              "Sarah Yoo",
              "Kevin Yu",
              "Qiming Yuan",
              "Wojciech Zaremba",
              "Rowan Zellers",
              "Chong Zhang",
              "Marvin Zhang",
              "Shengjia Zhao",
              "Tianhao Zheng",
              "Juntang Zhuang",
              "William Zhuk",
              "Barret Zoph"
            ],
            "organization": "OpenAI",
            "date": "2023-03-15T17:15:04Z",
            "url": "https://arxiv.org/pdf/2303.08774.pdf",
            "description": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
            "notes": "No details on the data or model architecture."
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 52,
          "function_name": "why_this_course_exists",
          "code": "response = query_gpt4o(prompt=\"Why teach a course on building language models from scratch? Answer in one sentence.\")  # @inspect response"
        }
      ],
      "env": {
        "response": "Teaching a course on building language models from scratch provides foundational understanding of natural language processing techniques, fosters innovation, and enables effective adaptation of models to specific tasks and domains."
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 54,
          "function_name": "why_this_course_exists",
          "code": "text(\"Problem: researchers are becoming **disconnected** from the underlying technology.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem: researchers are becoming **disconnected** from the underlying technology.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 55,
          "function_name": "why_this_course_exists",
          "code": "text(\"8 years ago, researchers would implement and train their own models.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "8 years ago, researchers would implement and train their own models.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 56,
          "function_name": "why_this_course_exists",
          "code": "text(\"6 years ago, researchers would download a model (e.g., BERT) and fine-tune it.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "6 years ago, researchers would download a model (e.g., BERT) and fine-tune it.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 57,
          "function_name": "why_this_course_exists",
          "code": "text(\"Today, researchers just prompt a proprietary model (e.g., GPT-4/Claude/Gemini).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Today, researchers just prompt a proprietary model (e.g., GPT-4/Claude/Gemini).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 59,
          "function_name": "why_this_course_exists",
          "code": "text(\"Moving up levels of abstractions boosts productivity, but\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Moving up levels of abstractions boosts productivity, but",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 60,
          "function_name": "why_this_course_exists",
          "code": "text(\"- These abstractions are leaky (in contrast to programming languages or operating systems).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- These abstractions are leaky (in contrast to programming languages or operating systems).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 61,
          "function_name": "why_this_course_exists",
          "code": "text(\"- There is still fundamental research to be done that require tearing up the stack.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- There is still fundamental research to be done that require tearing up the stack.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 63,
          "function_name": "why_this_course_exists",
          "code": "text(\"**Full understanding** of this technology is necessary for **fundamental research**.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Full understanding** of this technology is necessary for **fundamental research**.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 65,
          "function_name": "why_this_course_exists",
          "code": "text(\"This course: **understanding via building**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This course: **understanding via building**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 66,
          "function_name": "why_this_course_exists",
          "code": "text(\"But there's one small problem...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But there's one small problem...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 68,
          "function_name": "why_this_course_exists",
          "code": "text(\"## The industrialization of language models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## The industrialization of language models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 69,
          "function_name": "why_this_course_exists",
          "code": "image(\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Industrialisation.jpg/440px-Industrialisation.jpg\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-23a59a0326a766e0ce9b5e47c6dcad52-https_upload_wikimedia_org_wikipedia_commons_thumb_c_cc_Industrialisation_jpg_440px-Industrialisation_jpg",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 71,
          "function_name": "why_this_course_exists",
          "code": "text(\"GPT-4 supposedly has 1.8T parameters. \"), article_link(\"https://www.hpcwire.com/2024/03/19/the-generative-ai-future-is-now-nvidias-huang-says\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "GPT-4 supposedly has 1.8T parameters. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.hpcwire.com/2024/03/19/the-generative-ai-future-is-now-nvidias-huang-says",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 72,
          "function_name": "why_this_course_exists",
          "code": "text(\"GPT-4 supposedly cost $100M to train. \"), article_link(\"https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "GPT-4 supposedly cost $100M to train. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 73,
          "function_name": "why_this_course_exists",
          "code": "text(\"xAI builds cluster with 200,000 H100s to train Grok. \"), article_link(\"https://www.tomshardware.com/pc-components/gpus/elon-musk-is-doubling-the-worlds-largest-ai-gpu-cluster-expanding-colossus-gpu-cluster-to-200-000-soon-has-floated-300-000-in-the-past\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "xAI builds cluster with 200,000 H100s to train Grok. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.tomshardware.com/pc-components/gpus/elon-musk-is-doubling-the-worlds-largest-ai-gpu-cluster-expanding-colossus-gpu-cluster-to-200-000-soon-has-floated-300-000-in-the-past",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 74,
          "function_name": "why_this_course_exists",
          "code": "text(\"Stargate (OpenAI, NVIDIA, Oracle) invests $500B over 4 years. \"), article_link(\"https://openai.com/index/announcing-the-stargate-project/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Stargate (OpenAI, NVIDIA, Oracle) invests $500B over 4 years. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://openai.com/index/announcing-the-stargate-project/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 76,
          "function_name": "why_this_course_exists",
          "code": "text(\"Also, there are no public details on how frontier models are built.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Also, there are no public details on how frontier models are built.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 77,
          "function_name": "why_this_course_exists",
          "code": "text(\"From the GPT-4 technical report \"), link(gpt4), text(\":\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "From the GPT-4 technical report ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GPT-4 Technical Report",
            "authors": [
              "OpenAI",
              "Josh Achiam",
              "Steven Adler",
              "Sandhini Agarwal",
              "Lama Ahmad",
              "Ilge Akkaya",
              "Florencia Leoni Aleman",
              "Diogo Almeida",
              "Janko Altenschmidt",
              "Sam Altman",
              "Shyamal Anadkat",
              "Red Avila",
              "Igor Babuschkin",
              "Suchir Balaji",
              "Valerie Balcom",
              "Paul Baltescu",
              "Haiming Bao",
              "Mohammad Bavarian",
              "Jeff Belgum",
              "Irwan Bello",
              "Jake Berdine",
              "Gabriel Bernadett-Shapiro",
              "Christopher Berner",
              "Lenny Bogdonoff",
              "Oleg Boiko",
              "Madelaine Boyd",
              "Anna-Luisa Brakman",
              "Greg Brockman",
              "Tim Brooks",
              "Miles Brundage",
              "Kevin Button",
              "Trevor Cai",
              "Rosie Campbell",
              "Andrew Cann",
              "Brittany Carey",
              "Chelsea Carlson",
              "Rory Carmichael",
              "Brooke Chan",
              "Che Chang",
              "Fotis Chantzis",
              "Derek Chen",
              "Sully Chen",
              "Ruby Chen",
              "Jason Chen",
              "Mark Chen",
              "Ben Chess",
              "Chester Cho",
              "Casey Chu",
              "Hyung Won Chung",
              "Dave Cummings",
              "Jeremiah Currier",
              "Yunxing Dai",
              "Cory Decareaux",
              "Thomas Degry",
              "Noah Deutsch",
              "Damien Deville",
              "Arka Dhar",
              "David Dohan",
              "Steve Dowling",
              "Sheila Dunning",
              "Adrien Ecoffet",
              "Atty Eleti",
              "Tyna Eloundou",
              "David Farhi",
              "Liam Fedus",
              "Niko Felix",
              "Sim\u00f3n Posada Fishman",
              "Juston Forte",
              "Isabella Fulford",
              "Leo Gao",
              "Elie Georges",
              "Christian Gibson",
              "Vik Goel",
              "Tarun Gogineni",
              "Gabriel Goh",
              "Rapha Gontijo-Lopes",
              "Jonathan Gordon",
              "Morgan Grafstein",
              "Scott Gray",
              "Ryan Greene",
              "Joshua Gross",
              "Shixiang Shane Gu",
              "Yufei Guo",
              "Chris Hallacy",
              "Jesse Han",
              "Jeff Harris",
              "Yuchen He",
              "Mike Heaton",
              "Johannes Heidecke",
              "Chris Hesse",
              "Alan Hickey",
              "Wade Hickey",
              "Peter Hoeschele",
              "Brandon Houghton",
              "Kenny Hsu",
              "Shengli Hu",
              "Xin Hu",
              "Joost Huizinga",
              "Shantanu Jain",
              "Shawn Jain",
              "Joanne Jang",
              "Angela Jiang",
              "Roger Jiang",
              "Haozhun Jin",
              "Denny Jin",
              "Shino Jomoto",
              "Billie Jonn",
              "Heewoo Jun",
              "Tomer Kaftan",
              "\u0141ukasz Kaiser",
              "Ali Kamali",
              "Ingmar Kanitscheider",
              "Nitish Shirish Keskar",
              "Tabarak Khan",
              "Logan Kilpatrick",
              "Jong Wook Kim",
              "Christina Kim",
              "Yongjik Kim",
              "Jan Hendrik Kirchner",
              "Jamie Kiros",
              "Matt Knight",
              "Daniel Kokotajlo",
              "\u0141ukasz Kondraciuk",
              "Andrew Kondrich",
              "Aris Konstantinidis",
              "Kyle Kosic",
              "Gretchen Krueger",
              "Vishal Kuo",
              "Michael Lampe",
              "Ikai Lan",
              "Teddy Lee",
              "Jan Leike",
              "Jade Leung",
              "Daniel Levy",
              "Chak Ming Li",
              "Rachel Lim",
              "Molly Lin",
              "Stephanie Lin",
              "Mateusz Litwin",
              "Theresa Lopez",
              "Ryan Lowe",
              "Patricia Lue",
              "Anna Makanju",
              "Kim Malfacini",
              "Sam Manning",
              "Todor Markov",
              "Yaniv Markovski",
              "Bianca Martin",
              "Katie Mayer",
              "Andrew Mayne",
              "Bob McGrew",
              "Scott Mayer McKinney",
              "Christine McLeavey",
              "Paul McMillan",
              "Jake McNeil",
              "David Medina",
              "Aalok Mehta",
              "Jacob Menick",
              "Luke Metz",
              "Andrey Mishchenko",
              "Pamela Mishkin",
              "Vinnie Monaco",
              "Evan Morikawa",
              "Daniel Mossing",
              "Tong Mu",
              "Mira Murati",
              "Oleg Murk",
              "David M\u00e9ly",
              "Ashvin Nair",
              "Reiichiro Nakano",
              "Rajeev Nayak",
              "Arvind Neelakantan",
              "Richard Ngo",
              "Hyeonwoo Noh",
              "Long Ouyang",
              "Cullen O'Keefe",
              "Jakub Pachocki",
              "Alex Paino",
              "Joe Palermo",
              "Ashley Pantuliano",
              "Giambattista Parascandolo",
              "Joel Parish",
              "Emy Parparita",
              "Alex Passos",
              "Mikhail Pavlov",
              "Andrew Peng",
              "Adam Perelman",
              "Filipe de Avila Belbute Peres",
              "Michael Petrov",
              "Henrique Ponde de Oliveira Pinto",
              "Michael",
              "Pokorny",
              "Michelle Pokrass",
              "Vitchyr H. Pong",
              "Tolly Powell",
              "Alethea Power",
              "Boris Power",
              "Elizabeth Proehl",
              "Raul Puri",
              "Alec Radford",
              "Jack Rae",
              "Aditya Ramesh",
              "Cameron Raymond",
              "Francis Real",
              "Kendra Rimbach",
              "Carl Ross",
              "Bob Rotsted",
              "Henri Roussez",
              "Nick Ryder",
              "Mario Saltarelli",
              "Ted Sanders",
              "Shibani Santurkar",
              "Girish Sastry",
              "Heather Schmidt",
              "David Schnurr",
              "John Schulman",
              "Daniel Selsam",
              "Kyla Sheppard",
              "Toki Sherbakov",
              "Jessica Shieh",
              "Sarah Shoker",
              "Pranav Shyam",
              "Szymon Sidor",
              "Eric Sigler",
              "Maddie Simens",
              "Jordan Sitkin",
              "Katarina Slama",
              "Ian Sohl",
              "Benjamin Sokolowsky",
              "Yang Song",
              "Natalie Staudacher",
              "Felipe Petroski Such",
              "Natalie Summers",
              "Ilya Sutskever",
              "Jie Tang",
              "Nikolas Tezak",
              "Madeleine B. Thompson",
              "Phil Tillet",
              "Amin Tootoonchian",
              "Elizabeth Tseng",
              "Preston Tuggle",
              "Nick Turley",
              "Jerry Tworek",
              "Juan Felipe Cer\u00f3n Uribe",
              "Andrea Vallone",
              "Arun Vijayvergiya",
              "Chelsea Voss",
              "Carroll Wainwright",
              "Justin Jay Wang",
              "Alvin Wang",
              "Ben Wang",
              "Jonathan Ward",
              "Jason Wei",
              "CJ Weinmann",
              "Akila Welihinda",
              "Peter Welinder",
              "Jiayi Weng",
              "Lilian Weng",
              "Matt Wiethoff",
              "Dave Willner",
              "Clemens Winter",
              "Samuel Wolrich",
              "Hannah Wong",
              "Lauren Workman",
              "Sherwin Wu",
              "Jeff Wu",
              "Michael Wu",
              "Kai Xiao",
              "Tao Xu",
              "Sarah Yoo",
              "Kevin Yu",
              "Qiming Yuan",
              "Wojciech Zaremba",
              "Rowan Zellers",
              "Chong Zhang",
              "Marvin Zhang",
              "Shengjia Zhao",
              "Tianhao Zheng",
              "Juntang Zhuang",
              "William Zhuk",
              "Barret Zoph"
            ],
            "organization": "OpenAI",
            "date": "2023-03-15T17:15:04Z",
            "url": "https://arxiv.org/pdf/2303.08774.pdf",
            "description": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
            "notes": "No details on the data or model architecture."
          },
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": ":",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 78,
          "function_name": "why_this_course_exists",
          "code": "image(\"images/gpt4-no-details.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/gpt4-no-details.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 80,
          "function_name": "why_this_course_exists",
          "code": "text(\"## More is different\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## More is different",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 81,
          "function_name": "why_this_course_exists",
          "code": "text(\"Frontier models are out of reach for us.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Frontier models are out of reach for us.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 82,
          "function_name": "why_this_course_exists",
          "code": "text(\"But building small language models (<1B parameters in this class) might not be representative of large language models.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But building small language models (<1B parameters in this class) might not be representative of large language models.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 84,
          "function_name": "why_this_course_exists",
          "code": "text(\"Exmaple 1: fraction of FLOPs spent in attention versus MLP changes with scale. \"), x_link(\"https://x.com/stephenroller/status/1579993017234382849\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Exmaple 1: fraction of FLOPs spent in attention versus MLP changes with scale. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [X]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://x.com/stephenroller/status/1579993017234382849",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 85,
          "function_name": "why_this_course_exists",
          "code": "image(\"images/roller-flops.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/roller-flops.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 86,
          "function_name": "why_this_course_exists",
          "code": "text(\"Example 2: emergence of behavior with scale \"), link(\"https://arxiv.org/pdf/2206.07682\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example 2: emergence of behavior with scale ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Emergent Abilities of Large Language Models",
            "authors": [
              "Jason Wei",
              "Yi Tay",
              "Rishi Bommasani",
              "Colin Raffel",
              "Barret Zoph",
              "Sebastian Borgeaud",
              "Dani Yogatama",
              "Maarten Bosma",
              "Denny Zhou",
              "Donald Metzler",
              "Ed H. Chi",
              "Tatsunori Hashimoto",
              "Oriol Vinyals",
              "Percy Liang",
              "Jeff Dean",
              "William Fedus"
            ],
            "organization": null,
            "date": "2022-06-15T17:32:01Z",
            "url": "https://arxiv.org/pdf/2206.07682",
            "description": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 87,
          "function_name": "why_this_course_exists",
          "code": "image(\"images/wei-emergence-plot.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/wei-emergence-plot.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 89,
          "function_name": "why_this_course_exists",
          "code": "text(\"## What can we learn in this class that transfers to frontier models?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## What can we learn in this class that transfers to frontier models?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 90,
          "function_name": "why_this_course_exists",
          "code": "text(\"There are three types of knowledge:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "There are three types of knowledge:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 91,
          "function_name": "why_this_course_exists",
          "code": "text(\"- **Mechanics**: how things work (what a Transformer is, how model parallelism leverages GPUs)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **Mechanics**: how things work (what a Transformer is, how model parallelism leverages GPUs)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 92,
          "function_name": "why_this_course_exists",
          "code": "text(\"- **Mindset**: squeezing the most out of the hardware, taking scale seriously (scaling laws)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **Mindset**: squeezing the most out of the hardware, taking scale seriously (scaling laws)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 93,
          "function_name": "why_this_course_exists",
          "code": "text(\"- **Intuitions**: which data and modeling decisions yield good accuracy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **Intuitions**: which data and modeling decisions yield good accuracy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 95,
          "function_name": "why_this_course_exists",
          "code": "text(\"We can teach mechanics and mindset (these do transfer).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We can teach mechanics and mindset (these do transfer).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 96,
          "function_name": "why_this_course_exists",
          "code": "text(\"We can only partially teach intuitions (do not necessarily transfer across scales).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We can only partially teach intuitions (do not necessarily transfer across scales).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 98,
          "function_name": "why_this_course_exists",
          "code": "text(\"## Intuitions? \ud83e\udd37\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Intuitions? \ud83e\udd37",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 99,
          "function_name": "why_this_course_exists",
          "code": "text(\"Some design decisions are simply not (yet) justifiable and just come from experimentation.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Some design decisions are simply not (yet) justifiable and just come from experimentation.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 100,
          "function_name": "why_this_course_exists",
          "code": "text(\"Example: Noam Shazeer paper that introduced SwiGLU \"), link(shazeer_2020)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example: Noam Shazeer paper that introduced SwiGLU ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GLU Variants Improve Transformer",
            "authors": [
              "Noam Shazeer"
            ],
            "organization": "Google",
            "date": "2020-02-12T19:57:13Z",
            "url": "https://arxiv.org/pdf/2002.05202.pdf",
            "description": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.",
            "notes": "Experiments with different activation functions\nActivation functions: ReLU, GeLU, Swish\nApply idea of gated units (GLU): ReGLU, GeGLU, SwiGLU\nFFN-SwiGLU = Swish(x W1) * xV W2\nHave 3 matrices now, so make hidden dimension 2/3 of the 2 matrix version"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 101,
          "function_name": "why_this_course_exists",
          "code": "image(\"images/divine-benevolence.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/divine-benevolence.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 103,
          "function_name": "why_this_course_exists",
          "code": "text(\"## The bitter lesson\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## The bitter lesson",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 104,
          "function_name": "why_this_course_exists",
          "code": "text(\"Wrong interpretation: scale is all that matters, algorithms don't matter.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Wrong interpretation: scale is all that matters, algorithms don't matter.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 105,
          "function_name": "why_this_course_exists",
          "code": "text(\"Right interpretation: algorithms that scale is what matters.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Right interpretation: algorithms that scale is what matters.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 106,
          "function_name": "why_this_course_exists",
          "code": "text(\"### accuracy = efficiency x resources\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### accuracy = efficiency x resources",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 107,
          "function_name": "why_this_course_exists",
          "code": "text(\"In fact, efficiency is way more important at larger scale (can't afford to be wasteful).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In fact, efficiency is way more important at larger scale (can't afford to be wasteful).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 108,
          "function_name": "why_this_course_exists",
          "code": "link(\"https://arxiv.org/abs/2005.04305\"), text(\" showed 44x algorithmic efficiency on ImageNet between 2012 and 2019\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Measuring the Algorithmic Efficiency of Neural Networks",
            "authors": [
              "Danny Hernandez",
              "Tom B. Brown"
            ],
            "organization": null,
            "date": "2020-05-08T22:26:37Z",
            "url": "https://arxiv.org/abs/2005.04305",
            "description": "Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "markdown",
          "data": " showed 44x algorithmic efficiency on ImageNet between 2012 and 2019",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 110,
          "function_name": "why_this_course_exists",
          "code": "text(\"Framing: what is the best model one can build given a certain compute and data budget?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Framing: what is the best model one can build given a certain compute and data budget?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 111,
          "function_name": "why_this_course_exists",
          "code": "text(\"In other words, **maximize efficiency**!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In other words, **maximize efficiency**!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 25,
          "function_name": "main",
          "code": "why_this_course_exists()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 114,
          "function_name": "current_landscape",
          "code": "def current_landscape():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 115,
          "function_name": "current_landscape",
          "code": "text(\"## Pre-neural (before 2010s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Pre-neural (before 2010s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 116,
          "function_name": "current_landscape",
          "code": "text(\"- Language model to measure the entropy of English \"), link(shannon1950)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Language model to measure the entropy of English ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Prediction and Entropy of Printed English",
            "authors": [
              "Claude Shannon"
            ],
            "organization": null,
            "date": "1950-09-15",
            "url": "https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 117,
          "function_name": "current_landscape",
          "code": "text(\"- Lots of work on n-gram language models (for machine translation, speech recognition) \"), link(brants2007)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Lots of work on n-gram language models (for machine translation, speech recognition) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Language Models in Machine Translation",
            "authors": [
              "Thorsten Brants",
              "Ashok C. Popat",
              "Peng Xu",
              "Franz J. Och",
              "Jeffrey Dean"
            ],
            "organization": "Google",
            "date": "2007",
            "url": "https://aclanthology.org/D07-1090.pdf",
            "description": null,
            "notes": "Trained 5-gram model on 2T tokens"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 119,
          "function_name": "current_landscape",
          "code": "text(\"## Neural ingredients (2010s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Neural ingredients (2010s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 120,
          "function_name": "current_landscape",
          "code": "text(\"- First neural language model \"), link(bengio2003)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- First neural language model ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "A Neural Probabilistic Language Model",
            "authors": [
              "Yoshua Bengio",
              "Rejean Ducharme",
              "Pascal Vincent",
              "Christian Jauvin"
            ],
            "organization": null,
            "date": "2003-02-01",
            "url": "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",
            "description": null,
            "notes": "Used a feedforward neural network over last n words to predict the next word in a sequence"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 121,
          "function_name": "current_landscape",
          "code": "text(\"- Sequence-to-sequence modeling (for machine translation) \"), link(susketver2014)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Sequence-to-sequence modeling (for machine translation) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "authors": [
              "Ilya Sutskever",
              "Oriol Vinyals",
              "Quoc V. Le"
            ],
            "organization": "Google",
            "date": "2014-09-10T19:55:35Z",
            "url": "https://arxiv.org/pdf/1409.3215.pdf",
            "description": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
            "notes": "Introduced seq2seq (encode entire sentence into one vector, decode translation)"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 122,
          "function_name": "current_landscape",
          "code": "text(\"- Adam optimizer \"), link(adam2014)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Adam optimizer ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Adam: A Method for Stochastic Optimization",
            "authors": [
              "Diederik P. Kingma",
              "Jimmy Ba"
            ],
            "organization": null,
            "date": "2014-12-22T13:54:29Z",
            "url": "https://arxiv.org/pdf/1412.6980.pdf",
            "description": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "notes": "Introduced Adam optimizer based on RMSProp and momentum"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 123,
          "function_name": "current_landscape",
          "code": "text(\"- Attention mechanism (for machine translation) \"), link(bahdanau2015_attention)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Attention mechanism (for machine translation) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "authors": [
              "Dzmitry Bahdanau",
              "Kyunghyun Cho",
              "Yoshua Bengio"
            ],
            "organization": null,
            "date": "2014-09-01T16:33:02Z",
            "url": "https://arxiv.org/pdf/1409.0473.pdf",
            "description": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
            "notes": "Introduced attention mechanism (for machine translation)"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 124,
          "function_name": "current_landscape",
          "code": "text(\"- Transformer architecture (for machine translation) \"), link(transformer_2017)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Transformer architecture (for machine translation) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Attention Is All You Need",
            "authors": [
              "Ashish Vaswani",
              "Noam Shazeer",
              "Niki Parmar",
              "Jakob Uszkoreit",
              "Llion Jones",
              "Aidan N. Gomez",
              "Lukasz Kaiser",
              "Illia Polosukhin"
            ],
            "organization": "Google",
            "date": "2017-06-12T17:57:34Z",
            "url": "https://arxiv.org/pdf/1706.03762.pdf",
            "description": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "notes": "Introduced Transformer (for machine translation)"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 125,
          "function_name": "current_landscape",
          "code": "text(\"- Mixture of experts \"), link(moe_2017)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Mixture of experts ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
            "authors": [
              "Noam Shazeer",
              "Azalia Mirhoseini",
              "Krzysztof Maziarz",
              "Andy Davis",
              "Quoc Le",
              "Geoffrey Hinton",
              "Jeff Dean"
            ],
            "organization": "Google",
            "date": "2017-01-23T18:10:00Z",
            "url": "https://arxiv.org/pdf/1701.06538.pdf",
            "description": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 126,
          "function_name": "current_landscape",
          "code": "text(\"- Model parallelism \"), link(gpipe_2018), link(zero_2019), link(megatron_lm_2019)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model parallelism ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
            "authors": [
              "Yanping Huang",
              "Youlong Cheng",
              "Ankur Bapna",
              "Orhan Firat",
              "Mia Xu Chen",
              "Dehao Chen",
              "HyoukJoong Lee",
              "Jiquan Ngiam",
              "Quoc V. Le",
              "Yonghui Wu",
              "Zhifeng Chen"
            ],
            "organization": "Google",
            "date": "2018-11-16T18:43:28Z",
            "url": "https://arxiv.org/pdf/1811.06965.pdf",
            "description": "Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
            "authors": [
              "Samyam Rajbhandari",
              "Jeff Rasley",
              "Olatunji Ruwase",
              "Yuxiong He"
            ],
            "organization": "Microsoft",
            "date": "2019-10-04T17:29:39Z",
            "url": "https://arxiv.org/abs/1910.02054",
            "description": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.",
            "notes": "Introduced ZeRO optimizer, can train 100B parameter model over 400 GPUs"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
            "authors": [
              "Mohammad Shoeybi",
              "Mostofa Patwary",
              "Raul Puri",
              "Patrick LeGresley",
              "Jared Casper",
              "Bryan Catanzaro"
            ],
            "organization": "NVIDIA",
            "date": "2019-09-17T19:42:54Z",
            "url": "https://arxiv.org/pdf/1909.08053.pdf",
            "description": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 128,
          "function_name": "current_landscape",
          "code": "text(\"## Early foundation models (late 2010s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Early foundation models (late 2010s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 129,
          "function_name": "current_landscape",
          "code": "text(\"- ELMo: pretraining with LSTMs, fine-tuning helps tasks \"), link(elmo)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- ELMo: pretraining with LSTMs, fine-tuning helps tasks ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Deep contextualized word representations",
            "authors": [
              "Matthew E. Peters",
              "Mark Neumann",
              "Mohit Iyyer",
              "Matt Gardner",
              "Christopher Clark",
              "Kenton Lee",
              "Luke Zettlemoyer"
            ],
            "organization": null,
            "date": "2018-02-15T00:05:11Z",
            "url": "https://arxiv.org/abs/1802.05365",
            "description": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 130,
          "function_name": "current_landscape",
          "code": "text(\"- BERT: pretraining with Transformer, fine-tuning helps tasks \"), link(bert)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- BERT: pretraining with Transformer, fine-tuning helps tasks ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "authors": [
              "Jacob Devlin",
              "Ming-Wei Chang",
              "Kenton Lee",
              "Kristina Toutanova"
            ],
            "organization": null,
            "date": "2018-10-11T00:50:01Z",
            "url": "https://arxiv.org/abs/1810.04805",
            "description": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 131,
          "function_name": "current_landscape",
          "code": "text(\"- Google's T5 (11B): cast everything as text-to-text \"), link(t5)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Google's T5 (11B): cast everything as text-to-text ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "authors": [
              "Colin Raffel",
              "Noam Shazeer",
              "Adam Roberts",
              "Katherine Lee",
              "Sharan Narang",
              "Michael Matena",
              "Yanqi Zhou",
              "Wei Li",
              "Peter J. Liu"
            ],
            "organization": "Google",
            "date": "2019-10-23T17:37:36Z",
            "url": "https://arxiv.org/pdf/1910.10683.pdf",
            "description": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
            "notes": "Encoder-decoder, frames tasks as text-to-text\nIntroduced Colossal Cleaned Common Crawl (C4)\nFiltering (Section 2.2)\n11B parameters\nRemove bias from feedforward layers"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 133,
          "function_name": "current_landscape",
          "code": "text(\"## Embracing scaling, more closed\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Embracing scaling, more closed",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 134,
          "function_name": "current_landscape",
          "code": "text(\"- OpenAI's GPT-2 (1.5B): fluent text, first signs of zero-shot, staged release \"), link(gpt2)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- OpenAI's GPT-2 (1.5B): fluent text, first signs of zero-shot, staged release ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Language Models are Unsupervised Multitask Learners",
            "authors": [
              "Alec Radford",
              "Jeffrey Wu",
              "Rewon Child",
              "David Luan",
              "Dario Amodei",
              "Ilya Sutskever"
            ],
            "organization": "OpenAI",
            "date": "2019-02-14",
            "url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
            "description": null,
            "notes": "1.5B parameters\nPioneered stage release"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 135,
          "function_name": "current_landscape",
          "code": "text(\"- Scaling laws: provide hope / predictability for scaling \"), link(kaplan_scaling_laws_2020)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Scaling laws: provide hope / predictability for scaling ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Scaling Laws for Neural Language Models",
            "authors": [
              "Jared Kaplan",
              "Sam McCandlish",
              "Tom Henighan",
              "Tom B. Brown",
              "Benjamin Chess",
              "Rewon Child",
              "Scott Gray",
              "Alec Radford",
              "Jeffrey Wu",
              "Dario Amodei"
            ],
            "organization": "OpenAI",
            "date": "2020-01-23T03:59:20Z",
            "url": "https://arxiv.org/pdf/2001.08361.pdf",
            "description": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
            "notes": "Vary model size, dataset size, compute; get power laws\nLarger models require fewer tokens"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 136,
          "function_name": "current_landscape",
          "code": "text(\"- OpenAI's GPT-3 (175B): in-context learning, closed \"), link(gpt_3)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- OpenAI's GPT-3 (175B): in-context learning, closed ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Language Models are Few-Shot Learners",
            "authors": [
              "Tom B. Brown",
              "Benjamin Mann",
              "Nick Ryder",
              "Melanie Subbiah",
              "Jared Kaplan",
              "Prafulla Dhariwal",
              "Arvind Neelakantan",
              "Pranav Shyam",
              "Girish Sastry",
              "Amanda Askell",
              "Sandhini Agarwal",
              "Ariel Herbert-Voss",
              "Gretchen Krueger",
              "Tom Henighan",
              "Rewon Child",
              "Aditya Ramesh",
              "Daniel M. Ziegler",
              "Jeffrey Wu",
              "Clemens Winter",
              "Christopher Hesse",
              "Mark Chen",
              "Eric Sigler",
              "Mateusz Litwin",
              "Scott Gray",
              "Benjamin Chess",
              "Jack Clark",
              "Christopher Berner",
              "Sam McCandlish",
              "Alec Radford",
              "Ilya Sutskever",
              "Dario Amodei"
            ],
            "organization": "OpenAI",
            "date": "2020-05-28T17:29:03Z",
            "url": "https://arxiv.org/pdf/2005.14165.pdf",
            "description": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "notes": "Introduces GPT-3\nSame as GPT-2, but alternating sparse and dense attention layers\n175B parameters\nData: 300B tokens"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 137,
          "function_name": "current_landscape",
          "code": "text(\"- Google's PaLM (540B): massive scale, undertrained \"), link(palm)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Google's PaLM (540B): massive scale, undertrained ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "authors": [
              "Aakanksha Chowdhery",
              "Sharan Narang",
              "Jacob Devlin",
              "Maarten Bosma",
              "Gaurav Mishra",
              "Adam Roberts",
              "Paul Barham",
              "Hyung Won Chung",
              "Charles Sutton",
              "Sebastian Gehrmann",
              "Parker Schuh",
              "Kensen Shi",
              "Sasha Tsvyashchenko",
              "Joshua Maynez",
              "Abhishek Rao",
              "Parker Barnes",
              "Yi Tay",
              "Noam Shazeer",
              "Vinodkumar Prabhakaran",
              "Emily Reif",
              "Nan Du",
              "Ben Hutchinson",
              "Reiner Pope",
              "James Bradbury",
              "Jacob Austin",
              "Michael Isard",
              "Guy Gur-Ari",
              "Pengcheng Yin",
              "Toju Duke",
              "Anselm Levskaya",
              "Sanjay Ghemawat",
              "Sunipa Dev",
              "Henryk Michalewski",
              "Xavier Garcia",
              "Vedant Misra",
              "Kevin Robinson",
              "Liam Fedus",
              "Denny Zhou",
              "Daphne Ippolito",
              "David Luan",
              "Hyeontaek Lim",
              "Barret Zoph",
              "Alexander Spiridonov",
              "Ryan Sepassi",
              "David Dohan",
              "Shivani Agrawal",
              "Mark Omernick",
              "Andrew M. Dai",
              "Thanumalayan Sankaranarayana Pillai",
              "Marie Pellat",
              "Aitor Lewkowycz",
              "Erica Moreira",
              "Rewon Child",
              "Oleksandr Polozov",
              "Katherine Lee",
              "Zongwei Zhou",
              "Xuezhi Wang",
              "Brennan Saeta",
              "Mark Diaz",
              "Orhan Firat",
              "Michele Catasta",
              "Jason Wei",
              "Kathy Meier-Hellstern",
              "Douglas Eck",
              "Jeff Dean",
              "Slav Petrov",
              "Noah Fiedel"
            ],
            "organization": "Google",
            "date": "2022-04-05T16:11:45Z",
            "url": "https://arxiv.org/pdf/2204.02311.pdf",
            "description": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
            "notes": "Data: Social media conversations, webpages, books, GitHub, Wikipedia, news\n540B parameters\nSwiGLU, parallelize attention and feedforward layers, multi-query attention, RoPE, remove biases\nhardware: 6144 TPUv4, 46.2% MFU\noptimizer: Adafactor without factorization\nIntroduced the term model FLOPs utilization (MFU) metric (observed tokens/sec / theoretical max tokens/sec)"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 138,
          "function_name": "current_landscape",
          "code": "text(\"- DeepMind's Chinchilla (70B): compute-optimal scaling laws \"), link(chinchilla)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DeepMind's Chinchilla (70B): compute-optimal scaling laws ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Training Compute-Optimal Large Language Models",
            "authors": [
              "Jordan Hoffmann",
              "Sebastian Borgeaud",
              "Arthur Mensch",
              "Elena Buchatskaya",
              "Trevor Cai",
              "Eliza Rutherford",
              "Diego de Las Casas",
              "Lisa Anne Hendricks",
              "Johannes Welbl",
              "Aidan Clark",
              "Tom Hennigan",
              "Eric Noland",
              "Katie Millican",
              "George van den Driessche",
              "Bogdan Damoc",
              "Aurelia Guy",
              "Simon Osindero",
              "Karen Simonyan",
              "Erich Elsen",
              "Jack W. Rae",
              "Oriol Vinyals",
              "Laurent Sifre"
            ],
            "organization": "DeepMind",
            "date": "2022-03-29T13:38:03Z",
            "url": "https://arxiv.org/pdf/2203.15556.pdf",
            "description": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
            "notes": "Introduced the rigorous analysis scaling laws for language models\nKey improvement over Kaplan: tune learning rate for the compute budget\nApproach 1: for each model size, train with 4 learning rates, vary number of training tokens, fit lower envelope\nApproach 2 (IsoFLOP): for each model size, train with 9 training budgets, take last point\nApproach 3: fit parametric function L(N, D) = E + A/N^alpha + B/D^beta to data collected from approaches 1 and 2\nConclusion: model and data should scale up at same rate\nTable 3: extrapolate to 10 trillion parameters\nMassiveText, different data distribution (1.5 trillion tokens)\n70B parameters"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 140,
          "function_name": "current_landscape",
          "code": "text(\"## Open models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Open models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 141,
          "function_name": "current_landscape",
          "code": "text(\"- EleutherAI's open datasets (The Pile) and models (GPT-J) \"), link(the_pile), link(gpt_j)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- EleutherAI's open datasets (The Pile) and models (GPT-J) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
            "authors": [
              "Leo Gao",
              "Stella Biderman",
              "Sid Black",
              "Laurence Golding",
              "Travis Hoppe",
              "Charles Foster",
              "Jason Phang",
              "Horace He",
              "Anish Thite",
              "Noa Nabeshima",
              "Shawn Presser",
              "Connor Leahy"
            ],
            "organization": "EleutherAI",
            "date": "2020-12-31T19:00:10Z",
            "url": "https://arxiv.org/pdf/2101.00027.pdf",
            "description": "Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.",
            "notes": "825GB text, 22 diverse subsets (CommonCrawl, PubMed, ArXiv, GitHub, StackExchange, USPTO, OpenWebText2, Books3, etc.)"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GPT-J",
            "authors": [
              "Ben Wang",
              "Aran Komatsuzaki"
            ],
            "organization": "EleutherAI",
            "date": "2021-06-04",
            "url": "https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/",
            "description": null,
            "notes": "6.7B parameters\nAttention and feedforward layers put in parallel\nv3 256 TPUs (5.4 PFLOPs) for 5 weeks"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 142,
          "function_name": "current_landscape",
          "code": "text(\"- Meta's OPT (175B): GPT-3 replication, lots of hardware issues \"), link(opt_175b)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Meta's OPT (175B): GPT-3 replication, lots of hardware issues ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "OPT: Open Pre-trained Transformer Language Models",
            "authors": [
              "Susan Zhang",
              "Stephen Roller",
              "Naman Goyal",
              "Mikel Artetxe",
              "Moya Chen",
              "Shuohui Chen",
              "Christopher Dewan",
              "Mona Diab",
              "Xian Li",
              "Xi Victoria Lin",
              "Todor Mihaylov",
              "Myle Ott",
              "Sam Shleifer",
              "Kurt Shuster",
              "Daniel Simig",
              "Punit Singh Koura",
              "Anjali Sridhar",
              "Tianlu Wang",
              "Luke Zettlemoyer"
            ],
            "organization": "Meta",
            "date": "2022-05-02T17:49:50Z",
            "url": "https://arxiv.org/pdf/2205.01068.pdf",
            "description": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
            "notes": "Data: The Pile, PushShift.io Reddit, deduplication\n175B parameters\nhardware: 992 A100 80GB for 2 months, lots of hardware failures\nFSDP with Megatron-LM, fp16 with loss scaling"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 143,
          "function_name": "current_landscape",
          "code": "text(\"- Hugging Face / BigScience's BLOOM: focused on data sourcing \"), link(bloom)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Hugging Face / BigScience's BLOOM: focused on data sourcing ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
            "authors": [
              "BigScience Workshop",
              ":",
              "Teven Le Scao",
              "Angela Fan",
              "Christopher Akiki",
              "Ellie Pavlick",
              "Suzana Ili\u0107",
              "Daniel Hesslow",
              "Roman Castagn\u00e9",
              "Alexandra Sasha Luccioni",
              "Fran\u00e7ois Yvon",
              "Matthias Gall\u00e9",
              "Jonathan Tow",
              "Alexander M. Rush",
              "Stella Biderman",
              "Albert Webson",
              "Pawan Sasanka Ammanamanchi",
              "Thomas Wang",
              "Beno\u00eet Sagot",
              "Niklas Muennighoff",
              "Albert Villanova del Moral",
              "Olatunji Ruwase",
              "Rachel Bawden",
              "Stas Bekman",
              "Angelina McMillan-Major",
              "Iz Beltagy",
              "Huu Nguyen",
              "Lucile Saulnier",
              "Samson Tan",
              "Pedro Ortiz Suarez",
              "Victor Sanh",
              "Hugo Lauren\u00e7on",
              "Yacine Jernite",
              "Julien Launay",
              "Margaret Mitchell",
              "Colin Raffel",
              "Aaron Gokaslan",
              "Adi Simhi",
              "Aitor Soroa",
              "Alham Fikri Aji",
              "Amit Alfassy",
              "Anna Rogers",
              "Ariel Kreisberg Nitzav",
              "Canwen Xu",
              "Chenghao Mou",
              "Chris Emezue",
              "Christopher Klamm",
              "Colin Leong",
              "Daniel van Strien",
              "David Ifeoluwa Adelani",
              "Dragomir Radev",
              "Eduardo Gonz\u00e1lez Ponferrada",
              "Efrat Levkovizh",
              "Ethan Kim",
              "Eyal Bar Natan",
              "Francesco De Toni",
              "G\u00e9rard Dupont",
              "Germ\u00e1n Kruszewski",
              "Giada Pistilli",
              "Hady Elsahar",
              "Hamza Benyamina",
              "Hieu Tran",
              "Ian Yu",
              "Idris Abdulmumin",
              "Isaac Johnson",
              "Itziar Gonzalez-Dios",
              "Javier de la Rosa",
              "Jenny Chim",
              "Jesse Dodge",
              "Jian Zhu",
              "Jonathan Chang",
              "J\u00f6rg Frohberg",
              "Joseph Tobing",
              "Joydeep Bhattacharjee",
              "Khalid Almubarak",
              "Kimbo Chen",
              "Kyle Lo",
              "Leandro Von Werra",
              "Leon Weber",
              "Long Phan",
              "Loubna Ben allal",
              "Ludovic Tanguy",
              "Manan Dey",
              "Manuel Romero Mu\u00f1oz",
              "Maraim Masoud",
              "Mar\u00eda Grandury",
              "Mario \u0160a\u0161ko",
              "Max Huang",
              "Maximin Coavoux",
              "Mayank Singh",
              "Mike Tian-Jian Jiang",
              "Minh Chien Vu",
              "Mohammad A. Jauhar",
              "Mustafa Ghaleb",
              "Nishant Subramani",
              "Nora Kassner",
              "Nurulaqilla Khamis",
              "Olivier Nguyen",
              "Omar Espejel",
              "Ona de Gibert",
              "Paulo Villegas",
              "Peter Henderson",
              "Pierre Colombo",
              "Priscilla Amuok",
              "Quentin Lhoest",
              "Rheza Harliman",
              "Rishi Bommasani",
              "Roberto Luis L\u00f3pez",
              "Rui Ribeiro",
              "Salomey Osei",
              "Sampo Pyysalo",
              "Sebastian Nagel",
              "Shamik Bose",
              "Shamsuddeen Hassan Muhammad",
              "Shanya Sharma",
              "Shayne Longpre",
              "Somaieh Nikpoor",
              "Stanislav Silberberg",
              "Suhas Pai",
              "Sydney Zink",
              "Tiago Timponi Torrent",
              "Timo Schick",
              "Tristan Thrush",
              "Valentin Danchev",
              "Vassilina Nikoulina",
              "Veronika Laippala",
              "Violette Lepercq",
              "Vrinda Prabhu",
              "Zaid Alyafeai",
              "Zeerak Talat",
              "Arun Raja",
              "Benjamin Heinzerling",
              "Chenglei Si",
              "Davut Emre Ta\u015far",
              "Elizabeth Salesky",
              "Sabrina J. Mielke",
              "Wilson Y. Lee",
              "Abheesht Sharma",
              "Andrea Santilli",
              "Antoine Chaffin",
              "Arnaud Stiegler",
              "Debajyoti Datta",
              "Eliza Szczechla",
              "Gunjan Chhablani",
              "Han Wang",
              "Harshit Pandey",
              "Hendrik Strobelt",
              "Jason Alan Fries",
              "Jos Rozen",
              "Leo Gao",
              "Lintang Sutawika",
              "M Saiful Bari",
              "Maged S. Al-shaibani",
              "Matteo Manica",
              "Nihal Nayak",
              "Ryan Teehan",
              "Samuel Albanie",
              "Sheng Shen",
              "Srulik Ben-David",
              "Stephen H. Bach",
              "Taewoon Kim",
              "Tali Bers",
              "Thibault Fevry",
              "Trishala Neeraj",
              "Urmish Thakker",
              "Vikas Raunak",
              "Xiangru Tang",
              "Zheng-Xin Yong",
              "Zhiqing Sun",
              "Shaked Brody",
              "Yallow Uri",
              "Hadar Tojarieh",
              "Adam Roberts",
              "Hyung Won Chung",
              "Jaesung Tae",
              "Jason Phang",
              "Ofir Press",
              "Conglong Li",
              "Deepak Narayanan",
              "Hatim Bourfoune",
              "Jared Casper",
              "Jeff Rasley",
              "Max Ryabinin",
              "Mayank Mishra",
              "Minjia Zhang",
              "Mohammad Shoeybi",
              "Myriam Peyrounette",
              "Nicolas Patry",
              "Nouamane Tazi",
              "Omar Sanseviero",
              "Patrick von Platen",
              "Pierre Cornette",
              "Pierre Fran\u00e7ois Lavall\u00e9e",
              "R\u00e9mi Lacroix",
              "Samyam Rajbhandari",
              "Sanchit Gandhi",
              "Shaden Smith",
              "St\u00e9phane Requena",
              "Suraj Patil",
              "Tim Dettmers",
              "Ahmed Baruwa",
              "Amanpreet Singh",
              "Anastasia Cheveleva",
              "Anne-Laure Ligozat",
              "Arjun Subramonian",
              "Aur\u00e9lie N\u00e9v\u00e9ol",
              "Charles Lovering",
              "Dan Garrette",
              "Deepak Tunuguntla",
              "Ehud Reiter",
              "Ekaterina Taktasheva",
              "Ekaterina Voloshina",
              "Eli Bogdanov",
              "Genta Indra Winata",
              "Hailey Schoelkopf",
              "Jan-Christoph Kalo",
              "Jekaterina Novikova",
              "Jessica Zosa Forde",
              "Jordan Clive",
              "Jungo Kasai",
              "Ken Kawamura",
              "Liam Hazan",
              "Marine Carpuat",
              "Miruna Clinciu",
              "Najoung Kim",
              "Newton Cheng",
              "Oleg Serikov",
              "Omer Antverg",
              "Oskar van der Wal",
              "Rui Zhang",
              "Ruochen Zhang",
              "Sebastian Gehrmann",
              "Shachar Mirkin",
              "Shani Pais",
              "Tatiana Shavrina",
              "Thomas Scialom",
              "Tian Yun",
              "Tomasz Limisiewicz",
              "Verena Rieser",
              "Vitaly Protasov",
              "Vladislav Mikhailov",
              "Yada Pruksachatkun",
              "Yonatan Belinkov",
              "Zachary Bamberger",
              "Zden\u011bk Kasner",
              "Alice Rueda",
              "Amanda Pestana",
              "Amir Feizpour",
              "Ammar Khan",
              "Amy Faranak",
              "Ana Santos",
              "Anthony Hevia",
              "Antigona Unldreaj",
              "Arash Aghagol",
              "Arezoo Abdollahi",
              "Aycha Tammour",
              "Azadeh HajiHosseini",
              "Bahareh Behroozi",
              "Benjamin Ajibade",
              "Bharat Saxena",
              "Carlos Mu\u00f1oz Ferrandis",
              "Daniel McDuff",
              "Danish Contractor",
              "David Lansky",
              "Davis David",
              "Douwe Kiela",
              "Duong A. Nguyen",
              "Edward Tan",
              "Emi Baylor",
              "Ezinwanne Ozoani",
              "Fatima Mirza",
              "Frankline Ononiwu",
              "Habib Rezanejad",
              "Hessie Jones",
              "Indrani Bhattacharya",
              "Irene Solaiman",
              "Irina Sedenko",
              "Isar Nejadgholi",
              "Jesse Passmore",
              "Josh Seltzer",
              "Julio Bonis Sanz",
              "Livia Dutra",
              "Mairon Samagaio",
              "Maraim Elbadri",
              "Margot Mieskes",
              "Marissa Gerchick",
              "Martha Akinlolu",
              "Michael McKenna",
              "Mike Qiu",
              "Muhammed Ghauri",
              "Mykola Burynok",
              "Nafis Abrar",
              "Nazneen Rajani",
              "Nour Elkott",
              "Nour Fahmy",
              "Olanrewaju Samuel",
              "Ran An",
              "Rasmus Kromann",
              "Ryan Hao",
              "Samira Alizadeh",
              "Sarmad Shubber",
              "Silas Wang",
              "Sourav Roy",
              "Sylvain Viguier",
              "Thanh Le",
              "Tobi Oyebade",
              "Trieu Le",
              "Yoyo Yang",
              "Zach Nguyen",
              "Abhinav Ramesh Kashyap",
              "Alfredo Palasciano",
              "Alison Callahan",
              "Anima Shukla",
              "Antonio Miranda-Escalada",
              "Ayush Singh",
              "Benjamin Beilharz",
              "Bo Wang",
              "Caio Brito",
              "Chenxi Zhou",
              "Chirag Jain",
              "Chuxin Xu",
              "Cl\u00e9mentine Fourrier",
              "Daniel Le\u00f3n Peri\u00f1\u00e1n",
              "Daniel Molano",
              "Dian Yu",
              "Enrique Manjavacas",
              "Fabio Barth",
              "Florian Fuhrimann",
              "Gabriel Altay",
              "Giyaseddin Bayrak",
              "Gully Burns",
              "Helena U. Vrabec",
              "Imane Bello",
              "Ishani Dash",
              "Jihyun Kang",
              "John Giorgi",
              "Jonas Golde",
              "Jose David Posada",
              "Karthik Rangasai Sivaraman",
              "Lokesh Bulchandani",
              "Lu Liu",
              "Luisa Shinzato",
              "Madeleine Hahn de Bykhovetz",
              "Maiko Takeuchi",
              "Marc P\u00e0mies",
              "Maria A Castillo",
              "Marianna Nezhurina",
              "Mario S\u00e4nger",
              "Matthias Samwald",
              "Michael Cullan",
              "Michael Weinberg",
              "Michiel De Wolf",
              "Mina Mihaljcic",
              "Minna Liu",
              "Moritz Freidank",
              "Myungsun Kang",
              "Natasha Seelam",
              "Nathan Dahlberg",
              "Nicholas Michio Broad",
              "Nikolaus Muellner",
              "Pascale Fung",
              "Patrick Haller",
              "Ramya Chandrasekhar",
              "Renata Eisenberg",
              "Robert Martin",
              "Rodrigo Canalli",
              "Rosaline Su",
              "Ruisi Su",
              "Samuel Cahyawijaya",
              "Samuele Garda",
              "Shlok S Deshmukh",
              "Shubhanshu Mishra",
              "Sid Kiblawi",
              "Simon Ott",
              "Sinee Sang-aroonsiri",
              "Srishti Kumar",
              "Stefan Schweter",
              "Sushil Bharati",
              "Tanmay Laud",
              "Th\u00e9o Gigant",
              "Tomoya Kainuma",
              "Wojciech Kusa",
              "Yanis Labrak",
              "Yash Shailesh Bajaj",
              "Yash Venkatraman",
              "Yifan Xu",
              "Yingxin Xu",
              "Yu Xu",
              "Zhe Tan",
              "Zhongli Xie",
              "Zifan Ye",
              "Mathilde Bras",
              "Younes Belkada",
              "Thomas Wolf"
            ],
            "organization": "BigScience",
            "date": "2022-11-09T18:48:09Z",
            "url": "https://arxiv.org/abs/2211.05100",
            "description": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",
            "notes": "Model: BLOOM (176B parameters)\nData: ROOTS\nHardware: 48x8 A100s on Jean Zay supercomputer for 3.5 months\nZeRO stage 1"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 144,
          "function_name": "current_landscape",
          "code": "text(\"- Meta's Llama models \"), link(llama), link(llama2), link(llama3)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Meta's Llama models ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "LLaMA: Open and Efficient Foundation Language Models",
            "authors": [
              "Hugo Touvron",
              "Thibaut Lavril",
              "Gautier Izacard",
              "Xavier Martinet",
              "Marie-Anne Lachaux",
              "Timoth\u00e9e Lacroix",
              "Baptiste Rozi\u00e8re",
              "Naman Goyal",
              "Eric Hambro",
              "Faisal Azhar",
              "Aurelien Rodriguez",
              "Armand Joulin",
              "Edouard Grave",
              "Guillaume Lample"
            ],
            "organization": "Meta",
            "date": "2023-02-27T17:11:15Z",
            "url": "https://arxiv.org/pdf/2302.13971.pdf",
            "description": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
            "notes": "Train only on open data (detailed recipe that is replicated by RedPajama)\nOptimize for fast inference at 7B\nData: CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, StackExchange\nArchitecture: Pre-norm, SwiGLU, RoPE\nTraining: 2048 A100 80GB for 21 days"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "authors": [
              "Hugo Touvron",
              "Louis Martin",
              "Kevin Stone",
              "Peter Albert",
              "Amjad Almahairi",
              "Yasmine Babaei",
              "Nikolay Bashlykov",
              "Soumya Batra",
              "Prajjwal Bhargava",
              "Shruti Bhosale",
              "Dan Bikel",
              "Lukas Blecher",
              "Cristian Canton Ferrer",
              "Moya Chen",
              "Guillem Cucurull",
              "David Esiobu",
              "Jude Fernandes",
              "Jeremy Fu",
              "Wenyin Fu",
              "Brian Fuller",
              "Cynthia Gao",
              "Vedanuj Goswami",
              "Naman Goyal",
              "Anthony Hartshorn",
              "Saghar Hosseini",
              "Rui Hou",
              "Hakan Inan",
              "Marcin Kardas",
              "Viktor Kerkez",
              "Madian Khabsa",
              "Isabel Kloumann",
              "Artem Korenev",
              "Punit Singh Koura",
              "Marie-Anne Lachaux",
              "Thibaut Lavril",
              "Jenya Lee",
              "Diana Liskovich",
              "Yinghai Lu",
              "Yuning Mao",
              "Xavier Martinet",
              "Todor Mihaylov",
              "Pushkar Mishra",
              "Igor Molybog",
              "Yixin Nie",
              "Andrew Poulton",
              "Jeremy Reizenstein",
              "Rashi Rungta",
              "Kalyan Saladi",
              "Alan Schelten",
              "Ruan Silva",
              "Eric Michael Smith",
              "Ranjan Subramanian",
              "Xiaoqing Ellen Tan",
              "Binh Tang",
              "Ross Taylor",
              "Adina Williams",
              "Jian Xiang Kuan",
              "Puxin Xu",
              "Zheng Yan",
              "Iliyan Zarov",
              "Yuchen Zhang",
              "Angela Fan",
              "Melanie Kambadur",
              "Sharan Narang",
              "Aurelien Rodriguez",
              "Robert Stojnic",
              "Sergey Edunov",
              "Thomas Scialom"
            ],
            "organization": "Meta",
            "date": "2023-07-18T14:31:57Z",
            "url": "https://arxiv.org/pdf/2307.09288.pdf",
            "description": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
            "notes": "2T tokens70B parameters"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Llama 3 Herd of Models",
            "authors": [
              "Aaron Grattafiori",
              "Abhimanyu Dubey",
              "Abhinav Jauhri",
              "Abhinav Pandey",
              "Abhishek Kadian",
              "Ahmad Al-Dahle",
              "Aiesha Letman",
              "Akhil Mathur",
              "Alan Schelten",
              "Alex Vaughan",
              "Amy Yang",
              "Angela Fan",
              "Anirudh Goyal",
              "Anthony Hartshorn",
              "Aobo Yang",
              "Archi Mitra",
              "Archie Sravankumar",
              "Artem Korenev",
              "Arthur Hinsvark",
              "Arun Rao",
              "Aston Zhang",
              "Aurelien Rodriguez",
              "Austen Gregerson",
              "Ava Spataru",
              "Baptiste Roziere",
              "Bethany Biron",
              "Binh Tang",
              "Bobbie Chern",
              "Charlotte Caucheteux",
              "Chaya Nayak",
              "Chloe Bi",
              "Chris Marra",
              "Chris McConnell",
              "Christian Keller",
              "Christophe Touret",
              "Chunyang Wu",
              "Corinne Wong",
              "Cristian Canton Ferrer",
              "Cyrus Nikolaidis",
              "Damien Allonsius",
              "Daniel Song",
              "Danielle Pintz",
              "Danny Livshits",
              "Danny Wyatt",
              "David Esiobu",
              "Dhruv Choudhary",
              "Dhruv Mahajan",
              "Diego Garcia-Olano",
              "Diego Perino",
              "Dieuwke Hupkes",
              "Egor Lakomkin",
              "Ehab AlBadawy",
              "Elina Lobanova",
              "Emily Dinan",
              "Eric Michael Smith",
              "Filip Radenovic",
              "Francisco Guzm\u00e1n",
              "Frank Zhang",
              "Gabriel Synnaeve",
              "Gabrielle Lee",
              "Georgia Lewis Anderson",
              "Govind Thattai",
              "Graeme Nail",
              "Gregoire Mialon",
              "Guan Pang",
              "Guillem Cucurell",
              "Hailey Nguyen",
              "Hannah Korevaar",
              "Hu Xu",
              "Hugo Touvron",
              "Iliyan Zarov",
              "Imanol Arrieta Ibarra",
              "Isabel Kloumann",
              "Ishan Misra",
              "Ivan Evtimov",
              "Jack Zhang",
              "Jade Copet",
              "Jaewon Lee",
              "Jan Geffert",
              "Jana Vranes",
              "Jason Park",
              "Jay Mahadeokar",
              "Jeet Shah",
              "Jelmer van der Linde",
              "Jennifer Billock",
              "Jenny Hong",
              "Jenya Lee",
              "Jeremy Fu",
              "Jianfeng Chi",
              "Jianyu Huang",
              "Jiawen Liu",
              "Jie Wang",
              "Jiecao Yu",
              "Joanna Bitton",
              "Joe Spisak",
              "Jongsoo Park",
              "Joseph Rocca",
              "Joshua Johnstun",
              "Joshua Saxe",
              "Junteng Jia",
              "Kalyan Vasuden Alwala",
              "Karthik Prasad",
              "Kartikeya Upasani",
              "Kate Plawiak",
              "Ke Li",
              "Kenneth Heafield",
              "Kevin Stone",
              "Khalid El-Arini",
              "Krithika Iyer",
              "Kshitiz Malik",
              "Kuenley Chiu",
              "Kunal Bhalla",
              "Kushal Lakhotia",
              "Lauren Rantala-Yeary",
              "Laurens van der Maaten",
              "Lawrence Chen",
              "Liang Tan",
              "Liz Jenkins",
              "Louis Martin",
              "Lovish Madaan",
              "Lubo Malo",
              "Lukas Blecher",
              "Lukas Landzaat",
              "Luke de Oliveira",
              "Madeline Muzzi",
              "Mahesh Pasupuleti",
              "Mannat Singh",
              "Manohar Paluri",
              "Marcin Kardas",
              "Maria Tsimpoukelli",
              "Mathew Oldham",
              "Mathieu Rita",
              "Maya Pavlova",
              "Melanie Kambadur",
              "Mike Lewis",
              "Min Si",
              "Mitesh Kumar Singh",
              "Mona Hassan",
              "Naman Goyal",
              "Narjes Torabi",
              "Nikolay Bashlykov",
              "Nikolay Bogoychev",
              "Niladri Chatterji",
              "Ning Zhang",
              "Olivier Duchenne",
              "Onur \u00c7elebi",
              "Patrick Alrassy",
              "Pengchuan Zhang",
              "Pengwei Li",
              "Petar Vasic",
              "Peter Weng",
              "Prajjwal Bhargava",
              "Pratik Dubal",
              "Praveen Krishnan",
              "Punit Singh Koura",
              "Puxin Xu",
              "Qing He",
              "Qingxiao Dong",
              "Ragavan Srinivasan",
              "Raj Ganapathy",
              "Ramon Calderer",
              "Ricardo Silveira Cabral",
              "Robert Stojnic",
              "Roberta Raileanu",
              "Rohan Maheswari",
              "Rohit Girdhar",
              "Rohit Patel",
              "Romain Sauvestre",
              "Ronnie Polidoro",
              "Roshan Sumbaly",
              "Ross Taylor",
              "Ruan Silva",
              "Rui Hou",
              "Rui Wang",
              "Saghar Hosseini",
              "Sahana Chennabasappa",
              "Sanjay Singh",
              "Sean Bell",
              "Seohyun Sonia Kim",
              "Sergey Edunov",
              "Shaoliang Nie",
              "Sharan Narang",
              "Sharath Raparthy",
              "Sheng Shen",
              "Shengye Wan",
              "Shruti Bhosale",
              "Shun Zhang",
              "Simon Vandenhende",
              "Soumya Batra",
              "Spencer Whitman",
              "Sten Sootla",
              "Stephane Collot",
              "Suchin Gururangan",
              "Sydney Borodinsky",
              "Tamar Herman",
              "Tara Fowler",
              "Tarek Sheasha",
              "Thomas Georgiou",
              "Thomas Scialom",
              "Tobias Speckbacher",
              "Todor Mihaylov",
              "Tong Xiao",
              "Ujjwal Karn",
              "Vedanuj Goswami",
              "Vibhor Gupta",
              "Vignesh Ramanathan",
              "Viktor Kerkez",
              "Vincent Gonguet",
              "Virginie Do",
              "Vish Vogeti",
              "V\u00edtor Albiero",
              "Vladan Petrovic",
              "Weiwei Chu",
              "Wenhan Xiong",
              "Wenyin Fu",
              "Whitney Meers",
              "Xavier Martinet",
              "Xiaodong Wang",
              "Xiaofang Wang",
              "Xiaoqing Ellen Tan",
              "Xide Xia",
              "Xinfeng Xie",
              "Xuchao Jia",
              "Xuewei Wang",
              "Yaelle Goldschlag",
              "Yashesh Gaur",
              "Yasmine Babaei",
              "Yi Wen",
              "Yiwen Song",
              "Yuchen Zhang",
              "Yue Li",
              "Yuning Mao",
              "Zacharie Delpierre Coudert",
              "Zheng Yan",
              "Zhengxing Chen",
              "Zoe Papakipos",
              "Aaditya Singh",
              "Aayushi Srivastava",
              "Abha Jain",
              "Adam Kelsey",
              "Adam Shajnfeld",
              "Adithya Gangidi",
              "Adolfo Victoria",
              "Ahuva Goldstand",
              "Ajay Menon",
              "Ajay Sharma",
              "Alex Boesenberg",
              "Alexei Baevski",
              "Allie Feinstein",
              "Amanda Kallet",
              "Amit Sangani",
              "Amos Teo",
              "Anam Yunus",
              "Andrei Lupu",
              "Andres Alvarado",
              "Andrew Caples",
              "Andrew Gu",
              "Andrew Ho",
              "Andrew Poulton",
              "Andrew Ryan",
              "Ankit Ramchandani",
              "Annie Dong",
              "Annie Franco",
              "Anuj Goyal",
              "Aparajita Saraf",
              "Arkabandhu Chowdhury",
              "Ashley Gabriel",
              "Ashwin Bharambe",
              "Assaf Eisenman",
              "Azadeh Yazdan",
              "Beau James",
              "Ben Maurer",
              "Benjamin Leonhardi",
              "Bernie Huang",
              "Beth Loyd",
              "Beto De Paola",
              "Bhargavi Paranjape",
              "Bing Liu",
              "Bo Wu",
              "Boyu Ni",
              "Braden Hancock",
              "Bram Wasti",
              "Brandon Spence",
              "Brani Stojkovic",
              "Brian Gamido",
              "Britt Montalvo",
              "Carl Parker",
              "Carly Burton",
              "Catalina Mejia",
              "Ce Liu",
              "Changhan Wang",
              "Changkyu Kim",
              "Chao Zhou",
              "Chester Hu",
              "Ching-Hsiang Chu",
              "Chris Cai",
              "Chris Tindal",
              "Christoph Feichtenhofer",
              "Cynthia Gao",
              "Damon Civin",
              "Dana Beaty",
              "Daniel Kreymer",
              "Daniel Li",
              "David Adkins",
              "David Xu",
              "Davide Testuggine",
              "Delia David",
              "Devi Parikh",
              "Diana Liskovich",
              "Didem Foss",
              "Dingkang Wang",
              "Duc Le",
              "Dustin Holland",
              "Edward Dowling",
              "Eissa Jamil",
              "Elaine Montgomery",
              "Eleonora Presani",
              "Emily Hahn",
              "Emily Wood",
              "Eric-Tuan Le",
              "Erik Brinkman",
              "Esteban Arcaute",
              "Evan Dunbar",
              "Evan Smothers",
              "Fei Sun",
              "Felix Kreuk",
              "Feng Tian",
              "Filippos Kokkinos",
              "Firat Ozgenel",
              "Francesco Caggioni",
              "Frank Kanayet",
              "Frank Seide",
              "Gabriela Medina Florez",
              "Gabriella Schwarz",
              "Gada Badeer",
              "Georgia Swee",
              "Gil Halpern",
              "Grant Herman",
              "Grigory Sizov",
              "Guangyi",
              "Zhang",
              "Guna Lakshminarayanan",
              "Hakan Inan",
              "Hamid Shojanazeri",
              "Han Zou",
              "Hannah Wang",
              "Hanwen Zha",
              "Haroun Habeeb",
              "Harrison Rudolph",
              "Helen Suk",
              "Henry Aspegren",
              "Hunter Goldman",
              "Hongyuan Zhan",
              "Ibrahim Damlaj",
              "Igor Molybog",
              "Igor Tufanov",
              "Ilias Leontiadis",
              "Irina-Elena Veliche",
              "Itai Gat",
              "Jake Weissman",
              "James Geboski",
              "James Kohli",
              "Janice Lam",
              "Japhet Asher",
              "Jean-Baptiste Gaya",
              "Jeff Marcus",
              "Jeff Tang",
              "Jennifer Chan",
              "Jenny Zhen",
              "Jeremy Reizenstein",
              "Jeremy Teboul",
              "Jessica Zhong",
              "Jian Jin",
              "Jingyi Yang",
              "Joe Cummings",
              "Jon Carvill",
              "Jon Shepard",
              "Jonathan McPhie",
              "Jonathan Torres",
              "Josh Ginsburg",
              "Junjie Wang",
              "Kai Wu",
              "Kam Hou U",
              "Karan Saxena",
              "Kartikay Khandelwal",
              "Katayoun Zand",
              "Kathy Matosich",
              "Kaushik Veeraraghavan",
              "Kelly Michelena",
              "Keqian Li",
              "Kiran Jagadeesh",
              "Kun Huang",
              "Kunal Chawla",
              "Kyle Huang",
              "Lailin Chen",
              "Lakshya Garg",
              "Lavender A",
              "Leandro Silva",
              "Lee Bell",
              "Lei Zhang",
              "Liangpeng Guo",
              "Licheng Yu",
              "Liron Moshkovich",
              "Luca Wehrstedt",
              "Madian Khabsa",
              "Manav Avalani",
              "Manish Bhatt",
              "Martynas Mankus",
              "Matan Hasson",
              "Matthew Lennie",
              "Matthias Reso",
              "Maxim Groshev",
              "Maxim Naumov",
              "Maya Lathi",
              "Meghan Keneally",
              "Miao Liu",
              "Michael L. Seltzer",
              "Michal Valko",
              "Michelle Restrepo",
              "Mihir Patel",
              "Mik Vyatskov",
              "Mikayel Samvelyan",
              "Mike Clark",
              "Mike Macey",
              "Mike Wang",
              "Miquel Jubert Hermoso",
              "Mo Metanat",
              "Mohammad Rastegari",
              "Munish Bansal",
              "Nandhini Santhanam",
              "Natascha Parks",
              "Natasha White",
              "Navyata Bawa",
              "Nayan Singhal",
              "Nick Egebo",
              "Nicolas Usunier",
              "Nikhil Mehta",
              "Nikolay Pavlovich Laptev",
              "Ning Dong",
              "Norman Cheng",
              "Oleg Chernoguz",
              "Olivia Hart",
              "Omkar Salpekar",
              "Ozlem Kalinli",
              "Parkin Kent",
              "Parth Parekh",
              "Paul Saab",
              "Pavan Balaji",
              "Pedro Rittner",
              "Philip Bontrager",
              "Pierre Roux",
              "Piotr Dollar",
              "Polina Zvyagina",
              "Prashant Ratanchandani",
              "Pritish Yuvraj",
              "Qian Liang",
              "Rachad Alao",
              "Rachel Rodriguez",
              "Rafi Ayub",
              "Raghotham Murthy",
              "Raghu Nayani",
              "Rahul Mitra",
              "Rangaprabhu Parthasarathy",
              "Raymond Li",
              "Rebekkah Hogan",
              "Robin Battey",
              "Rocky Wang",
              "Russ Howes",
              "Ruty Rinott",
              "Sachin Mehta",
              "Sachin Siby",
              "Sai Jayesh Bondu",
              "Samyak Datta",
              "Sara Chugh",
              "Sara Hunt",
              "Sargun Dhillon",
              "Sasha Sidorov",
              "Satadru Pan",
              "Saurabh Mahajan",
              "Saurabh Verma",
              "Seiji Yamamoto",
              "Sharadh Ramaswamy",
              "Shaun Lindsay",
              "Shaun Lindsay",
              "Sheng Feng",
              "Shenghao Lin",
              "Shengxin Cindy Zha",
              "Shishir Patil",
              "Shiva Shankar",
              "Shuqiang Zhang",
              "Shuqiang Zhang",
              "Sinong Wang",
              "Sneha Agarwal",
              "Soji Sajuyigbe",
              "Soumith Chintala",
              "Stephanie Max",
              "Stephen Chen",
              "Steve Kehoe",
              "Steve Satterfield",
              "Sudarshan Govindaprasad",
              "Sumit Gupta",
              "Summer Deng",
              "Sungmin Cho",
              "Sunny Virk",
              "Suraj Subramanian",
              "Sy Choudhury",
              "Sydney Goldman",
              "Tal Remez",
              "Tamar Glaser",
              "Tamara Best",
              "Thilo Koehler",
              "Thomas Robinson",
              "Tianhe Li",
              "Tianjun Zhang",
              "Tim Matthews",
              "Timothy Chou",
              "Tzook Shaked",
              "Varun Vontimitta",
              "Victoria Ajayi",
              "Victoria Montanez",
              "Vijai Mohan",
              "Vinay Satish Kumar",
              "Vishal Mangla",
              "Vlad Ionescu",
              "Vlad Poenaru",
              "Vlad Tiberiu Mihailescu",
              "Vladimir Ivanov",
              "Wei Li",
              "Wenchen Wang",
              "Wenwen Jiang",
              "Wes Bouaziz",
              "Will Constable",
              "Xiaocheng Tang",
              "Xiaojian Wu",
              "Xiaolan Wang",
              "Xilun Wu",
              "Xinbo Gao",
              "Yaniv Kleinman",
              "Yanjun Chen",
              "Ye Hu",
              "Ye Jia",
              "Ye Qi",
              "Yenda Li",
              "Yilin Zhang",
              "Ying Zhang",
              "Yossi Adi",
              "Youngjin Nam",
              "Yu",
              "Wang",
              "Yu Zhao",
              "Yuchen Hao",
              "Yundi Qian",
              "Yunlu Li",
              "Yuzi He",
              "Zach Rait",
              "Zachary DeVito",
              "Zef Rosnbrick",
              "Zhaoduo Wen",
              "Zhenyu Yang",
              "Zhiwei Zhao",
              "Zhiyu Ma"
            ],
            "organization": "Meta",
            "date": "2024-07-31T17:54:27Z",
            "url": "https://arxiv.org/abs/2407.21783",
            "description": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
            "notes": "15T tokens\n405B parameters"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 145,
          "function_name": "current_landscape",
          "code": "text(\"- Alibaba\\'s Qwen models \"), link(qwen_2_5)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Alibaba's Qwen models ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Qwen2.5 Technical Report",
            "authors": [
              "Qwen",
              ":",
              "An Yang",
              "Baosong Yang",
              "Beichen Zhang",
              "Binyuan Hui",
              "Bo Zheng",
              "Bowen Yu",
              "Chengyuan Li",
              "Dayiheng Liu",
              "Fei Huang",
              "Haoran Wei",
              "Huan Lin",
              "Jian Yang",
              "Jianhong Tu",
              "Jianwei Zhang",
              "Jianxin Yang",
              "Jiaxi Yang",
              "Jingren Zhou",
              "Junyang Lin",
              "Kai Dang",
              "Keming Lu",
              "Keqin Bao",
              "Kexin Yang",
              "Le Yu",
              "Mei Li",
              "Mingfeng Xue",
              "Pei Zhang",
              "Qin Zhu",
              "Rui Men",
              "Runji Lin",
              "Tianhao Li",
              "Tianyi Tang",
              "Tingyu Xia",
              "Xingzhang Ren",
              "Xuancheng Ren",
              "Yang Fan",
              "Yang Su",
              "Yichang Zhang",
              "Yu Wan",
              "Yuqiong Liu",
              "Zeyu Cui",
              "Zhenru Zhang",
              "Zihan Qiu"
            ],
            "organization": "Alibaba",
            "date": "2024-12-19T17:56:09Z",
            "url": "https://arxiv.org/abs/2412.15115",
            "description": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 146,
          "function_name": "current_landscape",
          "code": "text(\"- DeepSeek\\'s models \"), link(deepseek_67b), link(deepseek_v2), link(deepseek_v3)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DeepSeek's models ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
            "authors": [
              "DeepSeek-AI",
              ":",
              "Xiao Bi",
              "Deli Chen",
              "Guanting Chen",
              "Shanhuang Chen",
              "Damai Dai",
              "Chengqi Deng",
              "Honghui Ding",
              "Kai Dong",
              "Qiushi Du",
              "Zhe Fu",
              "Huazuo Gao",
              "Kaige Gao",
              "Wenjun Gao",
              "Ruiqi Ge",
              "Kang Guan",
              "Daya Guo",
              "Jianzhong Guo",
              "Guangbo Hao",
              "Zhewen Hao",
              "Ying He",
              "Wenjie Hu",
              "Panpan Huang",
              "Erhang Li",
              "Guowei Li",
              "Jiashi Li",
              "Yao Li",
              "Y. K. Li",
              "Wenfeng Liang",
              "Fangyun Lin",
              "A. X. Liu",
              "Bo Liu",
              "Wen Liu",
              "Xiaodong Liu",
              "Xin Liu",
              "Yiyuan Liu",
              "Haoyu Lu",
              "Shanghao Lu",
              "Fuli Luo",
              "Shirong Ma",
              "Xiaotao Nie",
              "Tian Pei",
              "Yishi Piao",
              "Junjie Qiu",
              "Hui Qu",
              "Tongzheng Ren",
              "Zehui Ren",
              "Chong Ruan",
              "Zhangli Sha",
              "Zhihong Shao",
              "Junxiao Song",
              "Xuecheng Su",
              "Jingxiang Sun",
              "Yaofeng Sun",
              "Minghui Tang",
              "Bingxuan Wang",
              "Peiyi Wang",
              "Shiyu Wang",
              "Yaohui Wang",
              "Yongji Wang",
              "Tong Wu",
              "Y. Wu",
              "Xin Xie",
              "Zhenda Xie",
              "Ziwei Xie",
              "Yiliang Xiong",
              "Hanwei Xu",
              "R. X. Xu",
              "Yanhong Xu",
              "Dejian Yang",
              "Yuxiang You",
              "Shuiping Yu",
              "Xingkai Yu",
              "B. Zhang",
              "Haowei Zhang",
              "Lecong Zhang",
              "Liyue Zhang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Wentao Zhang",
              "Yichao Zhang",
              "Chenggang Zhao",
              "Yao Zhao",
              "Shangyan Zhou",
              "Shunfeng Zhou",
              "Qihao Zhu",
              "Yuheng Zou"
            ],
            "organization": "DeepSeek",
            "date": "2024-01-05T18:59:13Z",
            "url": "https://arxiv.org/pdf/2401.02954.pdf",
            "description": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",
            "notes": "Data: DeepSeek, The Stack, Reddit, etc. (2T tokens)\nArchitecture: LLaMA, but: for GQA increased depth, 67B parameters\nScaling laws: used non-embedding FLOPs with IsoFLOP"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
            "authors": [
              "DeepSeek-AI",
              "Aixin Liu",
              "Bei Feng",
              "Bin Wang",
              "Bingxuan Wang",
              "Bo Liu",
              "Chenggang Zhao",
              "Chengqi Dengr",
              "Chong Ruan",
              "Damai Dai",
              "Daya Guo",
              "Dejian Yang",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Hanwei Xu",
              "Hao Yang",
              "Haowei Zhang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Li",
              "Hui Qu",
              "J. L. Cai",
              "Jian Liang",
              "Jianzhong Guo",
              "Jiaqi Ni",
              "Jiashi Li",
              "Jin Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junxiao Song",
              "Kai Dong",
              "Kaige Gao",
              "Kang Guan",
              "Lean Wang",
              "Lecong Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Liang Zhao",
              "Liyue Zhang",
              "Meng Li",
              "Miaojun Wang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peiyi Wang",
              "Peng Zhang",
              "Qihao Zhu",
              "Qinyu Chen",
              "Qiushi Du",
              "R. J. Chen",
              "R. L. Jin",
              "Ruiqi Ge",
              "Ruizhe Pan",
              "Runxin Xu",
              "Ruyi Chen",
              "S. S. Li",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Shirong Ma",
              "Shiyu Wang",
              "Shuang Zhou",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Size Zheng",
              "T. Wang",
              "Tian Pei",
              "Tian Yuan",
              "Tianyu Sun",
              "W. L. Xiao",
              "Wangding Zeng",
              "Wei An",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wentao Zhang",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xianzu Wang",
              "Xiao Bi",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaojin Shen",
              "Xiaokang Chen",
              "Xiaosha Chen",
              "Xiaotao Nie",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xin Liu",
              "Xin Xie",
              "Xingkai Yu",
              "Xinnan Song",
              "Xinyi Zhou",
              "Xinyu Yang",
              "Xuan Lu",
              "Xuecheng Su",
              "Y. Wu",
              "Y. K. Li",
              "Y. X. Wei",
              "Y. X. Zhu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Li",
              "Yaohui Wang",
              "Yi Zheng",
              "Yichao Zhang",
              "Yiliang Xiong",
              "Yilong Zhao",
              "Ying He",
              "Ying Tang",
              "Yishi Piao",
              "Yixin Dong",
              "Yixuan Tan",
              "Yiyuan Liu",
              "Yongji Wang",
              "Yongqiang Guo",
              "Yuchen Zhu",
              "Yuduan Wang",
              "Yuheng Zou",
              "Yukun Zha",
              "Yunxian Ma",
              "Yuting Yan",
              "Yuxiang You",
              "Yuxuan Liu",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhen Huang",
              "Zhen Zhang",
              "Zhenda Xie",
              "Zhewen Hao",
              "Zhihong Shao",
              "Zhiniu Wen",
              "Zhipeng Xu",
              "Zhongyu Zhang",
              "Zhuoshu Li",
              "Zihan Wang",
              "Zihui Gu",
              "Zilin Li",
              "Ziwei Xie"
            ],
            "organization": null,
            "date": "2024-05-07T15:56:43Z",
            "url": "https://arxiv.org/abs/2405.04434",
            "description": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-V3 Technical Report",
            "authors": [
              "DeepSeek-AI",
              "Aixin Liu",
              "Bei Feng",
              "Bing Xue",
              "Bingxuan Wang",
              "Bochao Wu",
              "Chengda Lu",
              "Chenggang Zhao",
              "Chengqi Deng",
              "Chenyu Zhang",
              "Chong Ruan",
              "Damai Dai",
              "Daya Guo",
              "Dejian Yang",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fucong Dai",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Han Bao",
              "Hanwei Xu",
              "Haocheng Wang",
              "Haowei Zhang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Li",
              "Hui Qu",
              "J. L. Cai",
              "Jian Liang",
              "Jianzhong Guo",
              "Jiaqi Ni",
              "Jiashi Li",
              "Jiawei Wang",
              "Jin Chen",
              "Jingchang Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junlong Li",
              "Junxiao Song",
              "Kai Dong",
              "Kai Hu",
              "Kaige Gao",
              "Kang Guan",
              "Kexin Huang",
              "Kuai Yu",
              "Lean Wang",
              "Lecong Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Liang Zhao",
              "Litong Wang",
              "Liyue Zhang",
              "Meng Li",
              "Miaojun Wang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peiyi Wang",
              "Peng Zhang",
              "Qiancheng Wang",
              "Qihao Zhu",
              "Qinyu Chen",
              "Qiushi Du",
              "R. J. Chen",
              "R. L. Jin",
              "Ruiqi Ge",
              "Ruisong Zhang",
              "Ruizhe Pan",
              "Runji Wang",
              "Runxin Xu",
              "Ruoyu Zhang",
              "Ruyi Chen",
              "S. S. Li",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Shengfeng Ye",
              "Shirong Ma",
              "Shiyu Wang",
              "Shuang Zhou",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Shuting Pan",
              "T. Wang",
              "Tao Yun",
              "Tian Pei",
              "Tianyu Sun",
              "W. L. Xiao",
              "Wangding Zeng",
              "Wanjia Zhao",
              "Wei An",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wenqin Yu",
              "Wentao Zhang",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xianzu Wang",
              "Xiao Bi",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaojin Shen",
              "Xiaokang Chen",
              "Xiaokang Zhang",
              "Xiaosha Chen",
              "Xiaotao Nie",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xin Cheng",
              "Xin Liu",
              "Xin Xie",
              "Xingchao Liu",
              "Xingkai Yu",
              "Xinnan Song",
              "Xinxia Shan",
              "Xinyi Zhou",
              "Xinyu Yang",
              "Xinyuan Li",
              "Xuecheng Su",
              "Xuheng Lin",
              "Y. K. Li",
              "Y. Q. Wang",
              "Y. X. Wei",
              "Y. X. Zhu",
              "Yang Zhang",
              "Yanhong Xu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Li",
              "Yaohui Wang",
              "Yi Yu",
              "Yi Zheng",
              "Yichao Zhang",
              "Yifan Shi",
              "Yiliang Xiong",
              "Ying He",
              "Ying Tang",
              "Yishi Piao",
              "Yisong Wang",
              "Yixuan Tan",
              "Yiyang Ma",
              "Yiyuan Liu",
              "Yongqiang Guo",
              "Yu Wu",
              "Yuan Ou",
              "Yuchen Zhu",
              "Yuduan Wang",
              "Yue Gong",
              "Yuheng Zou",
              "Yujia He",
              "Yukun Zha",
              "Yunfan Xiong",
              "Yunxian Ma",
              "Yuting Yan",
              "Yuxiang Luo",
              "Yuxiang You",
              "Yuxuan Liu",
              "Yuyang Zhou",
              "Z. F. Wu",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhean Xu",
              "Zhen Huang",
              "Zhen Zhang",
              "Zhenda Xie",
              "Zhengyan Zhang",
              "Zhewen Hao",
              "Zhibin Gou",
              "Zhicheng Ma",
              "Zhigang Yan",
              "Zhihong Shao",
              "Zhipeng Xu",
              "Zhiyu Wu",
              "Zhongyu Zhang",
              "Zhuoshu Li",
              "Zihui Gu",
              "Zijia Zhu",
              "Zijun Liu",
              "Zilin Li",
              "Ziwei Xie",
              "Ziyang Song",
              "Ziyi Gao",
              "Zizheng Pan"
            ],
            "organization": null,
            "date": "2024-12-27T04:03:16Z",
            "url": "https://arxiv.org/pdf/2412.19437.pdf",
            "description": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 147,
          "function_name": "current_landscape",
          "code": "text(\"- AI2's OLMo 2 \"), link(olmo_7b), link(olmo2),"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- AI2's OLMo 2 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "OLMo: Accelerating the Science of Language Models",
            "authors": [
              "Dirk Groeneveld",
              "Iz Beltagy",
              "Pete Walsh",
              "Akshita Bhagia",
              "Rodney Kinney",
              "Oyvind Tafjord",
              "Ananya Harsh Jha",
              "Hamish Ivison",
              "Ian Magnusson",
              "Yizhong Wang",
              "Shane Arora",
              "David Atkinson",
              "Russell Authur",
              "Khyathi Raghavi Chandu",
              "Arman Cohan",
              "Jennifer Dumas",
              "Yanai Elazar",
              "Yuling Gu",
              "Jack Hessel",
              "Tushar Khot",
              "William Merrill",
              "Jacob Morrison",
              "Niklas Muennighoff",
              "Aakanksha Naik",
              "Crystal Nam",
              "Matthew E. Peters",
              "Valentina Pyatkin",
              "Abhilasha Ravichander",
              "Dustin Schwenk",
              "Saurabh Shah",
              "Will Smith",
              "Emma Strubell",
              "Nishant Subramani",
              "Mitchell Wortsman",
              "Pradeep Dasigi",
              "Nathan Lambert",
              "Kyle Richardson",
              "Luke Zettlemoyer",
              "Jesse Dodge",
              "Kyle Lo",
              "Luca Soldaini",
              "Noah A. Smith",
              "Hannaneh Hajishirzi"
            ],
            "organization": "AI2",
            "date": "2024-02-01T18:28:55Z",
            "url": "https://arxiv.org/pdf/2402.00838.pdf",
            "description": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
            "notes": "Data: subset of Dolma (2.46T tokens, CommonCrawl, The Stack, Reddit, etc.)\nArchitecture: no biases, non-parametric layer norm, SwiGLU (8/3 d increased to closest multiple of 128)\nTraining: 256x4 AMD MI250X on LUMI supercomputer, 27x8 A100s, 800Gbps interconnect"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "2 OLMo 2 Furious",
            "authors": [
              "Team OLMo",
              "Pete Walsh",
              "Luca Soldaini",
              "Dirk Groeneveld",
              "Kyle Lo",
              "Shane Arora",
              "Akshita Bhagia",
              "Yuling Gu",
              "Shengyi Huang",
              "Matt Jordan",
              "Nathan Lambert",
              "Dustin Schwenk",
              "Oyvind Tafjord",
              "Taira Anderson",
              "David Atkinson",
              "Faeze Brahman",
              "Christopher Clark",
              "Pradeep Dasigi",
              "Nouha Dziri",
              "Michal Guerquin",
              "Hamish Ivison",
              "Pang Wei Koh",
              "Jiacheng Liu",
              "Saumya Malik",
              "William Merrill",
              "Lester James V. Miranda",
              "Jacob Morrison",
              "Tyler Murray",
              "Crystal Nam",
              "Valentina Pyatkin",
              "Aman Rangapur",
              "Michael Schmitz",
              "Sam Skjonsberg",
              "David Wadden",
              "Christopher Wilhelm",
              "Michael Wilson",
              "Luke Zettlemoyer",
              "Ali Farhadi",
              "Noah A. Smith",
              "Hannaneh Hajishirzi"
            ],
            "organization": "AI2",
            "date": "2024-12-31T21:55:10Z",
            "url": "https://arxiv.org/abs/2501.00656",
            "description": "We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\\\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 149,
          "function_name": "current_landscape",
          "code": "text(\"## Levels of openness\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Levels of openness",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 150,
          "function_name": "current_landscape",
          "code": "text(\"- Closed models (e.g., GPT-4o): API access only \"), link(gpt4)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Closed models (e.g., GPT-4o): API access only ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GPT-4 Technical Report",
            "authors": [
              "OpenAI",
              "Josh Achiam",
              "Steven Adler",
              "Sandhini Agarwal",
              "Lama Ahmad",
              "Ilge Akkaya",
              "Florencia Leoni Aleman",
              "Diogo Almeida",
              "Janko Altenschmidt",
              "Sam Altman",
              "Shyamal Anadkat",
              "Red Avila",
              "Igor Babuschkin",
              "Suchir Balaji",
              "Valerie Balcom",
              "Paul Baltescu",
              "Haiming Bao",
              "Mohammad Bavarian",
              "Jeff Belgum",
              "Irwan Bello",
              "Jake Berdine",
              "Gabriel Bernadett-Shapiro",
              "Christopher Berner",
              "Lenny Bogdonoff",
              "Oleg Boiko",
              "Madelaine Boyd",
              "Anna-Luisa Brakman",
              "Greg Brockman",
              "Tim Brooks",
              "Miles Brundage",
              "Kevin Button",
              "Trevor Cai",
              "Rosie Campbell",
              "Andrew Cann",
              "Brittany Carey",
              "Chelsea Carlson",
              "Rory Carmichael",
              "Brooke Chan",
              "Che Chang",
              "Fotis Chantzis",
              "Derek Chen",
              "Sully Chen",
              "Ruby Chen",
              "Jason Chen",
              "Mark Chen",
              "Ben Chess",
              "Chester Cho",
              "Casey Chu",
              "Hyung Won Chung",
              "Dave Cummings",
              "Jeremiah Currier",
              "Yunxing Dai",
              "Cory Decareaux",
              "Thomas Degry",
              "Noah Deutsch",
              "Damien Deville",
              "Arka Dhar",
              "David Dohan",
              "Steve Dowling",
              "Sheila Dunning",
              "Adrien Ecoffet",
              "Atty Eleti",
              "Tyna Eloundou",
              "David Farhi",
              "Liam Fedus",
              "Niko Felix",
              "Sim\u00f3n Posada Fishman",
              "Juston Forte",
              "Isabella Fulford",
              "Leo Gao",
              "Elie Georges",
              "Christian Gibson",
              "Vik Goel",
              "Tarun Gogineni",
              "Gabriel Goh",
              "Rapha Gontijo-Lopes",
              "Jonathan Gordon",
              "Morgan Grafstein",
              "Scott Gray",
              "Ryan Greene",
              "Joshua Gross",
              "Shixiang Shane Gu",
              "Yufei Guo",
              "Chris Hallacy",
              "Jesse Han",
              "Jeff Harris",
              "Yuchen He",
              "Mike Heaton",
              "Johannes Heidecke",
              "Chris Hesse",
              "Alan Hickey",
              "Wade Hickey",
              "Peter Hoeschele",
              "Brandon Houghton",
              "Kenny Hsu",
              "Shengli Hu",
              "Xin Hu",
              "Joost Huizinga",
              "Shantanu Jain",
              "Shawn Jain",
              "Joanne Jang",
              "Angela Jiang",
              "Roger Jiang",
              "Haozhun Jin",
              "Denny Jin",
              "Shino Jomoto",
              "Billie Jonn",
              "Heewoo Jun",
              "Tomer Kaftan",
              "\u0141ukasz Kaiser",
              "Ali Kamali",
              "Ingmar Kanitscheider",
              "Nitish Shirish Keskar",
              "Tabarak Khan",
              "Logan Kilpatrick",
              "Jong Wook Kim",
              "Christina Kim",
              "Yongjik Kim",
              "Jan Hendrik Kirchner",
              "Jamie Kiros",
              "Matt Knight",
              "Daniel Kokotajlo",
              "\u0141ukasz Kondraciuk",
              "Andrew Kondrich",
              "Aris Konstantinidis",
              "Kyle Kosic",
              "Gretchen Krueger",
              "Vishal Kuo",
              "Michael Lampe",
              "Ikai Lan",
              "Teddy Lee",
              "Jan Leike",
              "Jade Leung",
              "Daniel Levy",
              "Chak Ming Li",
              "Rachel Lim",
              "Molly Lin",
              "Stephanie Lin",
              "Mateusz Litwin",
              "Theresa Lopez",
              "Ryan Lowe",
              "Patricia Lue",
              "Anna Makanju",
              "Kim Malfacini",
              "Sam Manning",
              "Todor Markov",
              "Yaniv Markovski",
              "Bianca Martin",
              "Katie Mayer",
              "Andrew Mayne",
              "Bob McGrew",
              "Scott Mayer McKinney",
              "Christine McLeavey",
              "Paul McMillan",
              "Jake McNeil",
              "David Medina",
              "Aalok Mehta",
              "Jacob Menick",
              "Luke Metz",
              "Andrey Mishchenko",
              "Pamela Mishkin",
              "Vinnie Monaco",
              "Evan Morikawa",
              "Daniel Mossing",
              "Tong Mu",
              "Mira Murati",
              "Oleg Murk",
              "David M\u00e9ly",
              "Ashvin Nair",
              "Reiichiro Nakano",
              "Rajeev Nayak",
              "Arvind Neelakantan",
              "Richard Ngo",
              "Hyeonwoo Noh",
              "Long Ouyang",
              "Cullen O'Keefe",
              "Jakub Pachocki",
              "Alex Paino",
              "Joe Palermo",
              "Ashley Pantuliano",
              "Giambattista Parascandolo",
              "Joel Parish",
              "Emy Parparita",
              "Alex Passos",
              "Mikhail Pavlov",
              "Andrew Peng",
              "Adam Perelman",
              "Filipe de Avila Belbute Peres",
              "Michael Petrov",
              "Henrique Ponde de Oliveira Pinto",
              "Michael",
              "Pokorny",
              "Michelle Pokrass",
              "Vitchyr H. Pong",
              "Tolly Powell",
              "Alethea Power",
              "Boris Power",
              "Elizabeth Proehl",
              "Raul Puri",
              "Alec Radford",
              "Jack Rae",
              "Aditya Ramesh",
              "Cameron Raymond",
              "Francis Real",
              "Kendra Rimbach",
              "Carl Ross",
              "Bob Rotsted",
              "Henri Roussez",
              "Nick Ryder",
              "Mario Saltarelli",
              "Ted Sanders",
              "Shibani Santurkar",
              "Girish Sastry",
              "Heather Schmidt",
              "David Schnurr",
              "John Schulman",
              "Daniel Selsam",
              "Kyla Sheppard",
              "Toki Sherbakov",
              "Jessica Shieh",
              "Sarah Shoker",
              "Pranav Shyam",
              "Szymon Sidor",
              "Eric Sigler",
              "Maddie Simens",
              "Jordan Sitkin",
              "Katarina Slama",
              "Ian Sohl",
              "Benjamin Sokolowsky",
              "Yang Song",
              "Natalie Staudacher",
              "Felipe Petroski Such",
              "Natalie Summers",
              "Ilya Sutskever",
              "Jie Tang",
              "Nikolas Tezak",
              "Madeleine B. Thompson",
              "Phil Tillet",
              "Amin Tootoonchian",
              "Elizabeth Tseng",
              "Preston Tuggle",
              "Nick Turley",
              "Jerry Tworek",
              "Juan Felipe Cer\u00f3n Uribe",
              "Andrea Vallone",
              "Arun Vijayvergiya",
              "Chelsea Voss",
              "Carroll Wainwright",
              "Justin Jay Wang",
              "Alvin Wang",
              "Ben Wang",
              "Jonathan Ward",
              "Jason Wei",
              "CJ Weinmann",
              "Akila Welihinda",
              "Peter Welinder",
              "Jiayi Weng",
              "Lilian Weng",
              "Matt Wiethoff",
              "Dave Willner",
              "Clemens Winter",
              "Samuel Wolrich",
              "Hannah Wong",
              "Lauren Workman",
              "Sherwin Wu",
              "Jeff Wu",
              "Michael Wu",
              "Kai Xiao",
              "Tao Xu",
              "Sarah Yoo",
              "Kevin Yu",
              "Qiming Yuan",
              "Wojciech Zaremba",
              "Rowan Zellers",
              "Chong Zhang",
              "Marvin Zhang",
              "Shengjia Zhao",
              "Tianhao Zheng",
              "Juntang Zhuang",
              "William Zhuk",
              "Barret Zoph"
            ],
            "organization": "OpenAI",
            "date": "2023-03-15T17:15:04Z",
            "url": "https://arxiv.org/pdf/2303.08774.pdf",
            "description": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
            "notes": "No details on the data or model architecture."
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 151,
          "function_name": "current_landscape",
          "code": "text(\"- Open-weight models (e.g., DeepSeek): weights available, paper with architecture details, some training details, no data details \"), link(deepseek_v3)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Open-weight models (e.g., DeepSeek): weights available, paper with architecture details, some training details, no data details ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-V3 Technical Report",
            "authors": [
              "DeepSeek-AI",
              "Aixin Liu",
              "Bei Feng",
              "Bing Xue",
              "Bingxuan Wang",
              "Bochao Wu",
              "Chengda Lu",
              "Chenggang Zhao",
              "Chengqi Deng",
              "Chenyu Zhang",
              "Chong Ruan",
              "Damai Dai",
              "Daya Guo",
              "Dejian Yang",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fucong Dai",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Han Bao",
              "Hanwei Xu",
              "Haocheng Wang",
              "Haowei Zhang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Li",
              "Hui Qu",
              "J. L. Cai",
              "Jian Liang",
              "Jianzhong Guo",
              "Jiaqi Ni",
              "Jiashi Li",
              "Jiawei Wang",
              "Jin Chen",
              "Jingchang Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junlong Li",
              "Junxiao Song",
              "Kai Dong",
              "Kai Hu",
              "Kaige Gao",
              "Kang Guan",
              "Kexin Huang",
              "Kuai Yu",
              "Lean Wang",
              "Lecong Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Liang Zhao",
              "Litong Wang",
              "Liyue Zhang",
              "Meng Li",
              "Miaojun Wang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peiyi Wang",
              "Peng Zhang",
              "Qiancheng Wang",
              "Qihao Zhu",
              "Qinyu Chen",
              "Qiushi Du",
              "R. J. Chen",
              "R. L. Jin",
              "Ruiqi Ge",
              "Ruisong Zhang",
              "Ruizhe Pan",
              "Runji Wang",
              "Runxin Xu",
              "Ruoyu Zhang",
              "Ruyi Chen",
              "S. S. Li",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Shengfeng Ye",
              "Shirong Ma",
              "Shiyu Wang",
              "Shuang Zhou",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Shuting Pan",
              "T. Wang",
              "Tao Yun",
              "Tian Pei",
              "Tianyu Sun",
              "W. L. Xiao",
              "Wangding Zeng",
              "Wanjia Zhao",
              "Wei An",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wenqin Yu",
              "Wentao Zhang",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xianzu Wang",
              "Xiao Bi",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaojin Shen",
              "Xiaokang Chen",
              "Xiaokang Zhang",
              "Xiaosha Chen",
              "Xiaotao Nie",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xin Cheng",
              "Xin Liu",
              "Xin Xie",
              "Xingchao Liu",
              "Xingkai Yu",
              "Xinnan Song",
              "Xinxia Shan",
              "Xinyi Zhou",
              "Xinyu Yang",
              "Xinyuan Li",
              "Xuecheng Su",
              "Xuheng Lin",
              "Y. K. Li",
              "Y. Q. Wang",
              "Y. X. Wei",
              "Y. X. Zhu",
              "Yang Zhang",
              "Yanhong Xu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Li",
              "Yaohui Wang",
              "Yi Yu",
              "Yi Zheng",
              "Yichao Zhang",
              "Yifan Shi",
              "Yiliang Xiong",
              "Ying He",
              "Ying Tang",
              "Yishi Piao",
              "Yisong Wang",
              "Yixuan Tan",
              "Yiyang Ma",
              "Yiyuan Liu",
              "Yongqiang Guo",
              "Yu Wu",
              "Yuan Ou",
              "Yuchen Zhu",
              "Yuduan Wang",
              "Yue Gong",
              "Yuheng Zou",
              "Yujia He",
              "Yukun Zha",
              "Yunfan Xiong",
              "Yunxian Ma",
              "Yuting Yan",
              "Yuxiang Luo",
              "Yuxiang You",
              "Yuxuan Liu",
              "Yuyang Zhou",
              "Z. F. Wu",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhean Xu",
              "Zhen Huang",
              "Zhen Zhang",
              "Zhenda Xie",
              "Zhengyan Zhang",
              "Zhewen Hao",
              "Zhibin Gou",
              "Zhicheng Ma",
              "Zhigang Yan",
              "Zhihong Shao",
              "Zhipeng Xu",
              "Zhiyu Wu",
              "Zhongyu Zhang",
              "Zhuoshu Li",
              "Zihui Gu",
              "Zijia Zhu",
              "Zijun Liu",
              "Zilin Li",
              "Ziwei Xie",
              "Ziyang Song",
              "Ziyi Gao",
              "Zizheng Pan"
            ],
            "organization": null,
            "date": "2024-12-27T04:03:16Z",
            "url": "https://arxiv.org/pdf/2412.19437.pdf",
            "description": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 152,
          "function_name": "current_landscape",
          "code": "text(\"- Open-source models (e.g., OLMo): weights and data available, paper with most details (but not necessarily the rationale, failed experiments) \"), link(olmo_7b)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Open-source models (e.g., OLMo): weights and data available, paper with most details (but not necessarily the rationale, failed experiments) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "OLMo: Accelerating the Science of Language Models",
            "authors": [
              "Dirk Groeneveld",
              "Iz Beltagy",
              "Pete Walsh",
              "Akshita Bhagia",
              "Rodney Kinney",
              "Oyvind Tafjord",
              "Ananya Harsh Jha",
              "Hamish Ivison",
              "Ian Magnusson",
              "Yizhong Wang",
              "Shane Arora",
              "David Atkinson",
              "Russell Authur",
              "Khyathi Raghavi Chandu",
              "Arman Cohan",
              "Jennifer Dumas",
              "Yanai Elazar",
              "Yuling Gu",
              "Jack Hessel",
              "Tushar Khot",
              "William Merrill",
              "Jacob Morrison",
              "Niklas Muennighoff",
              "Aakanksha Naik",
              "Crystal Nam",
              "Matthew E. Peters",
              "Valentina Pyatkin",
              "Abhilasha Ravichander",
              "Dustin Schwenk",
              "Saurabh Shah",
              "Will Smith",
              "Emma Strubell",
              "Nishant Subramani",
              "Mitchell Wortsman",
              "Pradeep Dasigi",
              "Nathan Lambert",
              "Kyle Richardson",
              "Luke Zettlemoyer",
              "Jesse Dodge",
              "Kyle Lo",
              "Luca Soldaini",
              "Noah A. Smith",
              "Hannaneh Hajishirzi"
            ],
            "organization": "AI2",
            "date": "2024-02-01T18:28:55Z",
            "url": "https://arxiv.org/pdf/2402.00838.pdf",
            "description": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
            "notes": "Data: subset of Dolma (2.46T tokens, CommonCrawl, The Stack, Reddit, etc.)\nArchitecture: no biases, non-parametric layer norm, SwiGLU (8/3 d increased to closest multiple of 128)\nTraining: 256x4 AMD MI250X on LUMI supercomputer, 27x8 A100s, 800Gbps interconnect"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 154,
          "function_name": "current_landscape",
          "code": "text(\"## Today's frontier models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Today's frontier models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 155,
          "function_name": "current_landscape",
          "code": "text(\"- OpenAI's o3 \"), link(\"https://openai.com/index/openai-o3-mini/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- OpenAI's o3 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://openai.com/index/openai-o3-mini/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 156,
          "function_name": "current_landscape",
          "code": "text(\"- Anthropic's Claude Sonnet 3.7 \"), link(\"https://www.anthropic.com/news/claude-3-7-sonnet\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Anthropic's Claude Sonnet 3.7 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.anthropic.com/news/claude-3-7-sonnet",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 157,
          "function_name": "current_landscape",
          "code": "text(\"- xAI's Grok 3 \"), link(\"https://x.ai/news/grok-3\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- xAI's Grok 3 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://x.ai/news/grok-3",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 158,
          "function_name": "current_landscape",
          "code": "text(\"- Google's Gemini 2.5 \"), link(\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Google's Gemini 2.5 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 159,
          "function_name": "current_landscape",
          "code": "text(\"- Meta's Llama 3.3 \"), link(\"https://ai.meta.com/blog/meta-llama-3/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Meta's Llama 3.3 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://ai.meta.com/blog/meta-llama-3/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 160,
          "function_name": "current_landscape",
          "code": "text(\"- DeepSeek's r1 \"), link(deepseek_r1)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DeepSeek's r1 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
            "authors": [
              "DeepSeek-AI",
              "Daya Guo",
              "Dejian Yang",
              "Haowei Zhang",
              "Junxiao Song",
              "Ruoyu Zhang",
              "Runxin Xu",
              "Qihao Zhu",
              "Shirong Ma",
              "Peiyi Wang",
              "Xiao Bi",
              "Xiaokang Zhang",
              "Xingkai Yu",
              "Yu Wu",
              "Z. F. Wu",
              "Zhibin Gou",
              "Zhihong Shao",
              "Zhuoshu Li",
              "Ziyi Gao",
              "Aixin Liu",
              "Bing Xue",
              "Bingxuan Wang",
              "Bochao Wu",
              "Bei Feng",
              "Chengda Lu",
              "Chenggang Zhao",
              "Chengqi Deng",
              "Chenyu Zhang",
              "Chong Ruan",
              "Damai Dai",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fucong Dai",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Han Bao",
              "Hanwei Xu",
              "Haocheng Wang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Qu",
              "Hui Li",
              "Jianzhong Guo",
              "Jiashi Li",
              "Jiawei Wang",
              "Jingchang Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junlong Li",
              "J. L. Cai",
              "Jiaqi Ni",
              "Jian Liang",
              "Jin Chen",
              "Kai Dong",
              "Kai Hu",
              "Kaige Gao",
              "Kang Guan",
              "Kexin Huang",
              "Kuai Yu",
              "Lean Wang",
              "Lecong Zhang",
              "Liang Zhao",
              "Litong Wang",
              "Liyue Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Meng Li",
              "Miaojun Wang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peng Zhang",
              "Qiancheng Wang",
              "Qinyu Chen",
              "Qiushi Du",
              "Ruiqi Ge",
              "Ruisong Zhang",
              "Ruizhe Pan",
              "Runji Wang",
              "R. J. Chen",
              "R. L. Jin",
              "Ruyi Chen",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shengfeng Ye",
              "Shiyu Wang",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Shuting Pan",
              "S. S. Li",
              "Shuang Zhou",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Tao Yun",
              "Tian Pei",
              "Tianyu Sun",
              "T. Wang",
              "Wangding Zeng",
              "Wanjia Zhao",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wenqin Yu",
              "Wentao Zhang",
              "W. L. Xiao",
              "Wei An",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaokang Chen",
              "Xiaotao Nie",
              "Xin Cheng",
              "Xin Liu",
              "Xin Xie",
              "Xingchao Liu",
              "Xinyu Yang",
              "Xinyuan Li",
              "Xuecheng Su",
              "Xuheng Lin",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xiaojin Shen",
              "Xiaosha Chen",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xinnan Song",
              "Xinyi Zhou",
              "Xianzu Wang",
              "Xinxia Shan",
              "Y. K. Li",
              "Y. Q. Wang",
              "Y. X. Wei",
              "Yang Zhang",
              "Yanhong Xu",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Wang",
              "Yi Yu",
              "Yichao Zhang",
              "Yifan Shi",
              "Yiliang Xiong",
              "Ying He",
              "Yishi Piao",
              "Yisong Wang",
              "Yixuan Tan",
              "Yiyang Ma",
              "Yiyuan Liu",
              "Yongqiang Guo",
              "Yuan Ou",
              "Yuduan Wang",
              "Yue Gong",
              "Yuheng Zou",
              "Yujia He",
              "Yunfan Xiong",
              "Yuxiang Luo",
              "Yuxiang You",
              "Yuxuan Liu",
              "Yuyang Zhou",
              "Y. X. Zhu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yaohui Li",
              "Yi Zheng",
              "Yuchen Zhu",
              "Yunxian Ma",
              "Ying Tang",
              "Yukun Zha",
              "Yuting Yan",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhean Xu",
              "Zhenda Xie",
              "Zhengyan Zhang",
              "Zhewen Hao",
              "Zhicheng Ma",
              "Zhigang Yan",
              "Zhiyu Wu",
              "Zihui Gu",
              "Zijia Zhu",
              "Zijun Liu",
              "Zilin Li",
              "Ziwei Xie",
              "Ziyang Song",
              "Zizheng Pan",
              "Zhen Huang",
              "Zhipeng Xu",
              "Zhongyu Zhang",
              "Zhen Zhang"
            ],
            "organization": null,
            "date": "2025-01-22T15:19:35Z",
            "url": "https://arxiv.org/pdf/2501.12948.pdf",
            "description": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 161,
          "function_name": "current_landscape",
          "code": "text(\"- Alibaba's Qwen 2.5 Max \"), link(\"https://qwenlm.github.io/blog/qwen2.5-max/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Alibaba's Qwen 2.5 Max ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://qwenlm.github.io/blog/qwen2.5-max/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 162,
          "function_name": "current_landscape",
          "code": "text(\"- Tencent's Hunyuan-T1 \"), link(\"https://tencent.github.io/llm.hunyuan.T1/README_EN.html\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tencent's Hunyuan-T1 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://tencent.github.io/llm.hunyuan.T1/README_EN.html",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 26,
          "function_name": "main",
          "code": "current_landscape()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 165,
          "function_name": "what_is_this_program",
          "code": "def what_is_this_program():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 166,
          "function_name": "what_is_this_program",
          "code": "text(\"This is an *executable lecture*, a program whose execution delivers the content of a lecture.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is an *executable lecture*, a program whose execution delivers the content of a lecture.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 167,
          "function_name": "what_is_this_program",
          "code": "text(\"Executable lectures make it possible to:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Executable lectures make it possible to:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 168,
          "function_name": "what_is_this_program",
          "code": "text(\"- view and run code (since everything is code!),\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- view and run code (since everything is code!),",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 169,
          "function_name": "what_is_this_program",
          "code": "total = 0  # @inspect total"
        }
      ],
      "env": {
        "total": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 170,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 171,
          "function_name": "what_is_this_program",
          "code": "total += x  # @inspect total"
        }
      ],
      "env": {
        "total": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 170,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 171,
          "function_name": "what_is_this_program",
          "code": "total += x  # @inspect total"
        }
      ],
      "env": {
        "total": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 170,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 171,
          "function_name": "what_is_this_program",
          "code": "total += x  # @inspect total"
        }
      ],
      "env": {
        "total": 6
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 170,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 172,
          "function_name": "what_is_this_program",
          "code": "text(\"- see the hierarchical structure of the lecture, and\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- see the hierarchical structure of the lecture, and",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 173,
          "function_name": "what_is_this_program",
          "code": "text(\"- jump to definitions and concepts: \"), link(supervised_finetuning)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- jump to definitions and concepts: ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": "supervised_finetuning",
          "style": {},
          "external_link": null,
          "internal_link": {
            "path": "lecture_01.py",
            "line_number": 432
          }
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 28,
          "function_name": "main",
          "code": "what_is_this_program()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 176,
          "function_name": "course_logistics",
          "code": "def course_logistics():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 177,
          "function_name": "course_logistics",
          "code": "text(\"All information online: \"), link(\"https://stanford-cs336.github.io/spring2025/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "All information online: ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://stanford-cs336.github.io/spring2025/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 179,
          "function_name": "course_logistics",
          "code": "text(\"This is a 5-unit class.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is a 5-unit class.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 180,
          "function_name": "course_logistics",
          "code": "text(\"Comment from Spring 2024 course evaluation: *The entire assignment was approximately the same amount of work as all 5 assignments from CS 224n plus the final project. And that's just the first homework assignment.*\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Comment from Spring 2024 course evaluation: *The entire assignment was approximately the same amount of work as all 5 assignments from CS 224n plus the final project. And that's just the first homework assignment.*",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 182,
          "function_name": "course_logistics",
          "code": "text(\"## Why you should take this course\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Why you should take this course",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 183,
          "function_name": "course_logistics",
          "code": "text(\"- You have an obsessive need to understand how things work.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- You have an obsessive need to understand how things work.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 184,
          "function_name": "course_logistics",
          "code": "text(\"- You want to build up your research engineering muscles.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- You want to build up your research engineering muscles.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 186,
          "function_name": "course_logistics",
          "code": "text(\"## Why you should not take this course\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Why you should not take this course",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 187,
          "function_name": "course_logistics",
          "code": "text(\"- You actually want to get research done this quarter.<br>(Talk to your advisor.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- You actually want to get research done this quarter.<br>(Talk to your advisor.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 188,
          "function_name": "course_logistics",
          "code": "text(\"- You are interested in learning about the hottest new techniques in AI (e.g., multimodality, RAG, etc.).<br>(You should take a seminar class for that.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- You are interested in learning about the hottest new techniques in AI (e.g., multimodality, RAG, etc.).<br>(You should take a seminar class for that.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 189,
          "function_name": "course_logistics",
          "code": "text(\"- You want to get good results on your own application domain.<br>(You should just prompt or fine-tune an existing model.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- You want to get good results on your own application domain.<br>(You should just prompt or fine-tune an existing model.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 191,
          "function_name": "course_logistics",
          "code": "text(\"## How you can follow along at home\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## How you can follow along at home",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 192,
          "function_name": "course_logistics",
          "code": "text(\"- All lecture materials and assignments will be posted online, so feel free to follow on your own.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- All lecture materials and assignments will be posted online, so feel free to follow on your own.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 193,
          "function_name": "course_logistics",
          "code": "text(\"- Lectures are recorded via [CGOE, formally SCPD](https://cgoe.stanford.edu/) and be made available on YouTube (with some lag).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Lectures are recorded via [CGOE, formally SCPD](https://cgoe.stanford.edu/) and be made available on YouTube (with some lag).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 194,
          "function_name": "course_logistics",
          "code": "text(\"- We plan to offer this class again next year.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- We plan to offer this class again next year.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 196,
          "function_name": "course_logistics",
          "code": "text(\"## Assignments\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Assignments",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 197,
          "function_name": "course_logistics",
          "code": "text(\"- 5 assignments (basics, systems, scaling laws, data, alignment).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- 5 assignments (basics, systems, scaling laws, data, alignment).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 198,
          "function_name": "course_logistics",
          "code": "text(\"- No scaffolding code, but we provide unit tests and adapter interfaces to help you check correctness.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- No scaffolding code, but we provide unit tests and adapter interfaces to help you check correctness.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 199,
          "function_name": "course_logistics",
          "code": "text(\"- Implement locally to test for correctness, then run on cluster for benchmarking (accuracy and speed).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement locally to test for correctness, then run on cluster for benchmarking (accuracy and speed).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 200,
          "function_name": "course_logistics",
          "code": "text(\"- Leaderboard for some assignments (minimize perplexity given training budget).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Leaderboard for some assignments (minimize perplexity given training budget).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 201,
          "function_name": "course_logistics",
          "code": "text(\"- AI tools (e.g., CoPilot, Cursor) can take away from learning, so use at your own risk.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- AI tools (e.g., CoPilot, Cursor) can take away from learning, so use at your own risk.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 203,
          "function_name": "course_logistics",
          "code": "text(\"## Cluster\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Cluster",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 204,
          "function_name": "course_logistics",
          "code": "text(\"- Thanks to Together AI for providing a compute cluster. \ud83d\ude4f\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Thanks to Together AI for providing a compute cluster. \ud83d\ude4f",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 205,
          "function_name": "course_logistics",
          "code": "text(\"- Please read [the guide](https://docs.google.com/document/d/1BSSig7zInyjDKcbNGftVxubiHlwJ-ZqahQewIzBmBOo/edit) on how to use the cluster.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Please read [the guide](https://docs.google.com/document/d/1BSSig7zInyjDKcbNGftVxubiHlwJ-ZqahQewIzBmBOo/edit) on how to use the cluster.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 206,
          "function_name": "course_logistics",
          "code": "text(\"- Start your assignments early, since the cluster will fill up close to the deadline!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Start your assignments early, since the cluster will fill up close to the deadline!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 30,
          "function_name": "main",
          "code": "course_logistics()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 209,
          "function_name": "course_components",
          "code": "def course_components():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 210,
          "function_name": "course_components",
          "code": "text(\"## It's all about efficiency\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## It's all about efficiency",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 211,
          "function_name": "course_components",
          "code": "text(\"Resources: data + hardware (compute, memory, communication bandwidth)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Resources: data + hardware (compute, memory, communication bandwidth)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 212,
          "function_name": "course_components",
          "code": "text(\"How do you train the best model given a fixed set of resources?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How do you train the best model given a fixed set of resources?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 213,
          "function_name": "course_components",
          "code": "text(\"Example: given a Common Crawl dump and 32 H100s for 2 weeks, what should you do?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example: given a Common Crawl dump and 32 H100s for 2 weeks, what should you do?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 215,
          "function_name": "course_components",
          "code": "text(\"Design decisions:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Design decisions:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 216,
          "function_name": "course_components",
          "code": "image(\"images/design-decisions.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/design-decisions.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 218,
          "function_name": "course_components",
          "code": "text(\"## Overview of the course\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Overview of the course",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 247,
          "function_name": "basics",
          "code": "def basics():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 248,
          "function_name": "basics",
          "code": "text(\"Goal: get a basic version of the full pipeline working\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Goal: get a basic version of the full pipeline working",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 249,
          "function_name": "basics",
          "code": "text(\"Components: tokenization, model architecture, training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Components: tokenization, model architecture, training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 251,
          "function_name": "basics",
          "code": "text(\"## Tokenization\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Tokenization",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 252,
          "function_name": "basics",
          "code": "text(\"Tokenizers convert between strings and sequences of integers (tokens)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Tokenizers convert between strings and sequences of integers (tokens)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 253,
          "function_name": "basics",
          "code": "image(\"images/tokenized-example.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/tokenized-example.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 254,
          "function_name": "basics",
          "code": "text(\"Intuition: break up string into popular segments\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuition: break up string into popular segments",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 256,
          "function_name": "basics",
          "code": "text(\"This course: Byte-Pair Encoding (BPE) tokenizer \"), link(sennrich_2016)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This course: Byte-Pair Encoding (BPE) tokenizer ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Neural Machine Translation of Rare Words with Subword Units",
            "authors": [
              "Rico Sennrich",
              "Barry Haddow",
              "Alexandra Birch"
            ],
            "organization": null,
            "date": "2015-08-31T16:37:31Z",
            "url": "https://arxiv.org/abs/1508.07909",
            "description": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 258,
          "function_name": "basics",
          "code": "text(\"Tokenizer-free approaches: \"), link(byt5), link(megabyte), link(blt), link(tfree)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Tokenizer-free approaches: ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
            "authors": [
              "Linting Xue",
              "Aditya Barua",
              "Noah Constant",
              "Rami Al-Rfou",
              "Sharan Narang",
              "Mihir Kale",
              "Adam Roberts",
              "Colin Raffel"
            ],
            "organization": null,
            "date": "2021-05-28T07:03:22Z",
            "url": "https://arxiv.org/abs/2105.13626",
            "description": "Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
            "authors": [
              "Lili Yu",
              "D\u00e1niel Simig",
              "Colin Flaherty",
              "Armen Aghajanyan",
              "Luke Zettlemoyer",
              "Mike Lewis"
            ],
            "organization": null,
            "date": "2023-05-12T00:55:41Z",
            "url": "https://arxiv.org/pdf/2305.07185.pdf",
            "description": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Byte Latent Transformer: Patches Scale Better Than Tokens",
            "authors": [
              "Artidoro Pagnoni",
              "Ram Pasunuru",
              "Pedro Rodriguez",
              "John Nguyen",
              "Benjamin Muller",
              "Margaret Li",
              "Chunting Zhou",
              "Lili Yu",
              "Jason Weston",
              "Luke Zettlemoyer",
              "Gargi Ghosh",
              "Mike Lewis",
              "Ari Holtzman",
              "Srinivasan Iyer"
            ],
            "organization": null,
            "date": "2024-12-13T05:33:32Z",
            "url": "https://arxiv.org/abs/2412.09871",
            "description": "We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
            "authors": [
              "Bj\u00f6rn Deiseroth",
              "Manuel Brack",
              "Patrick Schramowski",
              "Kristian Kersting",
              "Samuel Weinbach"
            ],
            "organization": null,
            "date": "2024-06-27T14:49:08Z",
            "url": "https://arxiv.org/abs/2406.19223",
            "description": "Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages. To remedy these issues, we propose T-FREE, which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-FREE inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-FREE shows significant improvements in cross-lingual transfer learning.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 259,
          "function_name": "basics",
          "code": "text(\"Use bytes directly, promising, but have not yet been scaled up to the frontier.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Use bytes directly, promising, but have not yet been scaled up to the frontier.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 261,
          "function_name": "basics",
          "code": "text(\"## Architecture\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Architecture",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 262,
          "function_name": "basics",
          "code": "text(\"Starting point: original Transformer \"), link(transformer_2017)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Starting point: original Transformer ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Attention Is All You Need",
            "authors": [
              "Ashish Vaswani",
              "Noam Shazeer",
              "Niki Parmar",
              "Jakob Uszkoreit",
              "Llion Jones",
              "Aidan N. Gomez",
              "Lukasz Kaiser",
              "Illia Polosukhin"
            ],
            "organization": "Google",
            "date": "2017-06-12T17:57:34Z",
            "url": "https://arxiv.org/pdf/1706.03762.pdf",
            "description": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "notes": "Introduced Transformer (for machine translation)"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 263,
          "function_name": "basics",
          "code": "image(\"images/transformer-architecture.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/transformer-architecture.png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 265,
          "function_name": "basics",
          "code": "text(\"Variants:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Variants:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 266,
          "function_name": "basics",
          "code": "text(\"- Activation functions: ReLU, SwiGLU \"), link(shazeer_2020)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Activation functions: ReLU, SwiGLU ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GLU Variants Improve Transformer",
            "authors": [
              "Noam Shazeer"
            ],
            "organization": "Google",
            "date": "2020-02-12T19:57:13Z",
            "url": "https://arxiv.org/pdf/2002.05202.pdf",
            "description": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.",
            "notes": "Experiments with different activation functions\nActivation functions: ReLU, GeLU, Swish\nApply idea of gated units (GLU): ReGLU, GeGLU, SwiGLU\nFFN-SwiGLU = Swish(x W1) * xV W2\nHave 3 matrices now, so make hidden dimension 2/3 of the 2 matrix version"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 267,
          "function_name": "basics",
          "code": "text(\"- Positional encodings: sinusoidal, RoPE \"), link(rope_2021)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Positional encodings: sinusoidal, RoPE ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
            "authors": [
              "Jianlin Su",
              "Yu Lu",
              "Shengfeng Pan",
              "Ahmed Murtadha",
              "Bo Wen",
              "Yunfeng Liu"
            ],
            "organization": null,
            "date": "2021-04-20T09:54:06Z",
            "url": "https://arxiv.org/pdf/2104.09864.pdf",
            "description": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
            "notes": "Encodes absolute position with rotation matrix, incorporate relative position dependency in self-attention\nKey: R W x, where R is a block-diagonal sequence of d/2 rotation matrices (equation 13)\nExtrapolates to longer sequences"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 268,
          "function_name": "basics",
          "code": "text(\"- Normalization: LayerNorm, RMSNorm \"), link(layernorm_2016), link(rms_norm_2019)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Normalization: LayerNorm, RMSNorm ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Layer Normalization",
            "authors": [
              "Jimmy Lei Ba",
              "Jamie Ryan Kiros",
              "Geoffrey E. Hinton"
            ],
            "organization": null,
            "date": "2016-07-21T19:57:52Z",
            "url": "https://arxiv.org/pdf/1607.06450.pdf",
            "description": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
            "notes": "Introduced LayerNorm"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Root Mean Square Layer Normalization",
            "authors": [
              "Biao Zhang",
              "Rico Sennrich"
            ],
            "organization": null,
            "date": "2019-10-16T16:44:22Z",
            "url": "https://arxiv.org/abs/1910.07467",
            "description": "Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 269,
          "function_name": "basics",
          "code": "text(\"- Placement of normalization: pre-norm versus post-norm \"), link(pre_post_norm_2020)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Placement of normalization: pre-norm versus post-norm ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "On Layer Normalization in the Transformer Architecture",
            "authors": [
              "Ruibin Xiong",
              "Yunchang Yang",
              "Di He",
              "Kai Zheng",
              "Shuxin Zheng",
              "Chen Xing",
              "Huishuai Zhang",
              "Yanyan Lan",
              "Liwei Wang",
              "Tie-Yan Liu"
            ],
            "organization": null,
            "date": "2020-02-12T00:33:03Z",
            "url": "https://arxiv.org/pdf/2002.04745.pdf",
            "description": "The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 270,
          "function_name": "basics",
          "code": "text(\"- MLP: dense, mixture of experts \"), link(moe_2017)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- MLP: dense, mixture of experts ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
            "authors": [
              "Noam Shazeer",
              "Azalia Mirhoseini",
              "Krzysztof Maziarz",
              "Andy Davis",
              "Quoc Le",
              "Geoffrey Hinton",
              "Jeff Dean"
            ],
            "organization": "Google",
            "date": "2017-01-23T18:10:00Z",
            "url": "https://arxiv.org/pdf/1701.06538.pdf",
            "description": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 271,
          "function_name": "basics",
          "code": "text(\"- Attention: full, sliding window, linear \"), link(mistral_7b), link(\"https://arxiv.org/abs/2006.16236\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Attention: full, sliding window, linear ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Mistral 7B",
            "authors": [
              "Albert Q. Jiang",
              "Alexandre Sablayrolles",
              "Arthur Mensch",
              "Chris Bamford",
              "Devendra Singh Chaplot",
              "Diego de las Casas",
              "Florian Bressand",
              "Gianna Lengyel",
              "Guillaume Lample",
              "Lucile Saulnier",
              "L\u00e9lio Renard Lavaud",
              "Marie-Anne Lachaux",
              "Pierre Stock",
              "Teven Le Scao",
              "Thibaut Lavril",
              "Thomas Wang",
              "Timoth\u00e9e Lacroix",
              "William El Sayed"
            ],
            "organization": "Mistral",
            "date": "2023-10-10T17:54:58Z",
            "url": "https://arxiv.org/pdf/2310.06825.pdf",
            "description": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",
            "notes": "GQA, sliding window attention"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
            "authors": [
              "Angelos Katharopoulos",
              "Apoorv Vyas",
              "Nikolaos Pappas",
              "Fran\u00e7ois Fleuret"
            ],
            "organization": null,
            "date": "2020-06-29T17:55:38Z",
            "url": "https://arxiv.org/abs/2006.16236",
            "description": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 272,
          "function_name": "basics",
          "code": "text(\"- Lower-dimensional attention: group-query attention (GQA), multi-head latent attention (MLA) \"), link(gqa), link(mla)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Lower-dimensional attention: group-query attention (GQA), multi-head latent attention (MLA) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
            "authors": [
              "Joshua Ainslie",
              "James Lee-Thorp",
              "Michiel de Jong",
              "Yury Zemlyanskiy",
              "Federico Lebr\u00f3n",
              "Sumit Sanghai"
            ],
            "organization": "Google",
            "date": "2023-05-22T17:16:38Z",
            "url": "https://arxiv.org/pdf/2305.13245.pdf",
            "description": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
            "notes": "Multi-query attention (MQA) speeds up, but less expressive\nGQA: use an intermediate (more than one, less than number of heads) number of key-value heads\nExperiments on T5"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
            "authors": [
              "DeepSeek-AI",
              "Aixin Liu",
              "Bei Feng",
              "Bin Wang",
              "Bingxuan Wang",
              "Bo Liu",
              "Chenggang Zhao",
              "Chengqi Dengr",
              "Chong Ruan",
              "Damai Dai",
              "Daya Guo",
              "Dejian Yang",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Hanwei Xu",
              "Hao Yang",
              "Haowei Zhang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Li",
              "Hui Qu",
              "J. L. Cai",
              "Jian Liang",
              "Jianzhong Guo",
              "Jiaqi Ni",
              "Jiashi Li",
              "Jin Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junxiao Song",
              "Kai Dong",
              "Kaige Gao",
              "Kang Guan",
              "Lean Wang",
              "Lecong Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Liang Zhao",
              "Liyue Zhang",
              "Meng Li",
              "Miaojun Wang",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peiyi Wang",
              "Peng Zhang",
              "Qihao Zhu",
              "Qinyu Chen",
              "Qiushi Du",
              "R. J. Chen",
              "R. L. Jin",
              "Ruiqi Ge",
              "Ruizhe Pan",
              "Runxin Xu",
              "Ruyi Chen",
              "S. S. Li",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Shirong Ma",
              "Shiyu Wang",
              "Shuang Zhou",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Size Zheng",
              "T. Wang",
              "Tian Pei",
              "Tian Yuan",
              "Tianyu Sun",
              "W. L. Xiao",
              "Wangding Zeng",
              "Wei An",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wentao Zhang",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xianzu Wang",
              "Xiao Bi",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaojin Shen",
              "Xiaokang Chen",
              "Xiaosha Chen",
              "Xiaotao Nie",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xin Liu",
              "Xin Xie",
              "Xingkai Yu",
              "Xinnan Song",
              "Xinyi Zhou",
              "Xinyu Yang",
              "Xuan Lu",
              "Xuecheng Su",
              "Y. Wu",
              "Y. K. Li",
              "Y. X. Wei",
              "Y. X. Zhu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Li",
              "Yaohui Wang",
              "Yi Zheng",
              "Yichao Zhang",
              "Yiliang Xiong",
              "Yilong Zhao",
              "Ying He",
              "Ying Tang",
              "Yishi Piao",
              "Yixin Dong",
              "Yixuan Tan",
              "Yiyuan Liu",
              "Yongji Wang",
              "Yongqiang Guo",
              "Yuchen Zhu",
              "Yuduan Wang",
              "Yuheng Zou",
              "Yukun Zha",
              "Yunxian Ma",
              "Yuting Yan",
              "Yuxiang You",
              "Yuxuan Liu",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhen Huang",
              "Zhen Zhang",
              "Zhenda Xie",
              "Zhewen Hao",
              "Zhihong Shao",
              "Zhiniu Wen",
              "Zhipeng Xu",
              "Zhongyu Zhang",
              "Zhuoshu Li",
              "Zihan Wang",
              "Zihui Gu",
              "Zilin Li",
              "Ziwei Xie"
            ],
            "organization": null,
            "date": "2024-05-07T15:56:43Z",
            "url": "https://arxiv.org/abs/2405.04434",
            "description": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 273,
          "function_name": "basics",
          "code": "text(\"- State-space models: Hyena \"), link(\"https://arxiv.org/abs/2302.10866\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- State-space models: Hyena ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
            "authors": [
              "Michael Poli",
              "Stefano Massaroli",
              "Eric Nguyen",
              "Daniel Y. Fu",
              "Tri Dao",
              "Stephen Baccus",
              "Yoshua Bengio",
              "Stefano Ermon",
              "Christopher R\u00e9"
            ],
            "organization": null,
            "date": "2023-02-21T18:29:25Z",
            "url": "https://arxiv.org/abs/2302.10866",
            "description": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 275,
          "function_name": "basics",
          "code": "text(\"## Training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 276,
          "function_name": "basics",
          "code": "text(\"- Optimizer (e.g., AdamW, Muon, SOAP) \"), link(adam2014), link(adamw2017), link(muon), link(soap)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Optimizer (e.g., AdamW, Muon, SOAP) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Adam: A Method for Stochastic Optimization",
            "authors": [
              "Diederik P. Kingma",
              "Jimmy Ba"
            ],
            "organization": null,
            "date": "2014-12-22T13:54:29Z",
            "url": "https://arxiv.org/pdf/1412.6980.pdf",
            "description": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "notes": "Introduced Adam optimizer based on RMSProp and momentum"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Decoupled Weight Decay Regularization",
            "authors": [
              "Ilya Loshchilov",
              "Frank Hutter"
            ],
            "organization": null,
            "date": "2017-11-14T14:24:06Z",
            "url": "https://arxiv.org/pdf/1711.05101.pdf",
            "description": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW",
            "notes": "Improves Adam by decoupling weight decay"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Muon: An optimizer for hidden layers in neural networks",
            "authors": [
              "Jordan Keller"
            ],
            "organization": null,
            "date": "2024-12-08",
            "url": "https://kellerjordan.github.io/posts/muon/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "SOAP: Improving and Stabilizing Shampoo using Adam",
            "authors": [
              "Nikhil Vyas",
              "Depen Morwani",
              "Rosie Zhao",
              "Mujin Kwun",
              "Itai Shapira",
              "David Brandfonbrener",
              "Lucas Janson",
              "Sham Kakade"
            ],
            "organization": null,
            "date": "2024-09-17T16:18:05Z",
            "url": "https://arxiv.org/abs/2409.11321",
            "description": "There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient approximation of Adam -- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: $\\textbf{S}$hampo$\\textbf{O}$ with $\\textbf{A}$dam in the $\\textbf{P}$reconditioner's eigenbasis (SOAP). With regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at https://github.com/nikhilvyas/SOAP.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 277,
          "function_name": "basics",
          "code": "text(\"- Learning rate schedule (e.g., cosine, WSD) \"), link(cosine_learning_rate_2017), link(wsd_2024)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Learning rate schedule (e.g., cosine, WSD) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
            "authors": [
              "Ilya Loshchilov",
              "Frank Hutter"
            ],
            "organization": null,
            "date": "2016-08-13T13:46:05Z",
            "url": "https://arxiv.org/pdf/1608.03983.pdf",
            "description": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR",
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
            "authors": [
              "Shengding Hu",
              "Yuge Tu",
              "Xu Han",
              "Chaoqun He",
              "Ganqu Cui",
              "Xiang Long",
              "Zhi Zheng",
              "Yewei Fang",
              "Yuxiang Huang",
              "Weilin Zhao",
              "Xinrong Zhang",
              "Zheng Leng Thai",
              "Kaihuo Zhang",
              "Chongyi Wang",
              "Yuan Yao",
              "Chenyang Zhao",
              "Jie Zhou",
              "Jie Cai",
              "Zhongwu Zhai",
              "Ning Ding",
              "Chao Jia",
              "Guoyang Zeng",
              "Dahai Li",
              "Zhiyuan Liu",
              "Maosong Sun"
            ],
            "organization": "Tsinghua",
            "date": "2024-04-09T15:36:50Z",
            "url": "https://arxiv.org/pdf/2404.06395.pdf",
            "description": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 278,
          "function_name": "basics",
          "code": "text(\"- Batch size (e..g, critical batch size) \"), link(large_batch_training_2018)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Batch size (e..g, critical batch size) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "An Empirical Model of Large-Batch Training",
            "authors": [
              "Sam McCandlish",
              "Jared Kaplan",
              "Dario Amodei",
              "OpenAI Dota Team"
            ],
            "organization": null,
            "date": "2018-12-14T20:49:09Z",
            "url": "https://arxiv.org/pdf/1812.06162.pdf",
            "description": "In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.",
            "notes": "Introduced critical batch size"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 279,
          "function_name": "basics",
          "code": "text(\"- Regularization (e.g., dropout, weight decay)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Regularization (e.g., dropout, weight decay)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 280,
          "function_name": "basics",
          "code": "text(\"- Hyperparameters (number of heads, hidden dimension): grid search\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Hyperparameters (number of heads, hidden dimension): grid search",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 282,
          "function_name": "basics",
          "code": "text(\"## Assignment 1\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Assignment 1",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 283,
          "function_name": "basics",
          "code": "link(title=\"[GitHub]\", url=\"https://github.com/stanford-cs336/assignment1-basics\"), link(title=\"[PDF]\", url=\"https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[GitHub]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/assignment1-basics",
            "description": null,
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[PDF]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 284,
          "function_name": "basics",
          "code": "text(\"- Implement BPE tokenizer\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement BPE tokenizer",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 285,
          "function_name": "basics",
          "code": "text(\"- Implement Transformer, cross-entropy loss, AdamW optimizer, training loop\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement Transformer, cross-entropy loss, AdamW optimizer, training loop",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 286,
          "function_name": "basics",
          "code": "text(\"- Train on TinyStories and OpenWebText\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Train on TinyStories and OpenWebText",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 287,
          "function_name": "basics",
          "code": "text(\"- Leaderboard: minimize OpenWebText perplexity given 90 minutes on a H100 \"), link(title=\"[last year's leaderboard]\", url=\"https://github.com/stanford-cs336/spring2024-assignment1-basics-leaderboard\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Leaderboard: minimize OpenWebText perplexity given 90 minutes on a H100 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[last year's leaderboard]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment1-basics-leaderboard",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 219,
          "function_name": "course_components",
          "code": "basics()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 290,
          "function_name": "systems",
          "code": "def systems():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 291,
          "function_name": "systems",
          "code": "text(\"Goal: squeeze the most out of the hardware\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Goal: squeeze the most out of the hardware",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 292,
          "function_name": "systems",
          "code": "text(\"Components: kernels, parallelism, inference\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Components: kernels, parallelism, inference",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 294,
          "function_name": "systems",
          "code": "text(\"## Kernels\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Kernels",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 295,
          "function_name": "systems",
          "code": "text(\"What a GPU (A100) looks like:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What a GPU (A100) looks like:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 296,
          "function_name": "systems",
          "code": "image(\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6xoBKi5kL2dZpivFe1-zgw.jpeg\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-672bd77c57df485d07926615162a44d5-https_miro_medium_com_v2_resize_fit_2000_format_webp_1_6xoBKi5kL2dZpivFe1-zgw_jpeg",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 297,
          "function_name": "systems",
          "code": "text(\"Analogy: warehouse : DRAM :: factory : SRAM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Analogy: warehouse : DRAM :: factory : SRAM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 298,
          "function_name": "systems",
          "code": "image(\"https://horace.io/img/perf_intro/factory_bandwidth.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-27f5a4326a831bfe8b3af774827dd675-https_horace_io_img_perf_intro_factory_bandwidth_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 299,
          "function_name": "systems",
          "code": "text(\"Trick: organize computation to maximize utilization of GPUs by minimizing data movement\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Trick: organize computation to maximize utilization of GPUs by minimizing data movement",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 300,
          "function_name": "systems",
          "code": "text(\"Write kernels in CUDA/**Triton**/CUTLASS/ThunderKittens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Write kernels in CUDA/**Triton**/CUTLASS/ThunderKittens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 302,
          "function_name": "systems",
          "code": "text(\"## Parallelism\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Parallelism",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 303,
          "function_name": "systems",
          "code": "text(\"What if we have multiple GPUs (8 A100s)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What if we have multiple GPUs (8 A100s)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 304,
          "function_name": "systems",
          "code": "image(\"https://www.fibermall.com/blog/wp-content/uploads/2024/09/the-hardware-topology-of-a-typical-8xA100-GPU-host.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-f92abaa9bd1dcd5fc90c7b74f3b29260-https_www_fibermall_com_blog_wp-content_uploads_2024_09_the-hardware-topology-of-a-typical-8xA100-GPU-host_png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 305,
          "function_name": "systems",
          "code": "text(\"Data movement between GPUs is even slower, but same 'minimize data movement' principle holds\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Data movement between GPUs is even slower, but same 'minimize data movement' principle holds",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 306,
          "function_name": "systems",
          "code": "text(\"Use collective operations (e.g., gather, reduce, all-reduce)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Use collective operations (e.g., gather, reduce, all-reduce)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 307,
          "function_name": "systems",
          "code": "text(\"Shard (parameters, activations, gradients, optimizer states) across GPUs\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Shard (parameters, activations, gradients, optimizer states) across GPUs",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 308,
          "function_name": "systems",
          "code": "text(\"How to split computation: {data,tensor,pipeline,sequence} parallelism\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How to split computation: {data,tensor,pipeline,sequence} parallelism",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 310,
          "function_name": "systems",
          "code": "text(\"## Inference\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Inference",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 311,
          "function_name": "systems",
          "code": "text(\"Goal: generate tokens given a prompt (needed to actually use models!)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Goal: generate tokens given a prompt (needed to actually use models!)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 312,
          "function_name": "systems",
          "code": "text(\"Inference is also needed for reinforcement learning, test-time compute, evaluation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Inference is also needed for reinforcement learning, test-time compute, evaluation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 313,
          "function_name": "systems",
          "code": "text(\"Globally, inference compute (every use) exceeds training compute (one-time cost)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Globally, inference compute (every use) exceeds training compute (one-time cost)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 314,
          "function_name": "systems",
          "code": "text(\"Two phases: prefill and decode\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Two phases: prefill and decode",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 315,
          "function_name": "systems",
          "code": "image(\"images/prefill-decode.png\", width=500)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/prefill-decode.png",
          "style": {
            "width": 500
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 316,
          "function_name": "systems",
          "code": "text(\"Prefill (similar to training): tokens are given, can process all at once (compute-bound)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Prefill (similar to training): tokens are given, can process all at once (compute-bound)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 317,
          "function_name": "systems",
          "code": "text(\"Decode: need to generate one token at a time (memory-bound)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Decode: need to generate one token at a time (memory-bound)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 318,
          "function_name": "systems",
          "code": "text(\"Methods to speed up decoding:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Methods to speed up decoding:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 319,
          "function_name": "systems",
          "code": "text(\"- Use cheaper model (via model pruning, quantization, distillation)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use cheaper model (via model pruning, quantization, distillation)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 320,
          "function_name": "systems",
          "code": "text(\"- Speculative decoding: use a cheaper \\\"draft\\\" model to generate multiple tokens, then use the full model to score in parallel (exact decoding!)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Speculative decoding: use a cheaper \"draft\" model to generate multiple tokens, then use the full model to score in parallel (exact decoding!)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 321,
          "function_name": "systems",
          "code": "text(\"- Systems optimizations: KV caching, batching\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Systems optimizations: KV caching, batching",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 323,
          "function_name": "systems",
          "code": "text(\"## Assignment 2\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Assignment 2",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 324,
          "function_name": "systems",
          "code": "link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment2-systems\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment2-systems/blob/master/cs336_spring2024_assignment2_systems.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[GitHub from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment2-systems",
            "description": null,
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[PDF from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment2-systems/blob/master/cs336_spring2024_assignment2_systems.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 325,
          "function_name": "systems",
          "code": "text(\"- Implement a fused RMSNorm kernel in Triton\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement a fused RMSNorm kernel in Triton",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 326,
          "function_name": "systems",
          "code": "text(\"- Implement distributed data parallel training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement distributed data parallel training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 327,
          "function_name": "systems",
          "code": "text(\"- Implement optimizer state sharding\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement optimizer state sharding",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 328,
          "function_name": "systems",
          "code": "text(\"- Benchmark and profile the implementations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Benchmark and profile the implementations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 220,
          "function_name": "course_components",
          "code": "systems()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 331,
          "function_name": "scaling_laws",
          "code": "def scaling_laws():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 332,
          "function_name": "scaling_laws",
          "code": "text(\"Goal: do experiments at small scale, predict hyperparameters/loss at large scale\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Goal: do experiments at small scale, predict hyperparameters/loss at large scale",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 333,
          "function_name": "scaling_laws",
          "code": "text(\"Question: given a FLOPs budget ($C$), use a bigger model ($N$) or train on more tokens ($D$)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Question: given a FLOPs budget ($C$), use a bigger model ($N$) or train on more tokens ($D$)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 334,
          "function_name": "scaling_laws",
          "code": "text(\"Compute-optimal scaling laws: \"), link(kaplan_scaling_laws_2020), link(chinchilla)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute-optimal scaling laws: ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Scaling Laws for Neural Language Models",
            "authors": [
              "Jared Kaplan",
              "Sam McCandlish",
              "Tom Henighan",
              "Tom B. Brown",
              "Benjamin Chess",
              "Rewon Child",
              "Scott Gray",
              "Alec Radford",
              "Jeffrey Wu",
              "Dario Amodei"
            ],
            "organization": "OpenAI",
            "date": "2020-01-23T03:59:20Z",
            "url": "https://arxiv.org/pdf/2001.08361.pdf",
            "description": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
            "notes": "Vary model size, dataset size, compute; get power laws\nLarger models require fewer tokens"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Training Compute-Optimal Large Language Models",
            "authors": [
              "Jordan Hoffmann",
              "Sebastian Borgeaud",
              "Arthur Mensch",
              "Elena Buchatskaya",
              "Trevor Cai",
              "Eliza Rutherford",
              "Diego de Las Casas",
              "Lisa Anne Hendricks",
              "Johannes Welbl",
              "Aidan Clark",
              "Tom Hennigan",
              "Eric Noland",
              "Katie Millican",
              "George van den Driessche",
              "Bogdan Damoc",
              "Aurelia Guy",
              "Simon Osindero",
              "Karen Simonyan",
              "Erich Elsen",
              "Jack W. Rae",
              "Oriol Vinyals",
              "Laurent Sifre"
            ],
            "organization": "DeepMind",
            "date": "2022-03-29T13:38:03Z",
            "url": "https://arxiv.org/pdf/2203.15556.pdf",
            "description": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
            "notes": "Introduced the rigorous analysis scaling laws for language models\nKey improvement over Kaplan: tune learning rate for the compute budget\nApproach 1: for each model size, train with 4 learning rates, vary number of training tokens, fit lower envelope\nApproach 2 (IsoFLOP): for each model size, train with 9 training budgets, take last point\nApproach 3: fit parametric function L(N, D) = E + A/N^alpha + B/D^beta to data collected from approaches 1 and 2\nConclusion: model and data should scale up at same rate\nTable 3: extrapolate to 10 trillion parameters\nMassiveText, different data distribution (1.5 trillion tokens)\n70B parameters"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 335,
          "function_name": "scaling_laws",
          "code": "image(\"images/chinchilla-isoflop.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/chinchilla-isoflop.png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 336,
          "function_name": "scaling_laws",
          "code": "text(\"TL;DR: $D^* = 20 N^*$ (e.g., 1.4B parameter model should be trained on 28B tokens)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "TL;DR: $D^* = 20 N^*$ (e.g., 1.4B parameter model should be trained on 28B tokens)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 337,
          "function_name": "scaling_laws",
          "code": "text(\"But this doesn't take into account inference costs!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But this doesn't take into account inference costs!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 339,
          "function_name": "scaling_laws",
          "code": "text(\"## Assignment 3\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Assignment 3",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 340,
          "function_name": "scaling_laws",
          "code": "link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment3-scaling\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment3-scaling/blob/master/cs336_spring2024_assignment3_scaling.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[GitHub from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment3-scaling",
            "description": null,
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[PDF from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment3-scaling/blob/master/cs336_spring2024_assignment3_scaling.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 341,
          "function_name": "scaling_laws",
          "code": "text(\"- We define a training API (hyperparameters -> loss) based on previous runs\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- We define a training API (hyperparameters -> loss) based on previous runs",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 342,
          "function_name": "scaling_laws",
          "code": "text(\"- Submit \\\"training jobs\\\" (under a FLOPs budget) and gather data points\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Submit \"training jobs\" (under a FLOPs budget) and gather data points",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 343,
          "function_name": "scaling_laws",
          "code": "text(\"- Fit a scaling law to the data points\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Fit a scaling law to the data points",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 344,
          "function_name": "scaling_laws",
          "code": "text(\"- Submit predictions for scaled up hyperparameters\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Submit predictions for scaled up hyperparameters",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 345,
          "function_name": "scaling_laws",
          "code": "text(\"- Leaderboard: minimize loss given FLOPs budget\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Leaderboard: minimize loss given FLOPs budget",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 221,
          "function_name": "course_components",
          "code": "scaling_laws()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 348,
          "function_name": "data",
          "code": "def data():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 349,
          "function_name": "data",
          "code": "text(\"Question: What capabilities do we want the model to have?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Question: What capabilities do we want the model to have?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 350,
          "function_name": "data",
          "code": "text(\"Multilingual? Code? Math?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Multilingual? Code? Math?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 351,
          "function_name": "data",
          "code": "image(\"https://ar5iv.labs.arxiv.org/html/2101.00027/assets/pile_chart2.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-b3aebfa83a900cd491e70acf27806db3-https_ar5iv_labs_arxiv_org_html_2101_00027_assets_pile_chart2_png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 353,
          "function_name": "data",
          "code": "text(\"## Evaluation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Evaluation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 354,
          "function_name": "data",
          "code": "text(\"- Perplexity: textbook evaluation for language models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Perplexity: textbook evaluation for language models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 355,
          "function_name": "data",
          "code": "text(\"- Standardized testing (e.g., MMLU, HellaSwag, GSM8K)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Standardized testing (e.g., MMLU, HellaSwag, GSM8K)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 356,
          "function_name": "data",
          "code": "text(\"- Instruction following (e.g., AlpacaEval, IFEval, WildBench)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Instruction following (e.g., AlpacaEval, IFEval, WildBench)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 357,
          "function_name": "data",
          "code": "text(\"- Scaling test-time compute: chain-of-thought, ensembling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Scaling test-time compute: chain-of-thought, ensembling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 358,
          "function_name": "data",
          "code": "text(\"- LM-as-a-judge: evaluate generative tasks\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- LM-as-a-judge: evaluate generative tasks",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 359,
          "function_name": "data",
          "code": "text(\"- Full system: RAG, agents\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Full system: RAG, agents",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 361,
          "function_name": "data",
          "code": "text(\"## Data curation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Data curation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 362,
          "function_name": "data",
          "code": "text(\"- Data does not just fall from the sky.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Data does not just fall from the sky.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 382,
          "function_name": "look_at_web_data",
          "code": "def look_at_web_data():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 383,
          "function_name": "look_at_web_data",
          "code": "urls = get_common_crawl_urls()[:3]  # @inspect urls"
        }
      ],
      "env": {
        "urls": [
          "https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-18/segments/1712296815919.75/warc/CC-MAIN-20240412101354-20240412131354-00000.warc.gz",
          "https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-18/segments/1712296815919.75/warc/CC-MAIN-20240412101354-20240412131354-00001.warc.gz",
          "https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-18/segments/1712296815919.75/warc/CC-MAIN-20240412101354-20240412131354-00002.warc.gz"
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 384,
          "function_name": "look_at_web_data",
          "code": "documents = list(read_common_crawl(urls[1], limit=300))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 385,
          "function_name": "look_at_web_data",
          "code": "random.seed(40)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 386,
          "function_name": "look_at_web_data",
          "code": "random.shuffle(documents)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 387,
          "function_name": "look_at_web_data",
          "code": "documents = markdownify_documents(documents[:10])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 388,
          "function_name": "look_at_web_data",
          "code": "write_documents(documents, \"var/sample-documents.txt\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 389,
          "function_name": "look_at_web_data",
          "code": "link(title=\"[sample documents]\", url=\"var/sample-documents.txt\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[sample documents]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "var/sample-documents.txt",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 390,
          "function_name": "look_at_web_data",
          "code": "text(\"It's a wasteland out there!  Need to really process the data.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "It's a wasteland out there!  Need to really process the data.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 363,
          "function_name": "data",
          "code": "look_at_web_data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 364,
          "function_name": "data",
          "code": "text(\"- Sources: webpages crawled from the Internet, books, arXiv papers, GitHub code, etc.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Sources: webpages crawled from the Internet, books, arXiv papers, GitHub code, etc.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 365,
          "function_name": "data",
          "code": "text(\"- Appeal to fair use to train on copyright data? \"), link(\"https://arxiv.org/pdf/2303.15715.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Appeal to fair use to train on copyright data? ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Foundation Models and Fair Use",
            "authors": [
              "Peter Henderson",
              "Xuechen Li",
              "Dan Jurafsky",
              "Tatsunori Hashimoto",
              "Mark A. Lemley",
              "Percy Liang"
            ],
            "organization": null,
            "date": "2023-03-28T03:58:40Z",
            "url": "https://arxiv.org/pdf/2303.15715.pdf",
            "description": "Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant U.S. case law, drawing parallels to existing and potential applications for generating text, source code, and visual art. Experiments confirm that popular foundation models can generate content considerably similar to copyrighted material. Second, we discuss technical mitigations that can help foundation models stay in line with fair use. We argue that more research is needed to align mitigation strategies with the current state of the law. Lastly, we suggest that the law and technical mitigations should co-evolve. For example, coupled with other policy mechanisms, the law could more explicitly consider safe harbors when strong technical tools are used to mitigate infringement harms. This co-evolution may help strike a balance between intellectual property and innovation, which speaks to the original goal of fair use. But we emphasize that the strategies we describe here are not a panacea and more work is needed to develop policies that address the potential harms of foundation models.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 366,
          "function_name": "data",
          "code": "text(\"- Might have to license data (e.g., Google with Reddit data) \"), article_link(\"https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Might have to license data (e.g., Google with Reddit data) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 367,
          "function_name": "data",
          "code": "text(\"- Formats: HTML, PDF, directories (not text!)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Formats: HTML, PDF, directories (not text!)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 369,
          "function_name": "data",
          "code": "text(\"## Data processing\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Data processing",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 370,
          "function_name": "data",
          "code": "text(\"- Transformation: convert HTML/PDF to text (preserve content, some structure, rewriting)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Transformation: convert HTML/PDF to text (preserve content, some structure, rewriting)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 371,
          "function_name": "data",
          "code": "text(\"- Filtering: keep high quality data, remove harmful content (via classifiers)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Filtering: keep high quality data, remove harmful content (via classifiers)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 372,
          "function_name": "data",
          "code": "text(\"- Deduplication: save compute, avoid memorization; use Bloom filters or MinHash\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Deduplication: save compute, avoid memorization; use Bloom filters or MinHash",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 374,
          "function_name": "data",
          "code": "text(\"## Assignment 4\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Assignment 4",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 375,
          "function_name": "data",
          "code": "link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment4-data\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment4-data/blob/master/cs336_spring2024_assignment4_data.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[GitHub from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment4-data",
            "description": null,
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[PDF from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment4-data/blob/master/cs336_spring2024_assignment4_data.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 376,
          "function_name": "data",
          "code": "text(\"- Convert Common Crawl HTML to text\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Convert Common Crawl HTML to text",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 377,
          "function_name": "data",
          "code": "text(\"- Train classifiers to filter for quality and harmful content\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Train classifiers to filter for quality and harmful content",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 378,
          "function_name": "data",
          "code": "text(\"- Deduplication using MinHash\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Deduplication using MinHash",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 379,
          "function_name": "data",
          "code": "text(\"- Leaderboard: minimize perplexity given token budget\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Leaderboard: minimize perplexity given token budget",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 222,
          "function_name": "course_components",
          "code": "data()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 393,
          "function_name": "alignment",
          "code": "def alignment():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 394,
          "function_name": "alignment",
          "code": "text(\"So far, a **base model** is raw potential, very good at completing the next token.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "So far, a **base model** is raw potential, very good at completing the next token.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 395,
          "function_name": "alignment",
          "code": "text(\"Alignment makes the model actually useful.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Alignment makes the model actually useful.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 397,
          "function_name": "alignment",
          "code": "text(\"Goals of alignment:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Goals of alignment:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 398,
          "function_name": "alignment",
          "code": "text(\"- Get the language model to follow instructions\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Get the language model to follow instructions",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 399,
          "function_name": "alignment",
          "code": "text(\"- Tune the style (format, length, tone, etc.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tune the style (format, length, tone, etc.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 400,
          "function_name": "alignment",
          "code": "text(\"- Incorporate safety (e.g., refusals to answer harmful questions)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Incorporate safety (e.g., refusals to answer harmful questions)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 402,
          "function_name": "alignment",
          "code": "text(\"Two phases:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Two phases:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 432,
          "function_name": "supervised_finetuning",
          "code": "def supervised_finetuning():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 433,
          "function_name": "supervised_finetuning",
          "code": "text(\"## Supervised finetuning (SFT)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Supervised finetuning (SFT)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 435,
          "function_name": "supervised_finetuning",
          "code": "text(\"Instruction data: (prompt, response) pairs\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Instruction data: (prompt, response) pairs",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 437,
          "function_name": "supervised_finetuning",
          "code": "ChatExample("
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 439,
          "function_name": "supervised_finetuning",
          "code": "Turn(role=\"system\", content=\"You are a helpful assistant.\"),"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 440,
          "function_name": "supervised_finetuning",
          "code": "Turn(role=\"user\", content=\"What is 1 + 1?\"),"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 441,
          "function_name": "supervised_finetuning",
          "code": "Turn(role=\"assistant\", content=\"The answer is 2.\"),"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 438,
          "function_name": "supervised_finetuning",
          "code": "turns=["
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 437,
          "function_name": "supervised_finetuning",
          "code": "ChatExample("
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 436,
          "function_name": "supervised_finetuning",
          "code": "sft_data: list[ChatExample] = ["
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 445,
          "function_name": "supervised_finetuning",
          "code": "text(\"Data often involves human annotation.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Data often involves human annotation.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 446,
          "function_name": "supervised_finetuning",
          "code": "text(\"Intuition: base model already has the skills, just need few examples to surface them. \"), link(lima)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuition: base model already has the skills, just need few examples to surface them. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "LIMA: Less Is More for Alignment",
            "authors": [
              "Chunting Zhou",
              "Pengfei Liu",
              "Puxin Xu",
              "Srini Iyer",
              "Jiao Sun",
              "Yuning Mao",
              "Xuezhe Ma",
              "Avia Efrat",
              "Ping Yu",
              "Lili Yu",
              "Susan Zhang",
              "Gargi Ghosh",
              "Mike Lewis",
              "Luke Zettlemoyer",
              "Omer Levy"
            ],
            "organization": null,
            "date": "2023-05-18T17:45:22Z",
            "url": "https://arxiv.org/pdf/2305.11206.pdf",
            "description": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 447,
          "function_name": "supervised_finetuning",
          "code": "text(\"Supervised learning: fine-tune model to maximize p(response | prompt).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Supervised learning: fine-tune model to maximize p(response | prompt).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 403,
          "function_name": "alignment",
          "code": "supervised_finetuning()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 450,
          "function_name": "learning_from_feedback",
          "code": "def learning_from_feedback():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 451,
          "function_name": "learning_from_feedback",
          "code": "text(\"Now we have a preliminary instruction following model.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now we have a preliminary instruction following model.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 452,
          "function_name": "learning_from_feedback",
          "code": "text(\"Let's make it better without expensive annotation.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's make it better without expensive annotation.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 454,
          "function_name": "learning_from_feedback",
          "code": "text(\"## Preference data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Preference data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 455,
          "function_name": "learning_from_feedback",
          "code": "text(\"Data: generate multiple responses using model (e.g., [A, B]) to a given prompt.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Data: generate multiple responses using model (e.g., [A, B]) to a given prompt.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 456,
          "function_name": "learning_from_feedback",
          "code": "text(\"User provides preferences (e.g., A < B or A > B).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "User provides preferences (e.g., A < B or A > B).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 458,
          "function_name": "learning_from_feedback",
          "code": "PreferenceExample("
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 460,
          "function_name": "learning_from_feedback",
          "code": "Turn(role=\"system\", content=\"You are a helpful assistant.\"),"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 461,
          "function_name": "learning_from_feedback",
          "code": "Turn(role=\"user\", content=\"What is the best way to train a language model?\"),"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 459,
          "function_name": "learning_from_feedback",
          "code": "history=["
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 463,
          "function_name": "learning_from_feedback",
          "code": "response_a=\"You should use a large dataset and train for a long time.\","
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 464,
          "function_name": "learning_from_feedback",
          "code": "response_b=\"You should use a small dataset and train for a short time.\","
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 465,
          "function_name": "learning_from_feedback",
          "code": "chosen=\"a\","
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 458,
          "function_name": "learning_from_feedback",
          "code": "PreferenceExample("
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 457,
          "function_name": "learning_from_feedback",
          "code": "preference_data: list[PreferenceExample] = ["
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 469,
          "function_name": "learning_from_feedback",
          "code": "text(\"## Verifiers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Verifiers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 470,
          "function_name": "learning_from_feedback",
          "code": "text(\"- Formal verifiers (e.g., for code, math)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Formal verifiers (e.g., for code, math)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 471,
          "function_name": "learning_from_feedback",
          "code": "text(\"- Learned verifiers: train against an LM-as-a-judge\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Learned verifiers: train against an LM-as-a-judge",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 473,
          "function_name": "learning_from_feedback",
          "code": "text(\"## Algorithms\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Algorithms",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 474,
          "function_name": "learning_from_feedback",
          "code": "text(\"- Proximal Policy Optimization (PPO) from reinforcement learning \"), link(ppo2017), link(instruct_gpt)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Proximal Policy Optimization (PPO) from reinforcement learning ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Proximal Policy Optimization Algorithms",
            "authors": [
              "John Schulman",
              "Filip Wolski",
              "Prafulla Dhariwal",
              "Alec Radford",
              "Oleg Klimov"
            ],
            "organization": null,
            "date": "2017-07-20T02:32:33Z",
            "url": "https://arxiv.org/pdf/1707.06347.pdf",
            "description": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
            "notes": "Introduced PPO (for RL)"
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Training language models to follow instructions with human feedback",
            "authors": [
              "Long Ouyang",
              "Jeff Wu",
              "Xu Jiang",
              "Diogo Almeida",
              "Carroll L. Wainwright",
              "Pamela Mishkin",
              "Chong Zhang",
              "Sandhini Agarwal",
              "Katarina Slama",
              "Alex Ray",
              "John Schulman",
              "Jacob Hilton",
              "Fraser Kelton",
              "Luke Miller",
              "Maddie Simens",
              "Amanda Askell",
              "Peter Welinder",
              "Paul Christiano",
              "Jan Leike",
              "Ryan Lowe"
            ],
            "organization": "OpenAI",
            "date": "2022-03-04T07:04:42Z",
            "url": "https://arxiv.org/pdf/2203.02155.pdf",
            "description": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "notes": "Training language models to follow instructions with human feedback"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 475,
          "function_name": "learning_from_feedback",
          "code": "text(\"- Direct Policy Optimization (DPO): for preference data, simpler \"), link(dpo)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Direct Policy Optimization (DPO): for preference data, simpler ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "authors": [
              "Rafael Rafailov",
              "Archit Sharma",
              "Eric Mitchell",
              "Stefano Ermon",
              "Christopher D. Manning",
              "Chelsea Finn"
            ],
            "organization": null,
            "date": "2023-05-29T17:57:46Z",
            "url": "https://arxiv.org/pdf/2305.18290.pdf",
            "description": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 476,
          "function_name": "learning_from_feedback",
          "code": "text(\"- Group Relative Preference Optimization (GRPO): remove value function \"), link(grpo)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Group Relative Preference Optimization (GRPO): remove value function ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
            "authors": [
              "Zhihong Shao",
              "Peiyi Wang",
              "Qihao Zhu",
              "Runxin Xu",
              "Junxiao Song",
              "Xiao Bi",
              "Haowei Zhang",
              "Mingchuan Zhang",
              "Y. K. Li",
              "Y. Wu",
              "Daya Guo"
            ],
            "organization": null,
            "date": "2024-02-05T18:55:32Z",
            "url": "https://arxiv.org/pdf/2402.03300.pdf",
            "description": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 404,
          "function_name": "alignment",
          "code": "learning_from_feedback()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 406,
          "function_name": "alignment",
          "code": "text(\"## Assignment 5\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Assignment 5",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 407,
          "function_name": "alignment",
          "code": "link(title=\"[GitHub from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment5-alignment\"), link(title=\"[PDF from 2024]\", url=\"https://github.com/stanford-cs336/spring2024-assignment5-alignment/blob/master/cs336_spring2024_assignment5_alignment.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[GitHub from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment5-alignment",
            "description": null,
            "notes": null
          },
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[PDF from 2024]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stanford-cs336/spring2024-assignment5-alignment/blob/master/cs336_spring2024_assignment5_alignment.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 408,
          "function_name": "alignment",
          "code": "text(\"- Implement supervised fine-tuning\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement supervised fine-tuning",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 409,
          "function_name": "alignment",
          "code": "text(\"- Implement Direct Preference Optimization (DPO)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement Direct Preference Optimization (DPO)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 410,
          "function_name": "alignment",
          "code": "text(\"- Implement Group Relative Preference Optimization (GRPO)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Implement Group Relative Preference Optimization (GRPO)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 223,
          "function_name": "course_components",
          "code": "alignment()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 225,
          "function_name": "course_components",
          "code": "text(\"## Efficiency drives design decisions\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Efficiency drives design decisions",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 227,
          "function_name": "course_components",
          "code": "text(\"Today, we are compute-constrained, so design decisions will reflect squeezing the most out of given hardware.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Today, we are compute-constrained, so design decisions will reflect squeezing the most out of given hardware.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 228,
          "function_name": "course_components",
          "code": "text(\"- Data processing: avoid wasting precious compute updating on bad / irrelevant data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Data processing: avoid wasting precious compute updating on bad / irrelevant data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 229,
          "function_name": "course_components",
          "code": "text(\"- Tokenization: working with raw bytes is elegant, but compute-inefficient with today's model architectures.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tokenization: working with raw bytes is elegant, but compute-inefficient with today's model architectures.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 230,
          "function_name": "course_components",
          "code": "text(\"- Model architecture: many changes motivated by reducing memory or FLOPs (e.g., sharing KV caches, sliding window attention)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model architecture: many changes motivated by reducing memory or FLOPs (e.g., sharing KV caches, sliding window attention)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 231,
          "function_name": "course_components",
          "code": "text(\"- Training: we can get away with a single epoch!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Training: we can get away with a single epoch!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 232,
          "function_name": "course_components",
          "code": "text(\"- Scaling laws: use less compute on smaller models to do hyperparameter tuning\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Scaling laws: use less compute on smaller models to do hyperparameter tuning",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 233,
          "function_name": "course_components",
          "code": "text(\"- Alignment: if tune model more to desired use cases, require smaller base models\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Alignment: if tune model more to desired use cases, require smaller base models",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 235,
          "function_name": "course_components",
          "code": "text(\"Tomorrow, we will become data-constrained...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Tomorrow, we will become data-constrained...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 31,
          "function_name": "main",
          "code": "course_components()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 487,
          "function_name": "tokenization",
          "code": "def tokenization():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 488,
          "function_name": "tokenization",
          "code": "text(\"This unit was inspired by Andrej Karpathy's video on tokenization; check it out! \"), youtube_link(\"https://www.youtube.com/watch?v=zduSFxRajkE\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This unit was inspired by Andrej Karpathy's video on tokenization; check it out! ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [video]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.youtube.com/watch?v=zduSFxRajkE",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 578,
          "function_name": "intro_to_tokenization",
          "code": "def intro_to_tokenization():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 579,
          "function_name": "intro_to_tokenization",
          "code": "text(\"Raw text is generally represented as Unicode strings.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Raw text is generally represented as Unicode strings.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 580,
          "function_name": "intro_to_tokenization",
          "code": "string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 582,
          "function_name": "intro_to_tokenization",
          "code": "text(\"A language model places a probability distribution over sequences of tokens (usually represented by integer indices).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "A language model places a probability distribution over sequences of tokens (usually represented by integer indices).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 583,
          "function_name": "intro_to_tokenization",
          "code": "indices = [15496, 11, 995, 0]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 585,
          "function_name": "intro_to_tokenization",
          "code": "text(\"So we need a procedure that *encodes* strings into tokens.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "So we need a procedure that *encodes* strings into tokens.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 586,
          "function_name": "intro_to_tokenization",
          "code": "text(\"We also need a procedure that *decodes* tokens back into strings.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We also need a procedure that *decodes* tokens back into strings.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 587,
          "function_name": "intro_to_tokenization",
          "code": "text(\"A \"), link(Tokenizer), text(\" is a class that implements the encode and decode methods.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "A ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": "Tokenizer",
          "style": {},
          "external_link": null,
          "internal_link": {
            "path": "lecture_01.py",
            "line_number": 238
          }
        },
        {
          "type": "markdown",
          "data": " is a class that implements the encode and decode methods.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 588,
          "function_name": "intro_to_tokenization",
          "code": "text(\"The **vocabulary size** is number of possible tokens (integers).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The **vocabulary size** is number of possible tokens (integers).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 490,
          "function_name": "tokenization",
          "code": "intro_to_tokenization()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 591,
          "function_name": "tokenization_examples",
          "code": "def tokenization_examples():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 592,
          "function_name": "tokenization_examples",
          "code": "text(\"To get a feel for how tokenizers work, play with this \"), link(title=\"interactive site\", url=\"https://tiktokenizer.vercel.app/?encoder=gpt2\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "To get a feel for how tokenizers work, play with this ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "interactive site",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://tiktokenizer.vercel.app/?encoder=gpt2",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 594,
          "function_name": "tokenization_examples",
          "code": "text(\"## Observations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Observations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 595,
          "function_name": "tokenization_examples",
          "code": "text(\"- A word and its preceding space are part of the same token (e.g., \\\" world\\\").\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- A word and its preceding space are part of the same token (e.g., \" world\").",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 596,
          "function_name": "tokenization_examples",
          "code": "text(\"- A word at the beginning and in the middle are represented differently (e.g., \\\"hello hello\\\").\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- A word at the beginning and in the middle are represented differently (e.g., \"hello hello\").",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 597,
          "function_name": "tokenization_examples",
          "code": "text(\"- Numbers are tokenized into every few digits.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Numbers are tokenized into every few digits.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 599,
          "function_name": "tokenization_examples",
          "code": "text(\"Here's the GPT-2 tokenizer from OpenAI (tiktoken) in action.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Here's the GPT-2 tokenizer from OpenAI (tiktoken) in action.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 600,
          "function_name": "tokenization_examples",
          "code": "tokenizer = get_gpt2_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 600,
          "function_name": "tokenization_examples",
          "code": "tokenizer = get_gpt2_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 572,
          "function_name": "get_gpt2_tokenizer",
          "code": "def get_gpt2_tokenizer():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 600,
          "function_name": "tokenization_examples",
          "code": "tokenizer = get_gpt2_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 575,
          "function_name": "get_gpt2_tokenizer",
          "code": "return tiktoken.get_encoding(\"gpt2\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 600,
          "function_name": "tokenization_examples",
          "code": "tokenizer = get_gpt2_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 601,
          "function_name": "tokenization_examples",
          "code": "string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\"  # @inspect string"
        }
      ],
      "env": {
        "string": "Hello, \ud83c\udf0d! \u4f60\u597d!"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 603,
          "function_name": "tokenization_examples",
          "code": "text(\"Check that encode() and decode() roundtrip:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Check that encode() and decode() roundtrip:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 604,
          "function_name": "tokenization_examples",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          15496,
          11,
          12520,
          234,
          235,
          0,
          220,
          19526,
          254,
          25001,
          121,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 605,
          "function_name": "tokenization_examples",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        }
      ],
      "env": {
        "reconstructed_string": "Hello, \ud83c\udf0d! \u4f60\u597d!"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 606,
          "function_name": "tokenization_examples",
          "code": "assert string == reconstructed_string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 607,
          "function_name": "tokenization_examples",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 607,
          "function_name": "tokenization_examples",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 565,
          "function_name": "get_compression_ratio",
          "code": "def get_compression_ratio(string: str, indices: list[int]) -> float:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 607,
          "function_name": "tokenization_examples",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 567,
          "function_name": "get_compression_ratio",
          "code": "num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes"
        }
      ],
      "env": {
        "num_bytes": 20
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 607,
          "function_name": "tokenization_examples",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 568,
          "function_name": "get_compression_ratio",
          "code": "num_tokens = len(indices)                       # @inspect num_tokens"
        }
      ],
      "env": {
        "num_tokens": 12
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 607,
          "function_name": "tokenization_examples",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 569,
          "function_name": "get_compression_ratio",
          "code": "return num_bytes / num_tokens"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 607,
          "function_name": "tokenization_examples",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        }
      ],
      "env": {
        "compression_ratio": 1.6666666666666667
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 491,
          "function_name": "tokenization",
          "code": "tokenization_examples()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 610,
          "function_name": "character_tokenizer",
          "code": "def character_tokenizer():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 611,
          "function_name": "character_tokenizer",
          "code": "text(\"## Character-based tokenization\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Character-based tokenization",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 613,
          "function_name": "character_tokenizer",
          "code": "text(\"A Unicode string is a sequence of Unicode characters.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "A Unicode string is a sequence of Unicode characters.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 614,
          "function_name": "character_tokenizer",
          "code": "text(\"Each character can be converted into a code point (integer) via `ord`.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Each character can be converted into a code point (integer) via `ord`.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 615,
          "function_name": "character_tokenizer",
          "code": "assert ord(\"a\") == 97"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 616,
          "function_name": "character_tokenizer",
          "code": "assert ord(\"\ud83c\udf0d\") == 127757"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 617,
          "function_name": "character_tokenizer",
          "code": "text(\"It can be converted back via `chr`.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "It can be converted back via `chr`.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 618,
          "function_name": "character_tokenizer",
          "code": "assert chr(97) == \"a\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 619,
          "function_name": "character_tokenizer",
          "code": "assert chr(127757) == \"\ud83c\udf0d\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 621,
          "function_name": "character_tokenizer",
          "code": "text(\"Now let's build a `Tokenizer` and make sure it round-trips:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now let's build a `Tokenizer` and make sure it round-trips:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 622,
          "function_name": "character_tokenizer",
          "code": "tokenizer = CharacterTokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 623,
          "function_name": "character_tokenizer",
          "code": "string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\"  # @inspect string"
        }
      ],
      "env": {
        "string": "Hello, \ud83c\udf0d! \u4f60\u597d!"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 624,
          "function_name": "character_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 624,
          "function_name": "character_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 513,
          "function_name": "encode",
          "code": "def encode(self, string: str) -> list[int]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 624,
          "function_name": "character_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 514,
          "function_name": "encode",
          "code": "return list(map(ord, string))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 624,
          "function_name": "character_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          72,
          101,
          108,
          108,
          111,
          44,
          32,
          127757,
          33,
          32,
          20320,
          22909,
          33
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 625,
          "function_name": "character_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 625,
          "function_name": "character_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 516,
          "function_name": "decode",
          "code": "def decode(self, indices: list[int]) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 625,
          "function_name": "character_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 517,
          "function_name": "decode",
          "code": "return \"\".join(map(chr, indices))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 625,
          "function_name": "character_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        }
      ],
      "env": {
        "reconstructed_string": "Hello, \ud83c\udf0d! \u4f60\u597d!"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 626,
          "function_name": "character_tokenizer",
          "code": "assert string == reconstructed_string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 628,
          "function_name": "character_tokenizer",
          "code": "text(\"There are approximately 150K Unicode characters. \"), link(title=\"[Wikipedia]\", url=\"https://en.wikipedia.org/wiki/List_of_Unicode_characters\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "There are approximately 150K Unicode characters. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Wikipedia]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/List_of_Unicode_characters",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 629,
          "function_name": "character_tokenizer",
          "code": "vocabulary_size = max(indices) + 1  # This is a lower bound @inspect vocabulary_size"
        }
      ],
      "env": {
        "vocabulary_size": 127758
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 630,
          "function_name": "character_tokenizer",
          "code": "text(\"Problem 1: this is a very large vocabulary.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem 1: this is a very large vocabulary.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 631,
          "function_name": "character_tokenizer",
          "code": "text(\"Problem 2: many characters are quite rare (e.g., \ud83c\udf0d), which is inefficient use of the vocabulary.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem 2: many characters are quite rare (e.g., \ud83c\udf0d), which is inefficient use of the vocabulary.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 632,
          "function_name": "character_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 632,
          "function_name": "character_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 565,
          "function_name": "get_compression_ratio",
          "code": "def get_compression_ratio(string: str, indices: list[int]) -> float:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 632,
          "function_name": "character_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 567,
          "function_name": "get_compression_ratio",
          "code": "num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes"
        }
      ],
      "env": {
        "num_bytes": 20
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 632,
          "function_name": "character_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 568,
          "function_name": "get_compression_ratio",
          "code": "num_tokens = len(indices)                       # @inspect num_tokens"
        }
      ],
      "env": {
        "num_tokens": 13
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 632,
          "function_name": "character_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 569,
          "function_name": "get_compression_ratio",
          "code": "return num_bytes / num_tokens"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 632,
          "function_name": "character_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        }
      ],
      "env": {
        "compression_ratio": 1.5384615384615385
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 492,
          "function_name": "tokenization",
          "code": "character_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 635,
          "function_name": "byte_tokenizer",
          "code": "def byte_tokenizer():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 636,
          "function_name": "byte_tokenizer",
          "code": "text(\"## Byte-based tokenization\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Byte-based tokenization",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 638,
          "function_name": "byte_tokenizer",
          "code": "text(\"Unicode strings can be represented as a sequence of bytes, which can be represented by integers between 0 and 255.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Unicode strings can be represented as a sequence of bytes, which can be represented by integers between 0 and 255.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 639,
          "function_name": "byte_tokenizer",
          "code": "text(\"The most common Unicode encoding is \"), link(title=\"UTF-8\", url=\"https://en.wikipedia.org/wiki/UTF-8\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The most common Unicode encoding is ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "UTF-8",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/UTF-8",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 641,
          "function_name": "byte_tokenizer",
          "code": "text(\"Some Unicode characters are represented by one byte:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Some Unicode characters are represented by one byte:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 642,
          "function_name": "byte_tokenizer",
          "code": "assert bytes(\"a\", encoding=\"utf-8\") == b\"a\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 643,
          "function_name": "byte_tokenizer",
          "code": "text(\"Others take multiple bytes:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Others take multiple bytes:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 644,
          "function_name": "byte_tokenizer",
          "code": "assert bytes(\"\ud83c\udf0d\", encoding=\"utf-8\") == b\"\\xf0\\x9f\\x8c\\x8d\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 646,
          "function_name": "byte_tokenizer",
          "code": "text(\"Now let's build a `Tokenizer` and make sure it round-trips:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now let's build a `Tokenizer` and make sure it round-trips:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 647,
          "function_name": "byte_tokenizer",
          "code": "tokenizer = ByteTokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 648,
          "function_name": "byte_tokenizer",
          "code": "string = \"Hello, \ud83c\udf0d! \u4f60\u597d!\"  # @inspect string"
        }
      ],
      "env": {
        "string": "Hello, \ud83c\udf0d! \u4f60\u597d!"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 649,
          "function_name": "byte_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 649,
          "function_name": "byte_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 522,
          "function_name": "encode",
          "code": "def encode(self, string: str) -> list[int]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 649,
          "function_name": "byte_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 523,
          "function_name": "encode",
          "code": "string_bytes = string.encode(\"utf-8\")  # @inspect string_bytes"
        }
      ],
      "env": {
        "string_bytes": "b'Hello, \\xf0\\x9f\\x8c\\x8d! \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd!'"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 649,
          "function_name": "byte_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 524,
          "function_name": "encode",
          "code": "indices = list(map(int, string_bytes))  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          72,
          101,
          108,
          108,
          111,
          44,
          32,
          240,
          159,
          140,
          141,
          33,
          32,
          228,
          189,
          160,
          229,
          165,
          189,
          33
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 649,
          "function_name": "byte_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 525,
          "function_name": "encode",
          "code": "return indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 649,
          "function_name": "byte_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          72,
          101,
          108,
          108,
          111,
          44,
          32,
          240,
          159,
          140,
          141,
          33,
          32,
          228,
          189,
          160,
          229,
          165,
          189,
          33
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 650,
          "function_name": "byte_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 650,
          "function_name": "byte_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 527,
          "function_name": "decode",
          "code": "def decode(self, indices: list[int]) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 650,
          "function_name": "byte_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 528,
          "function_name": "decode",
          "code": "string_bytes = bytes(indices)  # @inspect string_bytes"
        }
      ],
      "env": {
        "string_bytes": "b'Hello, \\xf0\\x9f\\x8c\\x8d! \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd!'"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 650,
          "function_name": "byte_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 529,
          "function_name": "decode",
          "code": "string = string_bytes.decode(\"utf-8\")  # @inspect string"
        }
      ],
      "env": {
        "string": "Hello, \ud83c\udf0d! \u4f60\u597d!"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 650,
          "function_name": "byte_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 530,
          "function_name": "decode",
          "code": "return string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 650,
          "function_name": "byte_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        }
      ],
      "env": {
        "reconstructed_string": "Hello, \ud83c\udf0d! \u4f60\u597d!"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 651,
          "function_name": "byte_tokenizer",
          "code": "assert string == reconstructed_string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 653,
          "function_name": "byte_tokenizer",
          "code": "text(\"The vocabulary is nice and small: a byte can represent 256 values.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The vocabulary is nice and small: a byte can represent 256 values.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 654,
          "function_name": "byte_tokenizer",
          "code": "vocabulary_size = 256  # @inspect vocabulary_size"
        }
      ],
      "env": {
        "vocabulary_size": 256
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 655,
          "function_name": "byte_tokenizer",
          "code": "text(\"What about the compression rate?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What about the compression rate?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 656,
          "function_name": "byte_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 656,
          "function_name": "byte_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 565,
          "function_name": "get_compression_ratio",
          "code": "def get_compression_ratio(string: str, indices: list[int]) -> float:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 656,
          "function_name": "byte_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 567,
          "function_name": "get_compression_ratio",
          "code": "num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes"
        }
      ],
      "env": {
        "num_bytes": 20
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 656,
          "function_name": "byte_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 568,
          "function_name": "get_compression_ratio",
          "code": "num_tokens = len(indices)                       # @inspect num_tokens"
        }
      ],
      "env": {
        "num_tokens": 20
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 656,
          "function_name": "byte_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 569,
          "function_name": "get_compression_ratio",
          "code": "return num_bytes / num_tokens"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 656,
          "function_name": "byte_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio"
        }
      ],
      "env": {
        "compression_ratio": 1.0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 657,
          "function_name": "byte_tokenizer",
          "code": "assert compression_ratio == 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 658,
          "function_name": "byte_tokenizer",
          "code": "text(\"The compression ratio is terrible, which means the sequences will be too long.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The compression ratio is terrible, which means the sequences will be too long.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 659,
          "function_name": "byte_tokenizer",
          "code": "text(\"Given that the context length of a Transformer is limited (since attention is quadratic), this is not looking great...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Given that the context length of a Transformer is limited (since attention is quadratic), this is not looking great...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 493,
          "function_name": "tokenization",
          "code": "byte_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 662,
          "function_name": "word_tokenizer",
          "code": "def word_tokenizer():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 663,
          "function_name": "word_tokenizer",
          "code": "text(\"## Word-based tokenization\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Word-based tokenization",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 665,
          "function_name": "word_tokenizer",
          "code": "text(\"Another approach (closer to what was done classically in NLP) is to split strings into words.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Another approach (closer to what was done classically in NLP) is to split strings into words.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 666,
          "function_name": "word_tokenizer",
          "code": "string = \"I'll say supercalifragilisticexpialidocious!\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 668,
          "function_name": "word_tokenizer",
          "code": "segments = regex.findall(r\"\\w+|.\", string)  # @inspect segments"
        }
      ],
      "env": {
        "segments": [
          "I",
          "'",
          "ll",
          " ",
          "say",
          " ",
          "supercalifragilisticexpialidocious",
          "!"
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 669,
          "function_name": "word_tokenizer",
          "code": "text(\"This regular expression keeps all alphanumeric characters together (words).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This regular expression keeps all alphanumeric characters together (words).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 671,
          "function_name": "word_tokenizer",
          "code": "text(\"Here is a fancier version:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Here is a fancier version:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 672,
          "function_name": "word_tokenizer",
          "code": "pattern = GPT2_TOKENIZER_REGEX  # @inspect pattern"
        }
      ],
      "env": {
        "pattern": "'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 673,
          "function_name": "word_tokenizer",
          "code": "segments = regex.findall(pattern, string)  # @inspect segments"
        }
      ],
      "env": {
        "segments": [
          "I",
          "'ll",
          " say",
          " supercalifragilisticexpialidocious",
          "!"
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 675,
          "function_name": "word_tokenizer",
          "code": "text(\"To turn this into a `Tokenizer`, we need to map these segments into integers.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "To turn this into a `Tokenizer`, we need to map these segments into integers.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 676,
          "function_name": "word_tokenizer",
          "code": "text(\"Then, we can build a mapping from each segment into an integer.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Then, we can build a mapping from each segment into an integer.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 678,
          "function_name": "word_tokenizer",
          "code": "text(\"But there are problems:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But there are problems:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 679,
          "function_name": "word_tokenizer",
          "code": "text(\"- The number of words is huge (like for Unicode characters).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- The number of words is huge (like for Unicode characters).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 680,
          "function_name": "word_tokenizer",
          "code": "text(\"- Many words are rare and the model won't learn much about them.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Many words are rare and the model won't learn much about them.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 681,
          "function_name": "word_tokenizer",
          "code": "text(\"- This doesn't obviously provide a fixed vocabulary size.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- This doesn't obviously provide a fixed vocabulary size.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 683,
          "function_name": "word_tokenizer",
          "code": "text(\"New words we haven't seen during training get a special UNK token, which is ugly and can mess up perplexity calculations.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "New words we haven't seen during training get a special UNK token, which is ugly and can mess up perplexity calculations.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 685,
          "function_name": "word_tokenizer",
          "code": "vocabulary_size = \"Number of distinct segments in the training data\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 686,
          "function_name": "word_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 686,
          "function_name": "word_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 565,
          "function_name": "get_compression_ratio",
          "code": "def get_compression_ratio(string: str, indices: list[int]) -> float:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 686,
          "function_name": "word_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 567,
          "function_name": "get_compression_ratio",
          "code": "num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes"
        }
      ],
      "env": {
        "num_bytes": 44
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 686,
          "function_name": "word_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 568,
          "function_name": "get_compression_ratio",
          "code": "num_tokens = len(indices)                       # @inspect num_tokens"
        }
      ],
      "env": {
        "num_tokens": 5
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 686,
          "function_name": "word_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio"
        },
        {
          "path": "lecture_01.py",
          "line_number": 569,
          "function_name": "get_compression_ratio",
          "code": "return num_bytes / num_tokens"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 686,
          "function_name": "word_tokenizer",
          "code": "compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio"
        }
      ],
      "env": {
        "compression_ratio": 8.8
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 494,
          "function_name": "tokenization",
          "code": "word_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 689,
          "function_name": "bpe_tokenizer",
          "code": "def bpe_tokenizer():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 690,
          "function_name": "bpe_tokenizer",
          "code": "text(\"## Byte Pair Encoding (BPE)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Byte Pair Encoding (BPE)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 691,
          "function_name": "bpe_tokenizer",
          "code": "link(title=\"[Wikipedia]\", url=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Wikipedia]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Byte_pair_encoding",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 692,
          "function_name": "bpe_tokenizer",
          "code": "text(\"The BPE algorithm was introduced by Philip Gage in 1994 for data compression. \"), article_link(\"http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The BPE algorithm was introduced by Philip Gage in 1994 for data compression. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 693,
          "function_name": "bpe_tokenizer",
          "code": "text(\"It was adapted to NLP for neural machine translation. \"), link(sennrich_2016)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "It was adapted to NLP for neural machine translation. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Neural Machine Translation of Rare Words with Subword Units",
            "authors": [
              "Rico Sennrich",
              "Barry Haddow",
              "Alexandra Birch"
            ],
            "organization": null,
            "date": "2015-08-31T16:37:31Z",
            "url": "https://arxiv.org/abs/1508.07909",
            "description": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 694,
          "function_name": "bpe_tokenizer",
          "code": "text(\"(Previously, papers had been using word-based tokenization.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "(Previously, papers had been using word-based tokenization.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 695,
          "function_name": "bpe_tokenizer",
          "code": "text(\"BPE was then used by GPT-2. \"), link(gpt2)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "BPE was then used by GPT-2. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Language Models are Unsupervised Multitask Learners",
            "authors": [
              "Alec Radford",
              "Jeffrey Wu",
              "Rewon Child",
              "David Luan",
              "Dario Amodei",
              "Ilya Sutskever"
            ],
            "organization": "OpenAI",
            "date": "2019-02-14",
            "url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
            "description": null,
            "notes": "1.5B parameters\nPioneered stage release"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 697,
          "function_name": "bpe_tokenizer",
          "code": "text(\"Basic idea: *train* the tokenizer on raw text to automatically determine the vocabulary.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Basic idea: *train* the tokenizer on raw text to automatically determine the vocabulary.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 698,
          "function_name": "bpe_tokenizer",
          "code": "text(\"Intuition: common sequences of characters are represented by a single token, rare sequences are represented by many tokens.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuition: common sequences of characters are represented by a single token, rare sequences are represented by many tokens.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 700,
          "function_name": "bpe_tokenizer",
          "code": "text(\"The GPT-2 paper used word-based tokenization to break up the text into inital segments and run the original BPE algorithm on each segment.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The GPT-2 paper used word-based tokenization to break up the text into inital segments and run the original BPE algorithm on each segment.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 701,
          "function_name": "bpe_tokenizer",
          "code": "text(\"Sketch: start with each byte as a token, and successively merge the most common pair of adjacent tokens.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sketch: start with each byte as a token, and successively merge the most common pair of adjacent tokens.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 703,
          "function_name": "bpe_tokenizer",
          "code": "text(\"## Training the tokenizer\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Training the tokenizer",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 704,
          "function_name": "bpe_tokenizer",
          "code": "string = \"the cat in the hat\"  # @inspect string"
        }
      ],
      "env": {
        "string": "the cat in the hat"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 722,
          "function_name": "train_bpe",
          "code": "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:  # @inspect string, @inspect num_merges"
        }
      ],
      "env": {
        "string": "the cat in the hat",
        "num_merges": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 723,
          "function_name": "train_bpe",
          "code": "text(\"Start with the list of bytes of `string`.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Start with the list of bytes of `string`.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 724,
          "function_name": "train_bpe",
          "code": "indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          116,
          104,
          101,
          32,
          99,
          97,
          116,
          32,
          105,
          110,
          32,
          116,
          104,
          101,
          32,
          104,
          97,
          116
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 725,
          "function_name": "train_bpe",
          "code": "merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 726,
          "function_name": "train_bpe",
          "code": "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 726,
          "function_name": "train_bpe",
          "code": "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes"
        },
        {
          "path": "lecture_01.py",
          "line_number": 726,
          "function_name": "<dictcomp>",
          "code": "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 726,
          "function_name": "train_bpe",
          "code": "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 728,
          "function_name": "train_bpe",
          "code": "for i in range(num_merges):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 729,
          "function_name": "train_bpe",
          "code": "text(\"Count the number of occurrences of each pair of tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Count the number of occurrences of each pair of tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 730,
          "function_name": "train_bpe",
          "code": "counts = defaultdict(int)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 1,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 116)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 2,
          "(104, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 116)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 2,
          "(104, 101)": 2,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 116)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 2,
          "(104, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 116)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 2,
          "(104, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 116)": 1,
          "(32, 104)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 2,
          "(104, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 116)": 1,
          "(32, 104)": 1,
          "(104, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(116, 104)": 2,
          "(104, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 2,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 116)": 1,
          "(32, 104)": 1,
          "(104, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 734,
          "function_name": "train_bpe",
          "code": "text(\"Find the most common pair.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Find the most common pair.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 735,
          "function_name": "train_bpe",
          "code": "pair = max(counts, key=counts.get)  # @inspect pair"
        }
      ],
      "env": {
        "pair": "(116, 104)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 736,
          "function_name": "train_bpe",
          "code": "index1, index2 = pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 738,
          "function_name": "train_bpe",
          "code": "text(\"Merge that pair.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Merge that pair.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 739,
          "function_name": "train_bpe",
          "code": "new_index = 256 + i  # @inspect new_index"
        }
      ],
      "env": {
        "new_index": 256
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 740,
          "function_name": "train_bpe",
          "code": "merges[pair] = new_index  # @inspect merges"
        }
      ],
      "env": {
        "merges": {
          "(116, 104)": 256
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 741,
          "function_name": "train_bpe",
          "code": "vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab"
        }
      ],
      "env": {
        "vocab": {
          "0": "b'\\x00'",
          "1": "b'\\x01'",
          "2": "b'\\x02'",
          "3": "b'\\x03'",
          "4": "b'\\x04'",
          "5": "b'\\x05'",
          "6": "b'\\x06'",
          "7": "b'\\x07'",
          "8": "b'\\x08'",
          "9": "b'\\t'",
          "10": "b'\\n'",
          "11": "b'\\x0b'",
          "12": "b'\\x0c'",
          "13": "b'\\r'",
          "14": "b'\\x0e'",
          "15": "b'\\x0f'",
          "16": "b'\\x10'",
          "17": "b'\\x11'",
          "18": "b'\\x12'",
          "19": "b'\\x13'",
          "20": "b'\\x14'",
          "21": "b'\\x15'",
          "22": "b'\\x16'",
          "23": "b'\\x17'",
          "24": "b'\\x18'",
          "25": "b'\\x19'",
          "26": "b'\\x1a'",
          "27": "b'\\x1b'",
          "28": "b'\\x1c'",
          "29": "b'\\x1d'",
          "30": "b'\\x1e'",
          "31": "b'\\x1f'",
          "32": "b' '",
          "33": "b'!'",
          "34": "b'\"'",
          "35": "b'#'",
          "36": "b'$'",
          "37": "b'%'",
          "38": "b'&'",
          "39": "b\"'\"",
          "40": "b'('",
          "41": "b')'",
          "42": "b'*'",
          "43": "b'+'",
          "44": "b','",
          "45": "b'-'",
          "46": "b'.'",
          "47": "b'/'",
          "48": "b'0'",
          "49": "b'1'",
          "50": "b'2'",
          "51": "b'3'",
          "52": "b'4'",
          "53": "b'5'",
          "54": "b'6'",
          "55": "b'7'",
          "56": "b'8'",
          "57": "b'9'",
          "58": "b':'",
          "59": "b';'",
          "60": "b'<'",
          "61": "b'='",
          "62": "b'>'",
          "63": "b'?'",
          "64": "b'@'",
          "65": "b'A'",
          "66": "b'B'",
          "67": "b'C'",
          "68": "b'D'",
          "69": "b'E'",
          "70": "b'F'",
          "71": "b'G'",
          "72": "b'H'",
          "73": "b'I'",
          "74": "b'J'",
          "75": "b'K'",
          "76": "b'L'",
          "77": "b'M'",
          "78": "b'N'",
          "79": "b'O'",
          "80": "b'P'",
          "81": "b'Q'",
          "82": "b'R'",
          "83": "b'S'",
          "84": "b'T'",
          "85": "b'U'",
          "86": "b'V'",
          "87": "b'W'",
          "88": "b'X'",
          "89": "b'Y'",
          "90": "b'Z'",
          "91": "b'['",
          "92": "b'\\\\'",
          "93": "b']'",
          "94": "b'^'",
          "95": "b'_'",
          "96": "b'`'",
          "97": "b'a'",
          "98": "b'b'",
          "99": "b'c'",
          "100": "b'd'",
          "101": "b'e'",
          "102": "b'f'",
          "103": "b'g'",
          "104": "b'h'",
          "105": "b'i'",
          "106": "b'j'",
          "107": "b'k'",
          "108": "b'l'",
          "109": "b'm'",
          "110": "b'n'",
          "111": "b'o'",
          "112": "b'p'",
          "113": "b'q'",
          "114": "b'r'",
          "115": "b's'",
          "116": "b't'",
          "117": "b'u'",
          "118": "b'v'",
          "119": "b'w'",
          "120": "b'x'",
          "121": "b'y'",
          "122": "b'z'",
          "123": "b'{'",
          "124": "b'|'",
          "125": "b'}'",
          "126": "b'~'",
          "127": "b'\\x7f'",
          "128": "b'\\x80'",
          "129": "b'\\x81'",
          "130": "b'\\x82'",
          "131": "b'\\x83'",
          "132": "b'\\x84'",
          "133": "b'\\x85'",
          "134": "b'\\x86'",
          "135": "b'\\x87'",
          "136": "b'\\x88'",
          "137": "b'\\x89'",
          "138": "b'\\x8a'",
          "139": "b'\\x8b'",
          "140": "b'\\x8c'",
          "141": "b'\\x8d'",
          "142": "b'\\x8e'",
          "143": "b'\\x8f'",
          "144": "b'\\x90'",
          "145": "b'\\x91'",
          "146": "b'\\x92'",
          "147": "b'\\x93'",
          "148": "b'\\x94'",
          "149": "b'\\x95'",
          "150": "b'\\x96'",
          "151": "b'\\x97'",
          "152": "b'\\x98'",
          "153": "b'\\x99'",
          "154": "b'\\x9a'",
          "155": "b'\\x9b'",
          "156": "b'\\x9c'",
          "157": "b'\\x9d'",
          "158": "b'\\x9e'",
          "159": "b'\\x9f'",
          "160": "b'\\xa0'",
          "161": "b'\\xa1'",
          "162": "b'\\xa2'",
          "163": "b'\\xa3'",
          "164": "b'\\xa4'",
          "165": "b'\\xa5'",
          "166": "b'\\xa6'",
          "167": "b'\\xa7'",
          "168": "b'\\xa8'",
          "169": "b'\\xa9'",
          "170": "b'\\xaa'",
          "171": "b'\\xab'",
          "172": "b'\\xac'",
          "173": "b'\\xad'",
          "174": "b'\\xae'",
          "175": "b'\\xaf'",
          "176": "b'\\xb0'",
          "177": "b'\\xb1'",
          "178": "b'\\xb2'",
          "179": "b'\\xb3'",
          "180": "b'\\xb4'",
          "181": "b'\\xb5'",
          "182": "b'\\xb6'",
          "183": "b'\\xb7'",
          "184": "b'\\xb8'",
          "185": "b'\\xb9'",
          "186": "b'\\xba'",
          "187": "b'\\xbb'",
          "188": "b'\\xbc'",
          "189": "b'\\xbd'",
          "190": "b'\\xbe'",
          "191": "b'\\xbf'",
          "192": "b'\\xc0'",
          "193": "b'\\xc1'",
          "194": "b'\\xc2'",
          "195": "b'\\xc3'",
          "196": "b'\\xc4'",
          "197": "b'\\xc5'",
          "198": "b'\\xc6'",
          "199": "b'\\xc7'",
          "200": "b'\\xc8'",
          "201": "b'\\xc9'",
          "202": "b'\\xca'",
          "203": "b'\\xcb'",
          "204": "b'\\xcc'",
          "205": "b'\\xcd'",
          "206": "b'\\xce'",
          "207": "b'\\xcf'",
          "208": "b'\\xd0'",
          "209": "b'\\xd1'",
          "210": "b'\\xd2'",
          "211": "b'\\xd3'",
          "212": "b'\\xd4'",
          "213": "b'\\xd5'",
          "214": "b'\\xd6'",
          "215": "b'\\xd7'",
          "216": "b'\\xd8'",
          "217": "b'\\xd9'",
          "218": "b'\\xda'",
          "219": "b'\\xdb'",
          "220": "b'\\xdc'",
          "221": "b'\\xdd'",
          "222": "b'\\xde'",
          "223": "b'\\xdf'",
          "224": "b'\\xe0'",
          "225": "b'\\xe1'",
          "226": "b'\\xe2'",
          "227": "b'\\xe3'",
          "228": "b'\\xe4'",
          "229": "b'\\xe5'",
          "230": "b'\\xe6'",
          "231": "b'\\xe7'",
          "232": "b'\\xe8'",
          "233": "b'\\xe9'",
          "234": "b'\\xea'",
          "235": "b'\\xeb'",
          "236": "b'\\xec'",
          "237": "b'\\xed'",
          "238": "b'\\xee'",
          "239": "b'\\xef'",
          "240": "b'\\xf0'",
          "241": "b'\\xf1'",
          "242": "b'\\xf2'",
          "243": "b'\\xf3'",
          "244": "b'\\xf4'",
          "245": "b'\\xf5'",
          "246": "b'\\xf6'",
          "247": "b'\\xf7'",
          "248": "b'\\xf8'",
          "249": "b'\\xf9'",
          "250": "b'\\xfa'",
          "251": "b'\\xfb'",
          "252": "b'\\xfc'",
          "253": "b'\\xfd'",
          "254": "b'\\xfe'",
          "255": "b'\\xff'",
          "256": "b'th'"
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 533,
          "function_name": "merge",
          "code": "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "indices": [
          116,
          104,
          101,
          32,
          99,
          97,
          116,
          32,
          105,
          110,
          32,
          116,
          104,
          101,
          32,
          104,
          97,
          116
        ],
        "pair": "(116, 104)",
        "new_index": 256
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 535,
          "function_name": "merge",
          "code": "new_indices = []  # @inspect new_indices"
        }
      ],
      "env": {
        "new_indices": []
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 536,
          "function_name": "merge",
          "code": "i = 0  # @inspect i"
        }
      ],
      "env": {
        "i": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 544,
          "function_name": "merge",
          "code": "return new_indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          256,
          101,
          32,
          99,
          97,
          116,
          32,
          105,
          110,
          32,
          256,
          101,
          32,
          104,
          97,
          116
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 728,
          "function_name": "train_bpe",
          "code": "for i in range(num_merges):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 729,
          "function_name": "train_bpe",
          "code": "text(\"Count the number of occurrences of each pair of tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Count the number of occurrences of each pair of tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 730,
          "function_name": "train_bpe",
          "code": "counts = defaultdict(int)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 1,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 256)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 2,
          "(101, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 256)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 256)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 256)": 1,
          "(32, 104)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 256)": 1,
          "(32, 104)": 1,
          "(104, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(256, 101)": 2,
          "(101, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 2,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 256)": 1,
          "(32, 104)": 1,
          "(104, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 734,
          "function_name": "train_bpe",
          "code": "text(\"Find the most common pair.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Find the most common pair.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 735,
          "function_name": "train_bpe",
          "code": "pair = max(counts, key=counts.get)  # @inspect pair"
        }
      ],
      "env": {
        "pair": "(256, 101)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 736,
          "function_name": "train_bpe",
          "code": "index1, index2 = pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 738,
          "function_name": "train_bpe",
          "code": "text(\"Merge that pair.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Merge that pair.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 739,
          "function_name": "train_bpe",
          "code": "new_index = 256 + i  # @inspect new_index"
        }
      ],
      "env": {
        "new_index": 257
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 740,
          "function_name": "train_bpe",
          "code": "merges[pair] = new_index  # @inspect merges"
        }
      ],
      "env": {
        "merges": {
          "(116, 104)": 256,
          "(256, 101)": 257
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 741,
          "function_name": "train_bpe",
          "code": "vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab"
        }
      ],
      "env": {
        "vocab": {
          "0": "b'\\x00'",
          "1": "b'\\x01'",
          "2": "b'\\x02'",
          "3": "b'\\x03'",
          "4": "b'\\x04'",
          "5": "b'\\x05'",
          "6": "b'\\x06'",
          "7": "b'\\x07'",
          "8": "b'\\x08'",
          "9": "b'\\t'",
          "10": "b'\\n'",
          "11": "b'\\x0b'",
          "12": "b'\\x0c'",
          "13": "b'\\r'",
          "14": "b'\\x0e'",
          "15": "b'\\x0f'",
          "16": "b'\\x10'",
          "17": "b'\\x11'",
          "18": "b'\\x12'",
          "19": "b'\\x13'",
          "20": "b'\\x14'",
          "21": "b'\\x15'",
          "22": "b'\\x16'",
          "23": "b'\\x17'",
          "24": "b'\\x18'",
          "25": "b'\\x19'",
          "26": "b'\\x1a'",
          "27": "b'\\x1b'",
          "28": "b'\\x1c'",
          "29": "b'\\x1d'",
          "30": "b'\\x1e'",
          "31": "b'\\x1f'",
          "32": "b' '",
          "33": "b'!'",
          "34": "b'\"'",
          "35": "b'#'",
          "36": "b'$'",
          "37": "b'%'",
          "38": "b'&'",
          "39": "b\"'\"",
          "40": "b'('",
          "41": "b')'",
          "42": "b'*'",
          "43": "b'+'",
          "44": "b','",
          "45": "b'-'",
          "46": "b'.'",
          "47": "b'/'",
          "48": "b'0'",
          "49": "b'1'",
          "50": "b'2'",
          "51": "b'3'",
          "52": "b'4'",
          "53": "b'5'",
          "54": "b'6'",
          "55": "b'7'",
          "56": "b'8'",
          "57": "b'9'",
          "58": "b':'",
          "59": "b';'",
          "60": "b'<'",
          "61": "b'='",
          "62": "b'>'",
          "63": "b'?'",
          "64": "b'@'",
          "65": "b'A'",
          "66": "b'B'",
          "67": "b'C'",
          "68": "b'D'",
          "69": "b'E'",
          "70": "b'F'",
          "71": "b'G'",
          "72": "b'H'",
          "73": "b'I'",
          "74": "b'J'",
          "75": "b'K'",
          "76": "b'L'",
          "77": "b'M'",
          "78": "b'N'",
          "79": "b'O'",
          "80": "b'P'",
          "81": "b'Q'",
          "82": "b'R'",
          "83": "b'S'",
          "84": "b'T'",
          "85": "b'U'",
          "86": "b'V'",
          "87": "b'W'",
          "88": "b'X'",
          "89": "b'Y'",
          "90": "b'Z'",
          "91": "b'['",
          "92": "b'\\\\'",
          "93": "b']'",
          "94": "b'^'",
          "95": "b'_'",
          "96": "b'`'",
          "97": "b'a'",
          "98": "b'b'",
          "99": "b'c'",
          "100": "b'd'",
          "101": "b'e'",
          "102": "b'f'",
          "103": "b'g'",
          "104": "b'h'",
          "105": "b'i'",
          "106": "b'j'",
          "107": "b'k'",
          "108": "b'l'",
          "109": "b'm'",
          "110": "b'n'",
          "111": "b'o'",
          "112": "b'p'",
          "113": "b'q'",
          "114": "b'r'",
          "115": "b's'",
          "116": "b't'",
          "117": "b'u'",
          "118": "b'v'",
          "119": "b'w'",
          "120": "b'x'",
          "121": "b'y'",
          "122": "b'z'",
          "123": "b'{'",
          "124": "b'|'",
          "125": "b'}'",
          "126": "b'~'",
          "127": "b'\\x7f'",
          "128": "b'\\x80'",
          "129": "b'\\x81'",
          "130": "b'\\x82'",
          "131": "b'\\x83'",
          "132": "b'\\x84'",
          "133": "b'\\x85'",
          "134": "b'\\x86'",
          "135": "b'\\x87'",
          "136": "b'\\x88'",
          "137": "b'\\x89'",
          "138": "b'\\x8a'",
          "139": "b'\\x8b'",
          "140": "b'\\x8c'",
          "141": "b'\\x8d'",
          "142": "b'\\x8e'",
          "143": "b'\\x8f'",
          "144": "b'\\x90'",
          "145": "b'\\x91'",
          "146": "b'\\x92'",
          "147": "b'\\x93'",
          "148": "b'\\x94'",
          "149": "b'\\x95'",
          "150": "b'\\x96'",
          "151": "b'\\x97'",
          "152": "b'\\x98'",
          "153": "b'\\x99'",
          "154": "b'\\x9a'",
          "155": "b'\\x9b'",
          "156": "b'\\x9c'",
          "157": "b'\\x9d'",
          "158": "b'\\x9e'",
          "159": "b'\\x9f'",
          "160": "b'\\xa0'",
          "161": "b'\\xa1'",
          "162": "b'\\xa2'",
          "163": "b'\\xa3'",
          "164": "b'\\xa4'",
          "165": "b'\\xa5'",
          "166": "b'\\xa6'",
          "167": "b'\\xa7'",
          "168": "b'\\xa8'",
          "169": "b'\\xa9'",
          "170": "b'\\xaa'",
          "171": "b'\\xab'",
          "172": "b'\\xac'",
          "173": "b'\\xad'",
          "174": "b'\\xae'",
          "175": "b'\\xaf'",
          "176": "b'\\xb0'",
          "177": "b'\\xb1'",
          "178": "b'\\xb2'",
          "179": "b'\\xb3'",
          "180": "b'\\xb4'",
          "181": "b'\\xb5'",
          "182": "b'\\xb6'",
          "183": "b'\\xb7'",
          "184": "b'\\xb8'",
          "185": "b'\\xb9'",
          "186": "b'\\xba'",
          "187": "b'\\xbb'",
          "188": "b'\\xbc'",
          "189": "b'\\xbd'",
          "190": "b'\\xbe'",
          "191": "b'\\xbf'",
          "192": "b'\\xc0'",
          "193": "b'\\xc1'",
          "194": "b'\\xc2'",
          "195": "b'\\xc3'",
          "196": "b'\\xc4'",
          "197": "b'\\xc5'",
          "198": "b'\\xc6'",
          "199": "b'\\xc7'",
          "200": "b'\\xc8'",
          "201": "b'\\xc9'",
          "202": "b'\\xca'",
          "203": "b'\\xcb'",
          "204": "b'\\xcc'",
          "205": "b'\\xcd'",
          "206": "b'\\xce'",
          "207": "b'\\xcf'",
          "208": "b'\\xd0'",
          "209": "b'\\xd1'",
          "210": "b'\\xd2'",
          "211": "b'\\xd3'",
          "212": "b'\\xd4'",
          "213": "b'\\xd5'",
          "214": "b'\\xd6'",
          "215": "b'\\xd7'",
          "216": "b'\\xd8'",
          "217": "b'\\xd9'",
          "218": "b'\\xda'",
          "219": "b'\\xdb'",
          "220": "b'\\xdc'",
          "221": "b'\\xdd'",
          "222": "b'\\xde'",
          "223": "b'\\xdf'",
          "224": "b'\\xe0'",
          "225": "b'\\xe1'",
          "226": "b'\\xe2'",
          "227": "b'\\xe3'",
          "228": "b'\\xe4'",
          "229": "b'\\xe5'",
          "230": "b'\\xe6'",
          "231": "b'\\xe7'",
          "232": "b'\\xe8'",
          "233": "b'\\xe9'",
          "234": "b'\\xea'",
          "235": "b'\\xeb'",
          "236": "b'\\xec'",
          "237": "b'\\xed'",
          "238": "b'\\xee'",
          "239": "b'\\xef'",
          "240": "b'\\xf0'",
          "241": "b'\\xf1'",
          "242": "b'\\xf2'",
          "243": "b'\\xf3'",
          "244": "b'\\xf4'",
          "245": "b'\\xf5'",
          "246": "b'\\xf6'",
          "247": "b'\\xf7'",
          "248": "b'\\xf8'",
          "249": "b'\\xf9'",
          "250": "b'\\xfa'",
          "251": "b'\\xfb'",
          "252": "b'\\xfc'",
          "253": "b'\\xfd'",
          "254": "b'\\xfe'",
          "255": "b'\\xff'",
          "256": "b'th'",
          "257": "b'the'"
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 533,
          "function_name": "merge",
          "code": "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "indices": [
          256,
          101,
          32,
          99,
          97,
          116,
          32,
          105,
          110,
          32,
          256,
          101,
          32,
          104,
          97,
          116
        ],
        "pair": "(256, 101)",
        "new_index": 257
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 535,
          "function_name": "merge",
          "code": "new_indices = []  # @inspect new_indices"
        }
      ],
      "env": {
        "new_indices": []
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 536,
          "function_name": "merge",
          "code": "i = 0  # @inspect i"
        }
      ],
      "env": {
        "i": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 544,
          "function_name": "merge",
          "code": "return new_indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          257,
          32,
          99,
          97,
          116,
          32,
          105,
          110,
          32,
          257,
          32,
          104,
          97,
          116
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 728,
          "function_name": "train_bpe",
          "code": "for i in range(num_merges):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 729,
          "function_name": "train_bpe",
          "code": "text(\"Count the number of occurrences of each pair of tokens\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Count the number of occurrences of each pair of tokens",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 730,
          "function_name": "train_bpe",
          "code": "counts = defaultdict(int)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 1,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 257)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 257)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 257)": 1,
          "(32, 104)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 1,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 257)": 1,
          "(32, 104)": 1,
          "(104, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 732,
          "function_name": "train_bpe",
          "code": "counts[(index1, index2)] += 1  # @inspect counts"
        }
      ],
      "env": {
        "counts": {
          "(257, 32)": 2,
          "(32, 99)": 1,
          "(99, 97)": 1,
          "(97, 116)": 2,
          "(116, 32)": 1,
          "(32, 105)": 1,
          "(105, 110)": 1,
          "(110, 32)": 1,
          "(32, 257)": 1,
          "(32, 104)": 1,
          "(104, 97)": 1
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 731,
          "function_name": "train_bpe",
          "code": "for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 734,
          "function_name": "train_bpe",
          "code": "text(\"Find the most common pair.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Find the most common pair.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 735,
          "function_name": "train_bpe",
          "code": "pair = max(counts, key=counts.get)  # @inspect pair"
        }
      ],
      "env": {
        "pair": "(257, 32)"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 736,
          "function_name": "train_bpe",
          "code": "index1, index2 = pair"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 738,
          "function_name": "train_bpe",
          "code": "text(\"Merge that pair.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Merge that pair.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 739,
          "function_name": "train_bpe",
          "code": "new_index = 256 + i  # @inspect new_index"
        }
      ],
      "env": {
        "new_index": 258
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 740,
          "function_name": "train_bpe",
          "code": "merges[pair] = new_index  # @inspect merges"
        }
      ],
      "env": {
        "merges": {
          "(116, 104)": 256,
          "(256, 101)": 257,
          "(257, 32)": 258
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 741,
          "function_name": "train_bpe",
          "code": "vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab"
        }
      ],
      "env": {
        "vocab": {
          "0": "b'\\x00'",
          "1": "b'\\x01'",
          "2": "b'\\x02'",
          "3": "b'\\x03'",
          "4": "b'\\x04'",
          "5": "b'\\x05'",
          "6": "b'\\x06'",
          "7": "b'\\x07'",
          "8": "b'\\x08'",
          "9": "b'\\t'",
          "10": "b'\\n'",
          "11": "b'\\x0b'",
          "12": "b'\\x0c'",
          "13": "b'\\r'",
          "14": "b'\\x0e'",
          "15": "b'\\x0f'",
          "16": "b'\\x10'",
          "17": "b'\\x11'",
          "18": "b'\\x12'",
          "19": "b'\\x13'",
          "20": "b'\\x14'",
          "21": "b'\\x15'",
          "22": "b'\\x16'",
          "23": "b'\\x17'",
          "24": "b'\\x18'",
          "25": "b'\\x19'",
          "26": "b'\\x1a'",
          "27": "b'\\x1b'",
          "28": "b'\\x1c'",
          "29": "b'\\x1d'",
          "30": "b'\\x1e'",
          "31": "b'\\x1f'",
          "32": "b' '",
          "33": "b'!'",
          "34": "b'\"'",
          "35": "b'#'",
          "36": "b'$'",
          "37": "b'%'",
          "38": "b'&'",
          "39": "b\"'\"",
          "40": "b'('",
          "41": "b')'",
          "42": "b'*'",
          "43": "b'+'",
          "44": "b','",
          "45": "b'-'",
          "46": "b'.'",
          "47": "b'/'",
          "48": "b'0'",
          "49": "b'1'",
          "50": "b'2'",
          "51": "b'3'",
          "52": "b'4'",
          "53": "b'5'",
          "54": "b'6'",
          "55": "b'7'",
          "56": "b'8'",
          "57": "b'9'",
          "58": "b':'",
          "59": "b';'",
          "60": "b'<'",
          "61": "b'='",
          "62": "b'>'",
          "63": "b'?'",
          "64": "b'@'",
          "65": "b'A'",
          "66": "b'B'",
          "67": "b'C'",
          "68": "b'D'",
          "69": "b'E'",
          "70": "b'F'",
          "71": "b'G'",
          "72": "b'H'",
          "73": "b'I'",
          "74": "b'J'",
          "75": "b'K'",
          "76": "b'L'",
          "77": "b'M'",
          "78": "b'N'",
          "79": "b'O'",
          "80": "b'P'",
          "81": "b'Q'",
          "82": "b'R'",
          "83": "b'S'",
          "84": "b'T'",
          "85": "b'U'",
          "86": "b'V'",
          "87": "b'W'",
          "88": "b'X'",
          "89": "b'Y'",
          "90": "b'Z'",
          "91": "b'['",
          "92": "b'\\\\'",
          "93": "b']'",
          "94": "b'^'",
          "95": "b'_'",
          "96": "b'`'",
          "97": "b'a'",
          "98": "b'b'",
          "99": "b'c'",
          "100": "b'd'",
          "101": "b'e'",
          "102": "b'f'",
          "103": "b'g'",
          "104": "b'h'",
          "105": "b'i'",
          "106": "b'j'",
          "107": "b'k'",
          "108": "b'l'",
          "109": "b'm'",
          "110": "b'n'",
          "111": "b'o'",
          "112": "b'p'",
          "113": "b'q'",
          "114": "b'r'",
          "115": "b's'",
          "116": "b't'",
          "117": "b'u'",
          "118": "b'v'",
          "119": "b'w'",
          "120": "b'x'",
          "121": "b'y'",
          "122": "b'z'",
          "123": "b'{'",
          "124": "b'|'",
          "125": "b'}'",
          "126": "b'~'",
          "127": "b'\\x7f'",
          "128": "b'\\x80'",
          "129": "b'\\x81'",
          "130": "b'\\x82'",
          "131": "b'\\x83'",
          "132": "b'\\x84'",
          "133": "b'\\x85'",
          "134": "b'\\x86'",
          "135": "b'\\x87'",
          "136": "b'\\x88'",
          "137": "b'\\x89'",
          "138": "b'\\x8a'",
          "139": "b'\\x8b'",
          "140": "b'\\x8c'",
          "141": "b'\\x8d'",
          "142": "b'\\x8e'",
          "143": "b'\\x8f'",
          "144": "b'\\x90'",
          "145": "b'\\x91'",
          "146": "b'\\x92'",
          "147": "b'\\x93'",
          "148": "b'\\x94'",
          "149": "b'\\x95'",
          "150": "b'\\x96'",
          "151": "b'\\x97'",
          "152": "b'\\x98'",
          "153": "b'\\x99'",
          "154": "b'\\x9a'",
          "155": "b'\\x9b'",
          "156": "b'\\x9c'",
          "157": "b'\\x9d'",
          "158": "b'\\x9e'",
          "159": "b'\\x9f'",
          "160": "b'\\xa0'",
          "161": "b'\\xa1'",
          "162": "b'\\xa2'",
          "163": "b'\\xa3'",
          "164": "b'\\xa4'",
          "165": "b'\\xa5'",
          "166": "b'\\xa6'",
          "167": "b'\\xa7'",
          "168": "b'\\xa8'",
          "169": "b'\\xa9'",
          "170": "b'\\xaa'",
          "171": "b'\\xab'",
          "172": "b'\\xac'",
          "173": "b'\\xad'",
          "174": "b'\\xae'",
          "175": "b'\\xaf'",
          "176": "b'\\xb0'",
          "177": "b'\\xb1'",
          "178": "b'\\xb2'",
          "179": "b'\\xb3'",
          "180": "b'\\xb4'",
          "181": "b'\\xb5'",
          "182": "b'\\xb6'",
          "183": "b'\\xb7'",
          "184": "b'\\xb8'",
          "185": "b'\\xb9'",
          "186": "b'\\xba'",
          "187": "b'\\xbb'",
          "188": "b'\\xbc'",
          "189": "b'\\xbd'",
          "190": "b'\\xbe'",
          "191": "b'\\xbf'",
          "192": "b'\\xc0'",
          "193": "b'\\xc1'",
          "194": "b'\\xc2'",
          "195": "b'\\xc3'",
          "196": "b'\\xc4'",
          "197": "b'\\xc5'",
          "198": "b'\\xc6'",
          "199": "b'\\xc7'",
          "200": "b'\\xc8'",
          "201": "b'\\xc9'",
          "202": "b'\\xca'",
          "203": "b'\\xcb'",
          "204": "b'\\xcc'",
          "205": "b'\\xcd'",
          "206": "b'\\xce'",
          "207": "b'\\xcf'",
          "208": "b'\\xd0'",
          "209": "b'\\xd1'",
          "210": "b'\\xd2'",
          "211": "b'\\xd3'",
          "212": "b'\\xd4'",
          "213": "b'\\xd5'",
          "214": "b'\\xd6'",
          "215": "b'\\xd7'",
          "216": "b'\\xd8'",
          "217": "b'\\xd9'",
          "218": "b'\\xda'",
          "219": "b'\\xdb'",
          "220": "b'\\xdc'",
          "221": "b'\\xdd'",
          "222": "b'\\xde'",
          "223": "b'\\xdf'",
          "224": "b'\\xe0'",
          "225": "b'\\xe1'",
          "226": "b'\\xe2'",
          "227": "b'\\xe3'",
          "228": "b'\\xe4'",
          "229": "b'\\xe5'",
          "230": "b'\\xe6'",
          "231": "b'\\xe7'",
          "232": "b'\\xe8'",
          "233": "b'\\xe9'",
          "234": "b'\\xea'",
          "235": "b'\\xeb'",
          "236": "b'\\xec'",
          "237": "b'\\xed'",
          "238": "b'\\xee'",
          "239": "b'\\xef'",
          "240": "b'\\xf0'",
          "241": "b'\\xf1'",
          "242": "b'\\xf2'",
          "243": "b'\\xf3'",
          "244": "b'\\xf4'",
          "245": "b'\\xf5'",
          "246": "b'\\xf6'",
          "247": "b'\\xf7'",
          "248": "b'\\xf8'",
          "249": "b'\\xf9'",
          "250": "b'\\xfa'",
          "251": "b'\\xfb'",
          "252": "b'\\xfc'",
          "253": "b'\\xfd'",
          "254": "b'\\xfe'",
          "255": "b'\\xff'",
          "256": "b'th'",
          "257": "b'the'",
          "258": "b'the '"
        }
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 533,
          "function_name": "merge",
          "code": "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "indices": [
          257,
          32,
          99,
          97,
          116,
          32,
          105,
          110,
          32,
          257,
          32,
          104,
          97,
          116
        ],
        "pair": "(257, 32)",
        "new_index": 258
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 535,
          "function_name": "merge",
          "code": "new_indices = []  # @inspect new_indices"
        }
      ],
      "env": {
        "new_indices": []
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 536,
          "function_name": "merge",
          "code": "i = 0  # @inspect i"
        }
      ],
      "env": {
        "i": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 544,
          "function_name": "merge",
          "code": "return new_indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 742,
          "function_name": "train_bpe",
          "code": "indices = merge(indices, pair, new_index)  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          258,
          99,
          97,
          116,
          32,
          105,
          110,
          32,
          258,
          104,
          97,
          116
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 728,
          "function_name": "train_bpe",
          "code": "for i in range(num_merges):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 744,
          "function_name": "train_bpe",
          "code": "return BPETokenizerParams(vocab=vocab, merges=merges)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 705,
          "function_name": "bpe_tokenizer",
          "code": "params = train_bpe(string, num_merges=3)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 707,
          "function_name": "bpe_tokenizer",
          "code": "text(\"## Using the tokenizer\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Using the tokenizer",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 708,
          "function_name": "bpe_tokenizer",
          "code": "text(\"Now, given a new text, we can encode it.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now, given a new text, we can encode it.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 709,
          "function_name": "bpe_tokenizer",
          "code": "tokenizer = BPETokenizer(params)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 709,
          "function_name": "bpe_tokenizer",
          "code": "tokenizer = BPETokenizer(params)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 549,
          "function_name": "__init__",
          "code": "def __init__(self, params: BPETokenizerParams):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 709,
          "function_name": "bpe_tokenizer",
          "code": "tokenizer = BPETokenizer(params)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 550,
          "function_name": "__init__",
          "code": "self.params = params"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 709,
          "function_name": "bpe_tokenizer",
          "code": "tokenizer = BPETokenizer(params)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 710,
          "function_name": "bpe_tokenizer",
          "code": "string = \"the quick brown fox\"  # @inspect string"
        }
      ],
      "env": {
        "string": "the quick brown fox"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 552,
          "function_name": "encode",
          "code": "def encode(self, string: str) -> list[int]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 553,
          "function_name": "encode",
          "code": "indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          116,
          104,
          101,
          32,
          113,
          117,
          105,
          99,
          107,
          32,
          98,
          114,
          111,
          119,
          110,
          32,
          102,
          111,
          120
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 555,
          "function_name": "encode",
          "code": "for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "pair": "(116, 104)",
        "new_index": 256
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 533,
          "function_name": "merge",
          "code": "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "indices": [
          116,
          104,
          101,
          32,
          113,
          117,
          105,
          99,
          107,
          32,
          98,
          114,
          111,
          119,
          110,
          32,
          102,
          111,
          120
        ],
        "pair": "(116, 104)",
        "new_index": 256
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 535,
          "function_name": "merge",
          "code": "new_indices = []  # @inspect new_indices"
        }
      ],
      "env": {
        "new_indices": []
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 536,
          "function_name": "merge",
          "code": "i = 0  # @inspect i"
        }
      ],
      "env": {
        "i": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 544,
          "function_name": "merge",
          "code": "return new_indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 555,
          "function_name": "encode",
          "code": "for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "pair": "(256, 101)",
        "new_index": 257
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 533,
          "function_name": "merge",
          "code": "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "indices": [
          256,
          101,
          32,
          113,
          117,
          105,
          99,
          107,
          32,
          98,
          114,
          111,
          119,
          110,
          32,
          102,
          111,
          120
        ],
        "pair": "(256, 101)",
        "new_index": 257
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 535,
          "function_name": "merge",
          "code": "new_indices = []  # @inspect new_indices"
        }
      ],
      "env": {
        "new_indices": []
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 536,
          "function_name": "merge",
          "code": "i = 0  # @inspect i"
        }
      ],
      "env": {
        "i": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 544,
          "function_name": "merge",
          "code": "return new_indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 555,
          "function_name": "encode",
          "code": "for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "pair": "(257, 32)",
        "new_index": 258
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 533,
          "function_name": "merge",
          "code": "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "indices": [
          257,
          32,
          113,
          117,
          105,
          99,
          107,
          32,
          98,
          114,
          111,
          119,
          110,
          32,
          102,
          111,
          120
        ],
        "pair": "(257, 32)",
        "new_index": 258
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 535,
          "function_name": "merge",
          "code": "new_indices = []  # @inspect new_indices"
        }
      ],
      "env": {
        "new_indices": []
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 536,
          "function_name": "merge",
          "code": "i = 0  # @inspect i"
        }
      ],
      "env": {
        "i": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 539,
          "function_name": "merge",
          "code": "new_indices.append(new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 540,
          "function_name": "merge",
          "code": "i += 2"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 538,
          "function_name": "merge",
          "code": "if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 542,
          "function_name": "merge",
          "code": "new_indices.append(indices[i])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 543,
          "function_name": "merge",
          "code": "i += 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 537,
          "function_name": "merge",
          "code": "while i < len(indices):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        },
        {
          "path": "lecture_01.py",
          "line_number": 544,
          "function_name": "merge",
          "code": "return new_indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 556,
          "function_name": "encode",
          "code": "indices = merge(indices, pair, new_index)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 555,
          "function_name": "encode",
          "code": "for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index"
        }
      ],
      "env": {
        "pair": "(257, 32)",
        "new_index": 258
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        },
        {
          "path": "lecture_01.py",
          "line_number": 557,
          "function_name": "encode",
          "code": "return indices"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 711,
          "function_name": "bpe_tokenizer",
          "code": "indices = tokenizer.encode(string)  # @inspect indices"
        }
      ],
      "env": {
        "indices": [
          258,
          113,
          117,
          105,
          99,
          107,
          32,
          98,
          114,
          111,
          119,
          110,
          32,
          102,
          111,
          120
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 712,
          "function_name": "bpe_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 712,
          "function_name": "bpe_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 559,
          "function_name": "decode",
          "code": "def decode(self, indices: list[int]) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 712,
          "function_name": "bpe_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 560,
          "function_name": "decode",
          "code": "bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list"
        }
      ],
      "env": {
        "bytes_list": [
          "b'the '",
          "b'q'",
          "b'u'",
          "b'i'",
          "b'c'",
          "b'k'",
          "b' '",
          "b'b'",
          "b'r'",
          "b'o'",
          "b'w'",
          "b'n'",
          "b' '",
          "b'f'",
          "b'o'",
          "b'x'"
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 712,
          "function_name": "bpe_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 561,
          "function_name": "decode",
          "code": "string = b\"\".join(bytes_list).decode(\"utf-8\")  # @inspect string"
        }
      ],
      "env": {
        "string": "the quick brown fox"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 712,
          "function_name": "bpe_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        },
        {
          "path": "lecture_01.py",
          "line_number": 562,
          "function_name": "decode",
          "code": "return string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 712,
          "function_name": "bpe_tokenizer",
          "code": "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string"
        }
      ],
      "env": {
        "reconstructed_string": "the quick brown fox"
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 713,
          "function_name": "bpe_tokenizer",
          "code": "assert string == reconstructed_string"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 715,
          "function_name": "bpe_tokenizer",
          "code": "text(\"In Assignment 1, you will go beyond this in the following ways:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In Assignment 1, you will go beyond this in the following ways:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 716,
          "function_name": "bpe_tokenizer",
          "code": "text(\"- encode() currently loops over all merges. Only loop over merges that matter.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- encode() currently loops over all merges. Only loop over merges that matter.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 717,
          "function_name": "bpe_tokenizer",
          "code": "text(\"- Detect and preserve special tokens (e.g., <|endoftext|>).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Detect and preserve special tokens (e.g., <|endoftext|>).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 718,
          "function_name": "bpe_tokenizer",
          "code": "text(\"- Use pre-tokenization (e.g., the GPT-2 tokenizer regex).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use pre-tokenization (e.g., the GPT-2 tokenizer regex).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 719,
          "function_name": "bpe_tokenizer",
          "code": "text(\"- Try to make the implementation as fast as possible.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Try to make the implementation as fast as possible.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 495,
          "function_name": "tokenization",
          "code": "bpe_tokenizer()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 497,
          "function_name": "tokenization",
          "code": "text(\"## Summary\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Summary",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 498,
          "function_name": "tokenization",
          "code": "text(\"- Tokenizer: strings <-> tokens (indices)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tokenizer: strings <-> tokens (indices)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 499,
          "function_name": "tokenization",
          "code": "text(\"- Character-based, byte-based, word-based tokenization highly suboptimal\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Character-based, byte-based, word-based tokenization highly suboptimal",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 500,
          "function_name": "tokenization",
          "code": "text(\"- BPE is an effective heuristic that looks at corpus statistics\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- BPE is an effective heuristic that looks at corpus statistics",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        },
        {
          "path": "lecture_01.py",
          "line_number": 501,
          "function_name": "tokenization",
          "code": "text(\"- Tokenization is a necessary evil, maybe one day we'll just do it from bytes...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Tokenization is a necessary evil, maybe one day we'll just do it from bytes...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 33,
          "function_name": "main",
          "code": "tokenization()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_01.py",
          "line_number": 35,
          "function_name": "main",
          "code": "text(\"Next time: PyTorch building blocks, resource accounting\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Next time: PyTorch building blocks, resource accounting",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    }
  ]
}