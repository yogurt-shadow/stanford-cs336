{
  "files": {
    "lecture_17.py": "import os\nimport sys\nfrom typing import Callable\nimport math\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn.functional import softmax\nfrom einops import einsum, rearrange, repeat\nfrom execute_util import text, link, image\nfrom lecture_util import named_link\nfrom references import ppo2017, grpo, qwen3, llama3\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef main():\n    text(\"Last lecture: overview of RL from verifiable rewards (policy gradient)\")\n    text(\"This lecture: deep dive into the mechanics of policy gradient (e.g., GRPO)\")\n\n    rl_setup_for_language_models()\n    policy_gradient()\n    training_walkthrough()\n\n    text(\"Summary\")\n    text(\"- Reinforcement learning is the key to surpassing human abilities\")\n    text(\"- **If** you can measure it, you can optimize it\")\n    text(\"- Policy gradient framework is conceptually clear, just need baselines to reduce variance\")\n    text(\"- RL systems is much more complex than pretraining (inference workloads, manage multiple models)\")\n\n    text(\"Final two lectures:\")\n    text(\"- Junyang Lin (Qwen) \"), link(qwen3)\n    text(\"- Mike Lewis (Llama) \"), link(llama3)\n\n\ndef rl_setup_for_language_models():\n    text(\"**State** s: prompt + generated response so far\")\n    text(\"**Action** a: generate next token\")\n\n    text(\"**Rewards** R: how good the response is; we'll focus on:\")\n    text(\"- Outcome rewards, which depend on the entire response\")\n    text(\"- Verifiable rewards, whose computation is deterministic\")\n    text(\"- Notions of discounting and bootstrapping are less applicable\")\n    text(\"Example: \\\"... Therefore, the answer is 3 miles.\\\"\")\n\n    text(\"**Transition probabilities** T(s' | s, a): deterministic s' = s + a\")\n    text(\"- Can do planning / test-time compute (unlike in robotics)\")\n    text(\"- States are really made up (different from robotics), so a lot of flexibility\")\n\n    text(\"**Policy** \u03c0(a | s): just a language model (fine-tuned)\")\n\n    text(\"**Rollout/episode/trajectory**: s \u2192 a \u2192 ... \u2192 a \u2192 a \u2192 R\")\n    text(\"**Objective**: maximize expected reward E[R]\")\n    text(\"(where the expectation is taken over prompts s and response tokens a)\")\n\n\ndef policy_gradient():\n    text(\"For notational simplicity, let *a* denote the entire response.\")\n\n    text(\"We want to maximize expected reward with respect to the policy \u03c0:\")\n    text(\"E[R] = \u222b p(s) \u03c0(a | s) R(s, a)\")\n\n    text(\"Obvious thing to do is to take the gradient:\")\n    text(\"\u2207 E[R] = \u222b p(s) \u2207 \u03c0(a | s) R(s, a)\")\n    text(\"\u2207 E[R] = \u222b p(s) \u03c0(a | s) \u2207 log \u03c0(a | s) R(s, a)\")\n    text(\"\u2207 E[R] = E[\u2207 log \u03c0(a | s) R(s, a)]\")\n\n    text(\"Naive policy gradient:\")\n    text(\"- Sample prompt s, sample response a ~ \u03c0(a | s)\")\n    text(\"- Update parameters based on \u2207 log \u03c0(a | s) R(s, a) (same as SFT, but weighted by R(s, a))\")\n\n    text(\"Setting: R(s, a) \u2208 {0, 1} = whether response is correct or not\")\n    text(\"- Naive policy gradient only updates on correct responses\")\n    text(\"- Like SFT, but dataset changing over time as policy changes\")\n\n    text(\"Challenge: high noise/variance\")\n    text(\"In this setting, sparse rewards (few responses get reward 1, most get 0)\")\n    text(\"In contrast: in RLHF, reward models (learned from pairwise preferences) are more continuous\")\n\n    text(\"### Baselines\")\n    text(\"Recall \u2207 E[R] = E[\u2207 log \u03c0(a | s) R(s, a)]\")\n    text(\"\u2207 log \u03c0(a | s) R(s, a) is an unbiased estimate of \u2207 E[R], but maybe there are others with lower variance...\")\n\n    text(\"Example: two states\")\n    text(\"- s1: a1 \u2192 reward 11, a2 \u2192 reward 9\")\n    text(\"- s2: a1 \u2192 reward 0, a2 \u2192 reward 2\")\n    text(\"Don't want s1 \u2192 a2 (reward 9) because a1 is better, want s2 \u2192 a2 (reward 2), but 9 > 2\")\n\n    text(\"Idea: maximize the baselined reward: E[R - b(s)]\")\n    text(\"This is just E[R] shifted by a constant E[b(s)] that doesn't depend on the policy \u03c0\")\n    text(\"We update based on \u2207 log \u03c0(a | s) (R(s, a) - b(s))\")\n\n    text(\"What b(s) should we use?\")\n\n    text(\"Example: two states\")\n    text(\"Assuming uniform distribution over (s, a) and |\u2207 \u03c0(a | s)| = 1\")\n    naive_variance = torch.std(torch.tensor([11., 9, 0, 2]))  # @inspect naive_variance\n    text(\"Define baseline b(s1) = 10, b(s2) = 1\")\n    baseline_variance = torch.std(torch.tensor([11. - 10, 9 - 10, 0 - 1, 2 - 1]))  # @inspect baseline_variance\n    text(f\"Variance reduced from {naive_variance:.3f} to {baseline_variance:.3f}\")\n\n    text(\"Optimal b*(s) = E[(\u2207 \u03c0(a | s))^2 R | s] / E[(\u2207 \u03c0(a | s))^2 | s] (for one-parameter models)\")\n    text(\"This is difficult to compute...\")\n    text(\"...so heuristic is to use the mean reward:\")\n    text(\"b(s) = E[R | s]\")\n    text(\"This is still hard to compute and must be estimated.\")\n\n    text(\"### Advantage functions\")\n    text(\"This choice of b(s) has connections to advantage functions.\")\n    text(\"- V(s) = E[R | s] = expected reward from state s\")\n    text(\"- Q(s, a) = E[R | s, a] = expected reward from state s taking action a\")\n    text(\"(Note: Q and R are the same here, because we're assuming *a* has all actions and we have outcome rewards.)\")\n\n    text(\"Definition (advantage): A(s, a) = Q(s, a) - V(s)\")\n    text(\"Intuition: how much better is action a than expected from state s\")\n\n    text(\"If b(s) = E[R | s], then the baselined reward is identical to the advantage!\")\n    text(\"E[R - b(s)] = A(s, a)\")\n\n    text(\"In general:\")\n    text(\"- Ideal: E[\u2207 log \u03c0(a | s) R(s, a)]\")\n    text(\"- Estimate: \u2207 log \u03c0(a | s) \u03b4\")\n    text(\"There are multiple choices of \u03b4, as we'll see later.\")\n\n    named_link(\"CS224R lecture notes\", \"https://cs224r.stanford.edu/slides/03_cs224r_policy_gradients_2025.pdf\")\n\n\ndef training_walkthrough():\n    text(\"Group Relative Policy Optimization (GRPO) \"), link(grpo)\n    text(\"- Simplification to PPO that removes the critic (value function)\")\n    text(\"- Leverages the group structure in the LM setting (multiple responses per prompt), which provides a natural baseline b(s).\")\n    image(\"images/grpo-algorithm.png\", width=700)\n\n    simple_task()        # Define a simple task\n    simple_model()       # Define a simple model\n\n    text(\"Let's now define the GRPO algorithm.\")\n    run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)\n\n    text(\"Let's actually train some models.\")\n    experiments()\n\n\ndef simple_task():\n    text(\"Task: sorting n numbers\")\n\n    text(\"Prompt: n numbers\")\n    prompt = [1, 0, 2]\n    text(\"Response: n numbers\")\n    response = [0, 1, 2]\n\n    text(\"Reward should capture how close to sorted the response is.\")\n\n    text(\"Define a reward that returns the number of positions where the response matches the ground truth.\")\n    reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward\n    reward = sort_distance_reward([3, 1, 0, 2], [7, 2, 2, 5])  # @inspect reward  @stepover\n    reward = sort_distance_reward([3, 1, 0, 2], [0, 3, 1, 2])  # @inspect reward  @stepover\n\n    text(\"Define an alternative reward that gives more partial credit.\")\n    reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward\n    reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [7, 2, 2, 5])  # @inspect reward  @stepover\n    reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 3, 1, 2])  # @inspect reward  @stepover\n\n    text(\"Note that the second reward function provides more credit to the 3rd response than the first reward function.\")\n\n\ndef simple_model():\n    text(\"Define a simple model that maps prompts to responses\")\n    text(\"- Assume fixed prompt and response length\")\n    text(\"- Captures positional information with separate per-position parameters\")\n    text(\"- Decode each position in the response independently (not autoregressive)\")\n\n    model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)\n\n    text(\"Start with a prompt s\")\n    prompts = torch.tensor([[1, 0, 2]])  # [batch pos]\n\n    text(\"Generate responses a\")\n    torch.manual_seed(10)\n    responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses\n\n    text(\"Compute rewards R of these responses:\")\n    rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards\n\n    text(\"Compute deltas \u03b4 given the rewards R (for performing the updates)\")\n    deltas = compute_deltas(rewards=rewards, mode=\"rewards\")  # [batch trial]  @inspect deltas\n    deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas\n    deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas\n    deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas\n\n    text(\"Compute log probabilities of these responses:\")\n    log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs\n\n    text(\"Compute loss so that we can use to update the model parameters\")\n    loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"naive\")  # @inspect loss\n\n    freezing_parameters()\n\n    old_model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)  # Pretend this is an old checkpoint @stepover\n    old_log_probs = compute_log_probs(prompts=prompts, responses=responses, model=old_model)  # @stepover\n    loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss\n    loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss\n\n    text(\"Sometimes, we can use an explicit KL penalty to regularize the model.\")\n    text(\"This can be useful if you want RL a new capability into a model, but you don't want it to forget its original capabilities.\")\n    text(\"KL(p || q) = E_{x ~ p}[log(p(x)/q(x))]\")\n    text(\"KL(p || q) = E_{x ~ p}[-log(q(x)/p(x))]\")\n    text(\"KL(p || q) = E_{x ~ p}[q(x)/p(x) - log(q(x)/p(x)) - 1] because E_{x ~ p}[q(x)/p(x)] = 1\")\n    kl_penalty = compute_kl_penalty(log_probs=log_probs, ref_log_probs=old_log_probs)  # @inspect kl_penalty\n\n    text(\"Summary:\")\n    text(\"- Generate responses\")\n    text(\"- Compute rewards R and \u03b4 (rewards, centered rewards, normalized rewards, max rewards)\")\n    text(\"- Compute log probs of responses\")\n    text(\"- Compute loss from log probs and \u03b4 (naive, unclipped, clipped)\")\n\n\ndef freezing_parameters():\n    text(\"Motivation: in GRPO you'll see ratios: p(a | s) / p_old(a | s)\")\n    text(\"When you're optimizing, it is important to freeze and not differentiate through p_old\")\n    w = torch.tensor(2., requires_grad=True)\n    p = torch.nn.Sigmoid()(w)\n    p_old = torch.nn.Sigmoid()(w)\n    ratio = p / p_old\n    ratio.backward()\n    grad = w.grad  # @inspect grad\n\n    text(\"Do it properly:\")\n    w = torch.tensor(2., requires_grad=True)\n    p = torch.nn.Sigmoid()(w)\n    with torch.no_grad():  # Important: treat p_old as a constant!\n        p_old = torch.nn.Sigmoid()(w)\n    ratio = p / p_old\n    ratio.backward()\n    grad = w.grad  # @inspect grad\n\n\ndef compute_reward(prompts: torch.Tensor, responses: torch.Tensor, reward_fn: Callable[[list[int], list[int]], float]) -> torch.Tensor:\n    \"\"\"\n    Args:\n        prompts (int[batch pos])\n        responses (int[batch trial pos])\n    Returns:\n        rewards (float[batch trial])\n    \"\"\"\n    batch_size, num_responses, _ = responses.shape\n    rewards = torch.empty(batch_size, num_responses, dtype=torch.float32)\n    for i in range(batch_size):\n        for j in range(num_responses):\n            rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])\n    return rewards\n\n\ndef sort_distance_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response\n    \"\"\"\n    Return how close response is to ground_truth = sorted(prompt).\n    In particular, compute number of positions where the response matches the ground truth.\n    \"\"\"\n    assert len(prompt) == len(response)\n    ground_truth = sorted(prompt)\n    return sum(1 for x, y in zip(response, ground_truth) if x == y)\n\n\ndef sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response\n    \"\"\"\n    Return how close response is to ground_truth = sorted(prompt).\n    \"\"\"\n    assert len(prompt) == len(response)\n\n    # Give one point for each token in the prompt that shows up in the response\n    inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward\n\n    # Give one point for each adjacent pair in response that's sorted\n    ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward\n\n    return inclusion_reward + ordering_reward\n\n\nclass Model(nn.Module):\n    def __init__(self, vocab_size: int, embedding_dim: int, prompt_length: int, response_length: int):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # For each position, we have a matrix for encoding and a matrix for decoding\n        self.encode_weights = nn.Parameter(torch.randn(prompt_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim))\n        self.decode_weights = nn.Parameter(torch.randn(response_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim))\n\n    def forward(self, prompts: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            prompts: int[batch pos]\n        Returns:\n            logits: float[batch pos vocab]\n        \"\"\"\n        # Embed the prompts\n        embeddings = self.embedding(prompts)   # [batch pos dim]\n\n        # Transform using per prompt position matrix, collapse into one vector\n        encoded = einsum(embeddings, self.encode_weights, \"batch pos dim1, pos dim1 dim2 -> batch dim2\")\n\n        # Turn into one vector per response position\n        decoded = einsum(encoded, self.decode_weights, \"batch dim2, pos dim2 dim1 -> batch pos dim1\")\n\n        # Convert to logits (input and output share embeddings)\n        logits = einsum(decoded, self.embedding.weight, \"batch pos dim1, vocab dim1 -> batch pos vocab\")\n\n        return logits\n\n\ndef generate_responses(prompts: torch.Tensor, model: Model, num_responses: int) -> torch.Tensor:\n    \"\"\"\n    Args:\n        prompts (int[batch pos])\n    Returns:\n        generated responses: int[batch trial pos]\n\n    Example (batch_size = 3, prompt_length = 3, num_responses = 2, response_length = 4)\n    p1 p1 p1 r1 r1 r1 r1\n             r2 r2 r2 r2\n    p2 p2 p2 r3 r3 r3 r3\n             r4 r4 r4 r4\n    p3 p3 p3 r5 r5 r5 r5\n             r6 r6 r6 r6\n    \"\"\"\n    logits = model(prompts)  # [batch pos vocab]\n    batch_size = prompts.shape[0]\n\n    # Sample num_responses (independently) for each [batch pos]\n    flattened_logits = rearrange(logits, \"batch pos vocab -> (batch pos) vocab\")\n    flattened_responses = torch.multinomial(softmax(flattened_logits, dim=-1), num_samples=num_responses, replacement=True)  # [batch pos trial]\n    responses = rearrange(flattened_responses, \"(batch pos) trial -> batch trial pos\", batch=batch_size)\n    return responses\n\n\ndef compute_log_probs(prompts: torch.Tensor, responses: torch.Tensor, model: Model) -> torch.Tensor:\n    \"\"\"\n    Args:\n        prompts (int[batch pos])\n        responses (int[batch trial pos])\n    Returns:\n        log_probs (float[batch trial pos]) under the model\n    \"\"\"\n    # Compute log prob of responses under model\n    logits = model(prompts)  # [batch pos vocab]\n    log_probs = F.log_softmax(logits, dim=-1)  # [batch pos vocab]\n\n    # Replicate to align with responses\n    num_responses = responses.shape[1]\n    log_probs = repeat(log_probs, \"batch pos vocab -> batch trial pos vocab\", trial=num_responses)  # [batch trial pos vocab]\n\n    # Index into log_probs using responses\n    log_probs = log_probs.gather(dim=-1, index=responses.unsqueeze(-1)).squeeze(-1)  # [batch trial pos]\n\n    return log_probs\n\n\ndef compute_deltas(rewards: torch.Tensor, mode: str) -> torch.Tensor:  # @inspect rewards\n    \"\"\"\n    Args:\n        rewards (float[batch trial])\n    Returns:\n        deltas (float[batch trial]) which are advantage-like quantities for updating\n    \"\"\"\n    if mode == \"rewards\":\n        return rewards\n\n    if mode == \"centered_rewards\":\n        # Compute mean over all the responses (trial) for each prompt (batch)\n        mean_rewards = rewards.mean(dim=-1, keepdim=True)  # @inspect mean_rewards\n        centered_rewards = rewards - mean_rewards  # @inspect centered_rewards\n        return centered_rewards\n\n    if mode == \"normalized_rewards\":\n        mean_rewards = rewards.mean(dim=-1, keepdim=True)  # @inspect mean_rewards\n        std_rewards = rewards.std(dim=-1, keepdim=True)  # @inspect std_rewards\n        centered_rewards = rewards - mean_rewards  # @inspect centered_rewards\n        normalized_rewards = centered_rewards / (std_rewards + 1e-5)  # @inspect normalized_rewards\n        return normalized_rewards\n\n    if mode == \"max_rewards\":\n        # Zero out any reward that isn't the maximum for each batch\n        max_rewards = rewards.max(dim=-1, keepdim=True)[0]\n        max_rewards = torch.where(rewards == max_rewards, rewards, torch.zeros_like(rewards))\n        return max_rewards\n\n    raise ValueError(f\"Unknown mode: {mode}\")\n\n\ndef compute_loss(log_probs: torch.Tensor, deltas: torch.Tensor, mode: str, old_log_probs: torch.Tensor | None = None) -> torch.Tensor:\n    if mode == \"naive\":\n        return -einsum(log_probs, deltas, \"batch trial pos, batch trial -> batch trial pos\").mean()\n\n    if mode == \"unclipped\":\n        ratios = log_probs / old_log_probs  # [batch trial]\n        return -einsum(ratios, deltas, \"batch trial pos, batch trial -> batch trial pos\").mean()\n\n    if mode == \"clipped\":\n        epsilon = 0.01\n        unclipped_ratios = log_probs / old_log_probs  # [batch trial]\n        unclipped = einsum(unclipped_ratios, deltas, \"batch trial pos, batch trial -> batch trial pos\")\n\n        clipped_ratios = torch.clamp(unclipped_ratios, min=1 - epsilon, max=1 + epsilon)\n        clipped = einsum(clipped_ratios, deltas, \"batch trial pos, batch trial -> batch trial pos\")\n        return -torch.minimum(unclipped, clipped).mean()\n\n    raise ValueError(f\"Unknown mode: {mode}\")\n\ndef compute_kl_penalty(log_probs: torch.Tensor, ref_log_probs: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute an estimate of KL(model | ref_model), where the models are given by:\n        log_probs [batch trial pos vocab]\n        ref_log_probs [batch trial pos vocab]\n    Use the estimate:\n        KL(p || q) = E_p[q/p - log(q/p) - 1]\n    \"\"\"\n    return (torch.exp(ref_log_probs - log_probs) - (ref_log_probs - log_probs) - 1).sum(dim=-1).mean()\n\n\ndef run_policy_gradient(num_epochs: int = 100,\n                        num_steps_per_epoch: int = 10,\n                        compute_ref_model_period: int = 10,\n                        num_responses: int = 10,\n                        deltas_mode: str = \"rewards\",\n                        loss_mode: str = \"naive\",\n                        kl_penalty: float = 0.0,\n                        reward_fn: Callable[[list[int], list[int]], float] = sort_inclusion_ordering_reward,\n                        use_cache: bool = False) -> tuple[str, str]:\n    \"\"\"Train a model using policy gradient.\n    Return:\n    - Path to the image of the learning curve.\n    - Path to the log file\n    \"\"\"\n    torch.manual_seed(5)\n\n    image_path = f\"var/policy_gradient_{deltas_mode}_{loss_mode}.png\"\n    log_path = f\"var/policy_gradient_{deltas_mode}_{loss_mode}.txt\"\n\n    # Already ran, just cache it\n    if use_cache and os.path.exists(image_path) and os.path.exists(log_path):\n        return image_path, log_path\n\n    # Define the data\n    prompts = torch.tensor([[1, 0, 2], [3, 2, 4], [1, 2, 3]])\n    vocab_size = prompts.max() + 1\n    prompt_length = response_length = prompts.shape[1]\n\n    model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    records = []\n    ref_log_probs = None\n    ref_model = None\n    old_log_probs = None\n\n    if use_cache:\n        out = open(log_path, \"w\")\n    else:\n        out = sys.stdout\n\n    for epoch in tqdm(range(num_epochs), desc=\"epoch\"):\n        # If using KL penalty, need to get the reference model (freeze it every few epochs)\n        if kl_penalty != 0:\n            if epoch % compute_ref_model_period == 0:\n                ref_model = model.clone()\n\n        # Sample responses and evaluate their rewards\n        responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]\n        rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]\n        deltas = compute_deltas(rewards=rewards, mode=deltas_mode)  # [batch trial]\n\n        if kl_penalty != 0:  # Compute under the reference model\n            with torch.no_grad():\n                ref_log_probs = compute_log_probs(prompts=prompts, responses=responses, model=ref_model)  # [batch trial]\n\n        if loss_mode != \"naive\":  # Compute under the current model (but freeze while we do the inner steps)\n            with torch.no_grad():\n                old_log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]\n\n        # Take a number of steps given the responses\n        for step in range(num_steps_per_epoch):\n            log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]\n            loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=loss_mode, old_log_probs=old_log_probs)  # @inspect loss\n            if kl_penalty != 0:\n                loss += kl_penalty * compute_kl_penalty(log_probs=log_probs, ref_log_probs=ref_log_probs)\n\n            # Print information\n            print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)\n            global_step = epoch * num_steps_per_epoch + step\n            records.append({\"epoch\": epoch, \"step\": global_step, \"loss\": loss.item(), \"mean_reward\": rewards.mean().item()})\n\n            # Backprop and update parameters\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    if use_cache:\n        out.close()\n\n    if use_cache:\n        # Plot step versus loss and reward in two subplots\n        steps = [r[\"step\"] for r in records]\n        losses = [r[\"loss\"] for r in records]\n        rewards = [r[\"mean_reward\"] for r in records]\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n        # Loss subplot\n        ax1.plot(steps, losses)\n        ax1.set_xlabel(\"Step\")\n        ax1.set_ylabel(\"Train Loss\")\n        ax1.set_title(\"Train Loss\")\n\n        # Reward subplot\n        ax2.plot(steps, rewards)\n        ax2.set_xlabel(\"Step\")\n        ax2.set_ylabel(\"Mean Reward\")\n        ax2.set_title(\"Mean Reward\")\n\n        plt.tight_layout()\n        plt.savefig(image_path)\n        plt.close()\n\n    return image_path, log_path\n\n\ndef print_information(epoch: int, step: int, loss: torch.Tensor, prompts: torch.Tensor, rewards: torch.Tensor, responses: torch.Tensor, log_probs: torch.Tensor, deltas: torch.Tensor, out):\n    print(f\"epoch = {epoch}, step = {step}, loss = {loss:.3f}, reward = {rewards.mean():.3f}\", file=out)\n    if epoch % 1 == 0 and step % 5 == 0:\n        for batch in range(prompts.shape[0]):\n            print(f\"  prompt = {prompts[batch, :]}\", file=out)\n            for trial in range(responses.shape[1]):\n                print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)\n\n\ndef tstr(x: torch.Tensor) -> str:\n    return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\"\n\n\ndef experiments():\n    text(\"Let's start with updating based on raw rewards.\")\n    image_path, log_path = run_policy_gradient(num_epochs=100, num_steps_per_epoch=10, num_responses=10, deltas_mode=\"rewards\", loss_mode=\"naive\", reward_fn=sort_inclusion_ordering_reward, use_cache=True)  # @stepover\n    image(image_path, width=600), link(log_path)\n    text(\"Looking through the output, you'll see that by the end, we haven't really learned sorting very well (and this is still the training set).\")\n\n    text(\"Let's try using centered rewards.\")\n    image_path, log_path = run_policy_gradient(num_epochs=100, num_steps_per_epoch=10, num_responses=10, deltas_mode=\"centered_rewards\", loss_mode=\"naive\", reward_fn=sort_inclusion_ordering_reward, use_cache=True)  # @stepover\n    image(image_path, width=600), link(log_path)\n    text(\"This seems to help, as:\")\n    text(\"- Suboptimal rewards get a negative gradient update, and\")\n    text(\"- If all the responses for a given prompt have the same reward, then we don't update.\")\n    text(\"Overall, this is better, but we're still getting stuck in local optima.\")\n\n    text(\"Finally, let's try normalizing by the standard deviation.\")\n    image_path, log_path = run_policy_gradient(num_epochs=100, num_steps_per_epoch=10, num_responses=10, deltas_mode=\"normalized_rewards\", loss_mode=\"naive\", reward_fn=sort_inclusion_ordering_reward, use_cache=True)  # @stepover\n    image(image_path, width=600), link(log_path)\n    text(\"There is not much difference here, and indeed, variants like Dr. GRPO do not perform this normalization to avoid length bias (not an issue here since all responses have the same length. \"), link(\"https://arxiv.org/abs/2503.20783\")\n\n    text(\"Overall, as you can see, reinforcement learning is not trivial, and you can easily get stuck in suboptimal states.\")\n    text(\"The hyperparameters could probably be tuned better...\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "steps": [
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 17,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 18,
          "function_name": "main",
          "code": "text(\"Last lecture: overview of RL from verifiable rewards (policy gradient)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Last lecture: overview of RL from verifiable rewards (policy gradient)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 19,
          "function_name": "main",
          "code": "text(\"This lecture: deep dive into the mechanics of policy gradient (e.g., GRPO)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This lecture: deep dive into the mechanics of policy gradient (e.g., GRPO)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 36,
          "function_name": "rl_setup_for_language_models",
          "code": "def rl_setup_for_language_models():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 37,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"**State** s: prompt + generated response so far\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**State** s: prompt + generated response so far",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 38,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"**Action** a: generate next token\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Action** a: generate next token",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 40,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"**Rewards** R: how good the response is; we'll focus on:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Rewards** R: how good the response is; we'll focus on:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 41,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"- Outcome rewards, which depend on the entire response\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Outcome rewards, which depend on the entire response",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 42,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"- Verifiable rewards, whose computation is deterministic\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Verifiable rewards, whose computation is deterministic",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 43,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"- Notions of discounting and bootstrapping are less applicable\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Notions of discounting and bootstrapping are less applicable",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 44,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"Example: \\\"... Therefore, the answer is 3 miles.\\\"\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example: \"... Therefore, the answer is 3 miles.\"",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 46,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"**Transition probabilities** T(s' | s, a): deterministic s' = s + a\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Transition probabilities** T(s' | s, a): deterministic s' = s + a",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 47,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"- Can do planning / test-time compute (unlike in robotics)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Can do planning / test-time compute (unlike in robotics)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 48,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"- States are really made up (different from robotics), so a lot of flexibility\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- States are really made up (different from robotics), so a lot of flexibility",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 50,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"**Policy** \u03c0(a | s): just a language model (fine-tuned)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Policy** \u03c0(a | s): just a language model (fine-tuned)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 52,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"**Rollout/episode/trajectory**: s \u2192 a \u2192 ... \u2192 a \u2192 a \u2192 R\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Rollout/episode/trajectory**: s \u2192 a \u2192 ... \u2192 a \u2192 a \u2192 R",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 53,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"**Objective**: maximize expected reward E[R]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Objective**: maximize expected reward E[R]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 54,
          "function_name": "rl_setup_for_language_models",
          "code": "text(\"(where the expectation is taken over prompts s and response tokens a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "(where the expectation is taken over prompts s and response tokens a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 21,
          "function_name": "main",
          "code": "rl_setup_for_language_models()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 57,
          "function_name": "policy_gradient",
          "code": "def policy_gradient():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 58,
          "function_name": "policy_gradient",
          "code": "text(\"For notational simplicity, let *a* denote the entire response.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "For notational simplicity, let *a* denote the entire response.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 60,
          "function_name": "policy_gradient",
          "code": "text(\"We want to maximize expected reward with respect to the policy \u03c0:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We want to maximize expected reward with respect to the policy \u03c0:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 61,
          "function_name": "policy_gradient",
          "code": "text(\"E[R] = \u222b p(s) \u03c0(a | s) R(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "E[R] = \u222b p(s) \u03c0(a | s) R(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 63,
          "function_name": "policy_gradient",
          "code": "text(\"Obvious thing to do is to take the gradient:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Obvious thing to do is to take the gradient:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 64,
          "function_name": "policy_gradient",
          "code": "text(\"\u2207 E[R] = \u222b p(s) \u2207 \u03c0(a | s) R(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207 E[R] = \u222b p(s) \u2207 \u03c0(a | s) R(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 65,
          "function_name": "policy_gradient",
          "code": "text(\"\u2207 E[R] = \u222b p(s) \u03c0(a | s) \u2207 log \u03c0(a | s) R(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207 E[R] = \u222b p(s) \u03c0(a | s) \u2207 log \u03c0(a | s) R(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 66,
          "function_name": "policy_gradient",
          "code": "text(\"\u2207 E[R] = E[\u2207 log \u03c0(a | s) R(s, a)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207 E[R] = E[\u2207 log \u03c0(a | s) R(s, a)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 68,
          "function_name": "policy_gradient",
          "code": "text(\"Naive policy gradient:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Naive policy gradient:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 69,
          "function_name": "policy_gradient",
          "code": "text(\"- Sample prompt s, sample response a ~ \u03c0(a | s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Sample prompt s, sample response a ~ \u03c0(a | s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 70,
          "function_name": "policy_gradient",
          "code": "text(\"- Update parameters based on \u2207 log \u03c0(a | s) R(s, a) (same as SFT, but weighted by R(s, a))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Update parameters based on \u2207 log \u03c0(a | s) R(s, a) (same as SFT, but weighted by R(s, a))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 72,
          "function_name": "policy_gradient",
          "code": "text(\"Setting: R(s, a) \u2208 {0, 1} = whether response is correct or not\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Setting: R(s, a) \u2208 {0, 1} = whether response is correct or not",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 73,
          "function_name": "policy_gradient",
          "code": "text(\"- Naive policy gradient only updates on correct responses\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Naive policy gradient only updates on correct responses",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 74,
          "function_name": "policy_gradient",
          "code": "text(\"- Like SFT, but dataset changing over time as policy changes\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Like SFT, but dataset changing over time as policy changes",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 76,
          "function_name": "policy_gradient",
          "code": "text(\"Challenge: high noise/variance\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Challenge: high noise/variance",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 77,
          "function_name": "policy_gradient",
          "code": "text(\"In this setting, sparse rewards (few responses get reward 1, most get 0)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In this setting, sparse rewards (few responses get reward 1, most get 0)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 78,
          "function_name": "policy_gradient",
          "code": "text(\"In contrast: in RLHF, reward models (learned from pairwise preferences) are more continuous\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In contrast: in RLHF, reward models (learned from pairwise preferences) are more continuous",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 80,
          "function_name": "policy_gradient",
          "code": "text(\"### Baselines\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Baselines",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 81,
          "function_name": "policy_gradient",
          "code": "text(\"Recall \u2207 E[R] = E[\u2207 log \u03c0(a | s) R(s, a)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall \u2207 E[R] = E[\u2207 log \u03c0(a | s) R(s, a)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 82,
          "function_name": "policy_gradient",
          "code": "text(\"\u2207 log \u03c0(a | s) R(s, a) is an unbiased estimate of \u2207 E[R], but maybe there are others with lower variance...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207 log \u03c0(a | s) R(s, a) is an unbiased estimate of \u2207 E[R], but maybe there are others with lower variance...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 84,
          "function_name": "policy_gradient",
          "code": "text(\"Example: two states\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example: two states",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 85,
          "function_name": "policy_gradient",
          "code": "text(\"- s1: a1 \u2192 reward 11, a2 \u2192 reward 9\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- s1: a1 \u2192 reward 11, a2 \u2192 reward 9",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 86,
          "function_name": "policy_gradient",
          "code": "text(\"- s2: a1 \u2192 reward 0, a2 \u2192 reward 2\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- s2: a1 \u2192 reward 0, a2 \u2192 reward 2",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 87,
          "function_name": "policy_gradient",
          "code": "text(\"Don't want s1 \u2192 a2 (reward 9) because a1 is better, want s2 \u2192 a2 (reward 2), but 9 > 2\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Don't want s1 \u2192 a2 (reward 9) because a1 is better, want s2 \u2192 a2 (reward 2), but 9 > 2",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 89,
          "function_name": "policy_gradient",
          "code": "text(\"Idea: maximize the baselined reward: E[R - b(s)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Idea: maximize the baselined reward: E[R - b(s)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 90,
          "function_name": "policy_gradient",
          "code": "text(\"This is just E[R] shifted by a constant E[b(s)] that doesn't depend on the policy \u03c0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is just E[R] shifted by a constant E[b(s)] that doesn't depend on the policy \u03c0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 91,
          "function_name": "policy_gradient",
          "code": "text(\"We update based on \u2207 log \u03c0(a | s) (R(s, a) - b(s))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We update based on \u2207 log \u03c0(a | s) (R(s, a) - b(s))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 93,
          "function_name": "policy_gradient",
          "code": "text(\"What b(s) should we use?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What b(s) should we use?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 95,
          "function_name": "policy_gradient",
          "code": "text(\"Example: two states\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Example: two states",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 96,
          "function_name": "policy_gradient",
          "code": "text(\"Assuming uniform distribution over (s, a) and |\u2207 \u03c0(a | s)| = 1\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Assuming uniform distribution over (s, a) and |\u2207 \u03c0(a | s)| = 1",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 97,
          "function_name": "policy_gradient",
          "code": "naive_variance = torch.std(torch.tensor([11., 9, 0, 2]))  # @inspect naive_variance"
        }
      ],
      "env": {
        "naive_variance": 5.322906494140625
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 98,
          "function_name": "policy_gradient",
          "code": "text(\"Define baseline b(s1) = 10, b(s2) = 1\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Define baseline b(s1) = 10, b(s2) = 1",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 99,
          "function_name": "policy_gradient",
          "code": "baseline_variance = torch.std(torch.tensor([11. - 10, 9 - 10, 0 - 1, 2 - 1]))  # @inspect baseline_variance"
        }
      ],
      "env": {
        "baseline_variance": 1.154700517654419
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 100,
          "function_name": "policy_gradient",
          "code": "text(f\"Variance reduced from {naive_variance:.3f} to {baseline_variance:.3f}\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Variance reduced from 5.323 to 1.155",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 102,
          "function_name": "policy_gradient",
          "code": "text(\"Optimal b*(s) = E[(\u2207 \u03c0(a | s))^2 R | s] / E[(\u2207 \u03c0(a | s))^2 | s] (for one-parameter models)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Optimal b*(s) = E[(\u2207 \u03c0(a | s))^2 R | s] / E[(\u2207 \u03c0(a | s))^2 | s] (for one-parameter models)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 103,
          "function_name": "policy_gradient",
          "code": "text(\"This is difficult to compute...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is difficult to compute...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 104,
          "function_name": "policy_gradient",
          "code": "text(\"...so heuristic is to use the mean reward:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...so heuristic is to use the mean reward:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 105,
          "function_name": "policy_gradient",
          "code": "text(\"b(s) = E[R | s]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "b(s) = E[R | s]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 106,
          "function_name": "policy_gradient",
          "code": "text(\"This is still hard to compute and must be estimated.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is still hard to compute and must be estimated.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 108,
          "function_name": "policy_gradient",
          "code": "text(\"### Advantage functions\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Advantage functions",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 109,
          "function_name": "policy_gradient",
          "code": "text(\"This choice of b(s) has connections to advantage functions.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This choice of b(s) has connections to advantage functions.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 110,
          "function_name": "policy_gradient",
          "code": "text(\"- V(s) = E[R | s] = expected reward from state s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- V(s) = E[R | s] = expected reward from state s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 111,
          "function_name": "policy_gradient",
          "code": "text(\"- Q(s, a) = E[R | s, a] = expected reward from state s taking action a\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q(s, a) = E[R | s, a] = expected reward from state s taking action a",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 112,
          "function_name": "policy_gradient",
          "code": "text(\"(Note: Q and R are the same here, because we're assuming *a* has all actions and we have outcome rewards.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "(Note: Q and R are the same here, because we're assuming *a* has all actions and we have outcome rewards.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 114,
          "function_name": "policy_gradient",
          "code": "text(\"Definition (advantage): A(s, a) = Q(s, a) - V(s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Definition (advantage): A(s, a) = Q(s, a) - V(s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 115,
          "function_name": "policy_gradient",
          "code": "text(\"Intuition: how much better is action a than expected from state s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuition: how much better is action a than expected from state s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 117,
          "function_name": "policy_gradient",
          "code": "text(\"If b(s) = E[R | s], then the baselined reward is identical to the advantage!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If b(s) = E[R | s], then the baselined reward is identical to the advantage!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 118,
          "function_name": "policy_gradient",
          "code": "text(\"E[R - b(s)] = A(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "E[R - b(s)] = A(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 120,
          "function_name": "policy_gradient",
          "code": "text(\"In general:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In general:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 121,
          "function_name": "policy_gradient",
          "code": "text(\"- Ideal: E[\u2207 log \u03c0(a | s) R(s, a)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Ideal: E[\u2207 log \u03c0(a | s) R(s, a)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 122,
          "function_name": "policy_gradient",
          "code": "text(\"- Estimate: \u2207 log \u03c0(a | s) \u03b4\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Estimate: \u2207 log \u03c0(a | s) \u03b4",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 123,
          "function_name": "policy_gradient",
          "code": "text(\"There are multiple choices of \u03b4, as we'll see later.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "There are multiple choices of \u03b4, as we'll see later.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 125,
          "function_name": "policy_gradient",
          "code": "named_link(\"CS224R lecture notes\", \"https://cs224r.stanford.edu/slides/03_cs224r_policy_gradients_2025.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [CS224R lecture notes]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://cs224r.stanford.edu/slides/03_cs224r_policy_gradients_2025.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 22,
          "function_name": "main",
          "code": "policy_gradient()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 128,
          "function_name": "training_walkthrough",
          "code": "def training_walkthrough():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 129,
          "function_name": "training_walkthrough",
          "code": "text(\"Group Relative Policy Optimization (GRPO) \"), link(grpo)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Group Relative Policy Optimization (GRPO) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
            "authors": [
              "Zhihong Shao",
              "Peiyi Wang",
              "Qihao Zhu",
              "Runxin Xu",
              "Junxiao Song",
              "Xiao Bi",
              "Haowei Zhang",
              "Mingchuan Zhang",
              "Y. K. Li",
              "Y. Wu",
              "Daya Guo"
            ],
            "organization": null,
            "date": "2024-02-05T18:55:32Z",
            "url": "https://arxiv.org/pdf/2402.03300.pdf",
            "description": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 130,
          "function_name": "training_walkthrough",
          "code": "text(\"- Simplification to PPO that removes the critic (value function)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Simplification to PPO that removes the critic (value function)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 131,
          "function_name": "training_walkthrough",
          "code": "text(\"- Leverages the group structure in the LM setting (multiple responses per prompt), which provides a natural baseline b(s).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Leverages the group structure in the LM setting (multiple responses per prompt), which provides a natural baseline b(s).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 132,
          "function_name": "training_walkthrough",
          "code": "image(\"images/grpo-algorithm.png\", width=700)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/grpo-algorithm.png",
          "style": {
            "width": 700
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 144,
          "function_name": "simple_task",
          "code": "def simple_task():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 145,
          "function_name": "simple_task",
          "code": "text(\"Task: sorting n numbers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Task: sorting n numbers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 147,
          "function_name": "simple_task",
          "code": "text(\"Prompt: n numbers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Prompt: n numbers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 148,
          "function_name": "simple_task",
          "code": "prompt = [1, 0, 2]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 149,
          "function_name": "simple_task",
          "code": "text(\"Response: n numbers\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Response: n numbers",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 150,
          "function_name": "simple_task",
          "code": "response = [0, 1, 2]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 152,
          "function_name": "simple_task",
          "code": "text(\"Reward should capture how close to sorted the response is.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reward should capture how close to sorted the response is.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 154,
          "function_name": "simple_task",
          "code": "text(\"Define a reward that returns the number of positions where the response matches the ground truth.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Define a reward that returns the number of positions where the response matches the ground truth.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 254,
          "function_name": "sort_distance_reward",
          "code": "def sort_distance_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          1,
          0,
          2
        ],
        "response": [
          0,
          1,
          2,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 259,
          "function_name": "sort_distance_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 260,
          "function_name": "sort_distance_reward",
          "code": "ground_truth = sorted(prompt)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 261,
          "function_name": "sort_distance_reward",
          "code": "return sum(1 for x, y in zip(response, ground_truth) if x == y)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 261,
          "function_name": "sort_distance_reward",
          "code": "return sum(1 for x, y in zip(response, ground_truth) if x == y)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 261,
          "function_name": "<genexpr>",
          "code": "return sum(1 for x, y in zip(response, ground_truth) if x == y)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 261,
          "function_name": "sort_distance_reward",
          "code": "return sum(1 for x, y in zip(response, ground_truth) if x == y)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 155,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        }
      ],
      "env": {
        "reward": 4
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 156,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [7, 2, 2, 5])  # @inspect reward  @stepover"
        }
      ],
      "env": {
        "reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 157,
          "function_name": "simple_task",
          "code": "reward = sort_distance_reward([3, 1, 0, 2], [0, 3, 1, 2])  # @inspect reward  @stepover"
        }
      ],
      "env": {
        "reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 159,
          "function_name": "simple_task",
          "code": "text(\"Define an alternative reward that gives more partial credit.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Define an alternative reward that gives more partial credit.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          1,
          0,
          2
        ],
        "response": [
          0,
          1,
          2,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 4
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 160,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 1, 2, 3])  # @inspect reward"
        }
      ],
      "env": {
        "reward": 7
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 161,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [7, 2, 2, 5])  # @inspect reward  @stepover"
        }
      ],
      "env": {
        "reward": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 162,
          "function_name": "simple_task",
          "code": "reward = sort_inclusion_ordering_reward([3, 1, 0, 2], [0, 3, 1, 2])  # @inspect reward  @stepover"
        }
      ],
      "env": {
        "reward": 6
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        },
        {
          "path": "lecture_17.py",
          "line_number": 164,
          "function_name": "simple_task",
          "code": "text(\"Note that the second reward function provides more credit to the 3rd response than the first reward function.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Note that the second reward function provides more credit to the 3rd response than the first reward function.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 134,
          "function_name": "training_walkthrough",
          "code": "simple_task()        # Define a simple task"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 167,
          "function_name": "simple_model",
          "code": "def simple_model():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 168,
          "function_name": "simple_model",
          "code": "text(\"Define a simple model that maps prompts to responses\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Define a simple model that maps prompts to responses",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 169,
          "function_name": "simple_model",
          "code": "text(\"- Assume fixed prompt and response length\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Assume fixed prompt and response length",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 170,
          "function_name": "simple_model",
          "code": "text(\"- Captures positional information with separate per-position parameters\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Captures positional information with separate per-position parameters",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 171,
          "function_name": "simple_model",
          "code": "text(\"- Decode each position in the response independently (not autoregressive)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Decode each position in the response independently (not autoregressive)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "def __init__(self, vocab_size: int, embedding_dim: int, prompt_length: int, response_length: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 281,
          "function_name": "__init__",
          "code": "super().__init__()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 282,
          "function_name": "__init__",
          "code": "self.embedding_dim = embedding_dim"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 283,
          "function_name": "__init__",
          "code": "self.embedding = nn.Embedding(vocab_size, embedding_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 285,
          "function_name": "__init__",
          "code": "self.encode_weights = nn.Parameter(torch.randn(prompt_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 286,
          "function_name": "__init__",
          "code": "self.decode_weights = nn.Parameter(torch.randn(response_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 173,
          "function_name": "simple_model",
          "code": "model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 175,
          "function_name": "simple_model",
          "code": "text(\"Start with a prompt s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Start with a prompt s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 176,
          "function_name": "simple_model",
          "code": "prompts = torch.tensor([[1, 0, 2]])  # [batch pos]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 178,
          "function_name": "simple_model",
          "code": "text(\"Generate responses a\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Generate responses a",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 179,
          "function_name": "simple_model",
          "code": "torch.manual_seed(10)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 310,
          "function_name": "generate_responses",
          "code": "def generate_responses(prompts: torch.Tensor, model: Model, num_responses: int) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 288,
          "function_name": "forward",
          "code": "def forward(self, prompts: torch.Tensor) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 296,
          "function_name": "forward",
          "code": "embeddings = self.embedding(prompts)   # [batch pos dim]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 299,
          "function_name": "forward",
          "code": "encoded = einsum(embeddings, self.encode_weights, \"batch pos dim1, pos dim1 dim2 -> batch dim2\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 302,
          "function_name": "forward",
          "code": "decoded = einsum(encoded, self.decode_weights, \"batch dim2, pos dim2 dim1 -> batch pos dim1\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 305,
          "function_name": "forward",
          "code": "logits = einsum(decoded, self.embedding.weight, \"batch pos dim1, vocab dim1 -> batch pos vocab\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 307,
          "function_name": "forward",
          "code": "return logits"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 326,
          "function_name": "generate_responses",
          "code": "batch_size = prompts.shape[0]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 329,
          "function_name": "generate_responses",
          "code": "flattened_logits = rearrange(logits, \"batch pos vocab -> (batch pos) vocab\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 330,
          "function_name": "generate_responses",
          "code": "flattened_responses = torch.multinomial(softmax(flattened_logits, dim=-1), num_samples=num_responses, replacement=True)  # [batch pos trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 331,
          "function_name": "generate_responses",
          "code": "responses = rearrange(flattened_responses, \"(batch pos) trial -> batch trial pos\", batch=batch_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        },
        {
          "path": "lecture_17.py",
          "line_number": 332,
          "function_name": "generate_responses",
          "code": "return responses"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 180,
          "function_name": "simple_model",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=5)  # [batch trial pos]  @inspect responses"
        }
      ],
      "env": {
        "responses": [
          [
            [
              1,
              1,
              1
            ],
            [
              1,
              2,
              1
            ],
            [
              1,
              2,
              1
            ],
            [
              1,
              1,
              2
            ],
            [
              1,
              2,
              1
            ]
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 182,
          "function_name": "simple_model",
          "code": "text(\"Compute rewards R of these responses:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute rewards R of these responses:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 238,
          "function_name": "compute_reward",
          "code": "def compute_reward(prompts: torch.Tensor, responses: torch.Tensor, reward_fn: Callable[[list[int], list[int]], float]) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 246,
          "function_name": "compute_reward",
          "code": "batch_size, num_responses, _ = responses.shape"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 247,
          "function_name": "compute_reward",
          "code": "rewards = torch.empty(batch_size, num_responses, dtype=torch.float32)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 248,
          "function_name": "compute_reward",
          "code": "for i in range(batch_size):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          1,
          1,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          1,
          2,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          1,
          2,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          1,
          1,
          2
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          1,
          2,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 248,
          "function_name": "compute_reward",
          "code": "for i in range(batch_size):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        },
        {
          "path": "lecture_17.py",
          "line_number": 251,
          "function_name": "compute_reward",
          "code": "return rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 183,
          "function_name": "simple_model",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=sort_inclusion_ordering_reward)  # [batch trial]  @inspect rewards"
        }
      ],
      "env": {
        "rewards": [
          [
            3.0,
            3.0,
            3.0,
            4.0,
            3.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 185,
          "function_name": "simple_model",
          "code": "text(\"Compute deltas \u03b4 given the rewards R (for performing the updates)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute deltas \u03b4 given the rewards R (for performing the updates)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 186,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 186,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 357,
          "function_name": "compute_deltas",
          "code": "def compute_deltas(rewards: torch.Tensor, mode: str) -> torch.Tensor:  # @inspect rewards"
        }
      ],
      "env": {
        "rewards": [
          [
            3.0,
            3.0,
            3.0,
            4.0,
            3.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 186,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 364,
          "function_name": "compute_deltas",
          "code": "if mode == \"rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 186,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 365,
          "function_name": "compute_deltas",
          "code": "return rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 186,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {
        "deltas": [
          [
            3.0,
            3.0,
            3.0,
            4.0,
            3.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 357,
          "function_name": "compute_deltas",
          "code": "def compute_deltas(rewards: torch.Tensor, mode: str) -> torch.Tensor:  # @inspect rewards"
        }
      ],
      "env": {
        "rewards": [
          [
            3.0,
            3.0,
            3.0,
            4.0,
            3.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 364,
          "function_name": "compute_deltas",
          "code": "if mode == \"rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 367,
          "function_name": "compute_deltas",
          "code": "if mode == \"centered_rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 369,
          "function_name": "compute_deltas",
          "code": "mean_rewards = rewards.mean(dim=-1, keepdim=True)  # @inspect mean_rewards"
        }
      ],
      "env": {
        "mean_rewards": [
          [
            3.200000047683716
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 370,
          "function_name": "compute_deltas",
          "code": "centered_rewards = rewards - mean_rewards  # @inspect centered_rewards"
        }
      ],
      "env": {
        "centered_rewards": [
          [
            -0.20000004768371582,
            -0.20000004768371582,
            -0.20000004768371582,
            0.7999999523162842,
            -0.20000004768371582
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 371,
          "function_name": "compute_deltas",
          "code": "return centered_rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 187,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"centered_rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {
        "deltas": [
          [
            -0.20000004768371582,
            -0.20000004768371582,
            -0.20000004768371582,
            0.7999999523162842,
            -0.20000004768371582
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 357,
          "function_name": "compute_deltas",
          "code": "def compute_deltas(rewards: torch.Tensor, mode: str) -> torch.Tensor:  # @inspect rewards"
        }
      ],
      "env": {
        "rewards": [
          [
            3.0,
            3.0,
            3.0,
            4.0,
            3.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 364,
          "function_name": "compute_deltas",
          "code": "if mode == \"rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 367,
          "function_name": "compute_deltas",
          "code": "if mode == \"centered_rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 373,
          "function_name": "compute_deltas",
          "code": "if mode == \"normalized_rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 374,
          "function_name": "compute_deltas",
          "code": "mean_rewards = rewards.mean(dim=-1, keepdim=True)  # @inspect mean_rewards"
        }
      ],
      "env": {
        "mean_rewards": [
          [
            3.200000047683716
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 375,
          "function_name": "compute_deltas",
          "code": "std_rewards = rewards.std(dim=-1, keepdim=True)  # @inspect std_rewards"
        }
      ],
      "env": {
        "std_rewards": [
          [
            0.4472135901451111
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 376,
          "function_name": "compute_deltas",
          "code": "centered_rewards = rewards - mean_rewards  # @inspect centered_rewards"
        }
      ],
      "env": {
        "centered_rewards": [
          [
            -0.20000004768371582,
            -0.20000004768371582,
            -0.20000004768371582,
            0.7999999523162842,
            -0.20000004768371582
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 377,
          "function_name": "compute_deltas",
          "code": "normalized_rewards = centered_rewards / (std_rewards + 1e-5)  # @inspect normalized_rewards"
        }
      ],
      "env": {
        "normalized_rewards": [
          [
            -0.44720369577407837,
            -0.44720369577407837,
            -0.44720369577407837,
            1.7888141870498657,
            -0.44720369577407837
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 378,
          "function_name": "compute_deltas",
          "code": "return normalized_rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 188,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"normalized_rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {
        "deltas": [
          [
            -0.44720369577407837,
            -0.44720369577407837,
            -0.44720369577407837,
            1.7888141870498657,
            -0.44720369577407837
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 357,
          "function_name": "compute_deltas",
          "code": "def compute_deltas(rewards: torch.Tensor, mode: str) -> torch.Tensor:  # @inspect rewards"
        }
      ],
      "env": {
        "rewards": [
          [
            3.0,
            3.0,
            3.0,
            4.0,
            3.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 364,
          "function_name": "compute_deltas",
          "code": "if mode == \"rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 367,
          "function_name": "compute_deltas",
          "code": "if mode == \"centered_rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 373,
          "function_name": "compute_deltas",
          "code": "if mode == \"normalized_rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 380,
          "function_name": "compute_deltas",
          "code": "if mode == \"max_rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 382,
          "function_name": "compute_deltas",
          "code": "max_rewards = rewards.max(dim=-1, keepdim=True)[0]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 383,
          "function_name": "compute_deltas",
          "code": "max_rewards = torch.where(rewards == max_rewards, rewards, torch.zeros_like(rewards))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        },
        {
          "path": "lecture_17.py",
          "line_number": 384,
          "function_name": "compute_deltas",
          "code": "return max_rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 189,
          "function_name": "simple_model",
          "code": "deltas = compute_deltas(rewards=rewards, mode=\"max_rewards\")  # [batch trial]  @inspect deltas"
        }
      ],
      "env": {
        "deltas": [
          [
            0.0,
            0.0,
            0.0,
            4.0,
            0.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 191,
          "function_name": "simple_model",
          "code": "text(\"Compute log probabilities of these responses:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute log probabilities of these responses:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 335,
          "function_name": "compute_log_probs",
          "code": "def compute_log_probs(prompts: torch.Tensor, responses: torch.Tensor, model: Model) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 288,
          "function_name": "forward",
          "code": "def forward(self, prompts: torch.Tensor) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 296,
          "function_name": "forward",
          "code": "embeddings = self.embedding(prompts)   # [batch pos dim]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 299,
          "function_name": "forward",
          "code": "encoded = einsum(embeddings, self.encode_weights, \"batch pos dim1, pos dim1 dim2 -> batch dim2\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 302,
          "function_name": "forward",
          "code": "decoded = einsum(encoded, self.decode_weights, \"batch dim2, pos dim2 dim1 -> batch pos dim1\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 305,
          "function_name": "forward",
          "code": "logits = einsum(decoded, self.embedding.weight, \"batch pos dim1, vocab dim1 -> batch pos vocab\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 307,
          "function_name": "forward",
          "code": "return logits"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 345,
          "function_name": "compute_log_probs",
          "code": "log_probs = F.log_softmax(logits, dim=-1)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 348,
          "function_name": "compute_log_probs",
          "code": "num_responses = responses.shape[1]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 349,
          "function_name": "compute_log_probs",
          "code": "log_probs = repeat(log_probs, \"batch pos vocab -> batch trial pos vocab\", trial=num_responses)  # [batch trial pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 352,
          "function_name": "compute_log_probs",
          "code": "log_probs = log_probs.gather(dim=-1, index=responses.unsqueeze(-1)).squeeze(-1)  # [batch trial pos]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        },
        {
          "path": "lecture_17.py",
          "line_number": 354,
          "function_name": "compute_log_probs",
          "code": "return log_probs"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 192,
          "function_name": "simple_model",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]  @inspect log_probs"
        }
      ],
      "env": {
        "log_probs": [
          [
            [
              -0.01567213609814644,
              -1.005124807357788,
              -0.05150599032640457
            ],
            [
              -0.01567213609814644,
              -0.8296878337860107,
              -0.05150599032640457
            ],
            [
              -0.01567213609814644,
              -0.8296878337860107,
              -0.05150599032640457
            ],
            [
              -0.01567213609814644,
              -1.005124807357788,
              -2.992189884185791
            ],
            [
              -0.01567213609814644,
              -0.8296878337860107,
              -0.05150599032640457
            ]
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 194,
          "function_name": "simple_model",
          "code": "text(\"Compute loss so that we can use to update the model parameters\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute loss so that we can use to update the model parameters",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 195,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"naive\")  # @inspect loss"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 195,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"naive\")  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 389,
          "function_name": "compute_loss",
          "code": "def compute_loss(log_probs: torch.Tensor, deltas: torch.Tensor, mode: str, old_log_probs: torch.Tensor | None = None) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 195,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"naive\")  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 390,
          "function_name": "compute_loss",
          "code": "if mode == \"naive\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 195,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"naive\")  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 391,
          "function_name": "compute_loss",
          "code": "return -einsum(log_probs, deltas, \"batch trial pos, batch trial -> batch trial pos\").mean()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 195,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"naive\")  # @inspect loss"
        }
      ],
      "env": {
        "loss": 1.0701297521591187
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 218,
          "function_name": "freezing_parameters",
          "code": "def freezing_parameters():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 219,
          "function_name": "freezing_parameters",
          "code": "text(\"Motivation: in GRPO you'll see ratios: p(a | s) / p_old(a | s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Motivation: in GRPO you'll see ratios: p(a | s) / p_old(a | s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 220,
          "function_name": "freezing_parameters",
          "code": "text(\"When you're optimizing, it is important to freeze and not differentiate through p_old\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "When you're optimizing, it is important to freeze and not differentiate through p_old",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 221,
          "function_name": "freezing_parameters",
          "code": "w = torch.tensor(2., requires_grad=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 222,
          "function_name": "freezing_parameters",
          "code": "p = torch.nn.Sigmoid()(w)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 223,
          "function_name": "freezing_parameters",
          "code": "p_old = torch.nn.Sigmoid()(w)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 224,
          "function_name": "freezing_parameters",
          "code": "ratio = p / p_old"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 225,
          "function_name": "freezing_parameters",
          "code": "ratio.backward()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 226,
          "function_name": "freezing_parameters",
          "code": "grad = w.grad  # @inspect grad"
        }
      ],
      "env": {
        "grad": 0.0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 228,
          "function_name": "freezing_parameters",
          "code": "text(\"Do it properly:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Do it properly:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 229,
          "function_name": "freezing_parameters",
          "code": "w = torch.tensor(2., requires_grad=True)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 230,
          "function_name": "freezing_parameters",
          "code": "p = torch.nn.Sigmoid()(w)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 231,
          "function_name": "freezing_parameters",
          "code": "with torch.no_grad():  # Important: treat p_old as a constant!"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 232,
          "function_name": "freezing_parameters",
          "code": "p_old = torch.nn.Sigmoid()(w)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 231,
          "function_name": "freezing_parameters",
          "code": "with torch.no_grad():  # Important: treat p_old as a constant!"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 233,
          "function_name": "freezing_parameters",
          "code": "ratio = p / p_old"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 234,
          "function_name": "freezing_parameters",
          "code": "ratio.backward()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 235,
          "function_name": "freezing_parameters",
          "code": "grad = w.grad  # @inspect grad"
        }
      ],
      "env": {
        "grad": 0.11920296400785446
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 197,
          "function_name": "simple_model",
          "code": "freezing_parameters()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 199,
          "function_name": "simple_model",
          "code": "old_model = Model(vocab_size=3, embedding_dim=10, prompt_length=3, response_length=3)  # Pretend this is an old checkpoint @stepover"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 200,
          "function_name": "simple_model",
          "code": "old_log_probs = compute_log_probs(prompts=prompts, responses=responses, model=old_model)  # @stepover"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 201,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 201,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 389,
          "function_name": "compute_loss",
          "code": "def compute_loss(log_probs: torch.Tensor, deltas: torch.Tensor, mode: str, old_log_probs: torch.Tensor | None = None) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 201,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 390,
          "function_name": "compute_loss",
          "code": "if mode == \"naive\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 201,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 393,
          "function_name": "compute_loss",
          "code": "if mode == \"unclipped\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 201,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 394,
          "function_name": "compute_loss",
          "code": "ratios = log_probs / old_log_probs  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 201,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 395,
          "function_name": "compute_loss",
          "code": "return -einsum(ratios, deltas, \"batch trial pos, batch trial -> batch trial pos\").mean()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 201,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"unclipped\", old_log_probs=old_log_probs)  # @inspect loss"
        }
      ],
      "env": {
        "loss": -574.9898071289062
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 389,
          "function_name": "compute_loss",
          "code": "def compute_loss(log_probs: torch.Tensor, deltas: torch.Tensor, mode: str, old_log_probs: torch.Tensor | None = None) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 390,
          "function_name": "compute_loss",
          "code": "if mode == \"naive\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 393,
          "function_name": "compute_loss",
          "code": "if mode == \"unclipped\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 397,
          "function_name": "compute_loss",
          "code": "if mode == \"clipped\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 398,
          "function_name": "compute_loss",
          "code": "epsilon = 0.01"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 399,
          "function_name": "compute_loss",
          "code": "unclipped_ratios = log_probs / old_log_probs  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 400,
          "function_name": "compute_loss",
          "code": "unclipped = einsum(unclipped_ratios, deltas, \"batch trial pos, batch trial -> batch trial pos\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 402,
          "function_name": "compute_loss",
          "code": "clipped_ratios = torch.clamp(unclipped_ratios, min=1 - epsilon, max=1 + epsilon)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 403,
          "function_name": "compute_loss",
          "code": "clipped = einsum(clipped_ratios, deltas, \"batch trial pos, batch trial -> batch trial pos\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 404,
          "function_name": "compute_loss",
          "code": "return -torch.minimum(unclipped, clipped).mean()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 202,
          "function_name": "simple_model",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=\"clipped\", old_log_probs=old_log_probs)  # @inspect loss"
        }
      ],
      "env": {
        "loss": -0.5361258387565613
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 204,
          "function_name": "simple_model",
          "code": "text(\"Sometimes, we can use an explicit KL penalty to regularize the model.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sometimes, we can use an explicit KL penalty to regularize the model.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 205,
          "function_name": "simple_model",
          "code": "text(\"This can be useful if you want RL a new capability into a model, but you don't want it to forget its original capabilities.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This can be useful if you want RL a new capability into a model, but you don't want it to forget its original capabilities.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 206,
          "function_name": "simple_model",
          "code": "text(\"KL(p || q) = E_{x ~ p}[log(p(x)/q(x))]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "KL(p || q) = E_{x ~ p}[log(p(x)/q(x))]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 207,
          "function_name": "simple_model",
          "code": "text(\"KL(p || q) = E_{x ~ p}[-log(q(x)/p(x))]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "KL(p || q) = E_{x ~ p}[-log(q(x)/p(x))]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 208,
          "function_name": "simple_model",
          "code": "text(\"KL(p || q) = E_{x ~ p}[q(x)/p(x) - log(q(x)/p(x)) - 1] because E_{x ~ p}[q(x)/p(x)] = 1\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "KL(p || q) = E_{x ~ p}[q(x)/p(x) - log(q(x)/p(x)) - 1] because E_{x ~ p}[q(x)/p(x)] = 1",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 209,
          "function_name": "simple_model",
          "code": "kl_penalty = compute_kl_penalty(log_probs=log_probs, ref_log_probs=old_log_probs)  # @inspect kl_penalty"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 209,
          "function_name": "simple_model",
          "code": "kl_penalty = compute_kl_penalty(log_probs=log_probs, ref_log_probs=old_log_probs)  # @inspect kl_penalty"
        },
        {
          "path": "lecture_17.py",
          "line_number": 408,
          "function_name": "compute_kl_penalty",
          "code": "def compute_kl_penalty(log_probs: torch.Tensor, ref_log_probs: torch.Tensor) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 209,
          "function_name": "simple_model",
          "code": "kl_penalty = compute_kl_penalty(log_probs=log_probs, ref_log_probs=old_log_probs)  # @inspect kl_penalty"
        },
        {
          "path": "lecture_17.py",
          "line_number": 416,
          "function_name": "compute_kl_penalty",
          "code": "return (torch.exp(ref_log_probs - log_probs) - (ref_log_probs - log_probs) - 1).sum(dim=-1).mean()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 209,
          "function_name": "simple_model",
          "code": "kl_penalty = compute_kl_penalty(log_probs=log_probs, ref_log_probs=old_log_probs)  # @inspect kl_penalty"
        }
      ],
      "env": {
        "kl_penalty": 1.9032036066055298
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 211,
          "function_name": "simple_model",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 212,
          "function_name": "simple_model",
          "code": "text(\"- Generate responses\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Generate responses",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 213,
          "function_name": "simple_model",
          "code": "text(\"- Compute rewards R and \u03b4 (rewards, centered rewards, normalized rewards, max rewards)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Compute rewards R and \u03b4 (rewards, centered rewards, normalized rewards, max rewards)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 214,
          "function_name": "simple_model",
          "code": "text(\"- Compute log probs of responses\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Compute log probs of responses",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        },
        {
          "path": "lecture_17.py",
          "line_number": 215,
          "function_name": "simple_model",
          "code": "text(\"- Compute loss from log probs and \u03b4 (naive, unclipped, clipped)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Compute loss from log probs and \u03b4 (naive, unclipped, clipped)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 135,
          "function_name": "training_walkthrough",
          "code": "simple_model()       # Define a simple model"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 137,
          "function_name": "training_walkthrough",
          "code": "text(\"Let's now define the GRPO algorithm.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's now define the GRPO algorithm.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 419,
          "function_name": "run_policy_gradient",
          "code": "def run_policy_gradient(num_epochs: int = 100,"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 433,
          "function_name": "run_policy_gradient",
          "code": "torch.manual_seed(5)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 435,
          "function_name": "run_policy_gradient",
          "code": "image_path = f\"var/policy_gradient_{deltas_mode}_{loss_mode}.png\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 436,
          "function_name": "run_policy_gradient",
          "code": "log_path = f\"var/policy_gradient_{deltas_mode}_{loss_mode}.txt\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 439,
          "function_name": "run_policy_gradient",
          "code": "if use_cache and os.path.exists(image_path) and os.path.exists(log_path):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 443,
          "function_name": "run_policy_gradient",
          "code": "prompts = torch.tensor([[1, 0, 2], [3, 2, 4], [1, 2, 3]])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 444,
          "function_name": "run_policy_gradient",
          "code": "vocab_size = prompts.max() + 1"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 445,
          "function_name": "run_policy_gradient",
          "code": "prompt_length = response_length = prompts.shape[1]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "def __init__(self, vocab_size: int, embedding_dim: int, prompt_length: int, response_length: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 281,
          "function_name": "__init__",
          "code": "super().__init__()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 282,
          "function_name": "__init__",
          "code": "self.embedding_dim = embedding_dim"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 283,
          "function_name": "__init__",
          "code": "self.embedding = nn.Embedding(vocab_size, embedding_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 285,
          "function_name": "__init__",
          "code": "self.encode_weights = nn.Parameter(torch.randn(prompt_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 286,
          "function_name": "__init__",
          "code": "self.decode_weights = nn.Parameter(torch.randn(response_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 447,
          "function_name": "run_policy_gradient",
          "code": "model = Model(vocab_size=vocab_size, embedding_dim=10, prompt_length=prompt_length, response_length=response_length)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 448,
          "function_name": "run_policy_gradient",
          "code": "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 450,
          "function_name": "run_policy_gradient",
          "code": "records = []"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 451,
          "function_name": "run_policy_gradient",
          "code": "ref_log_probs = None"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 452,
          "function_name": "run_policy_gradient",
          "code": "ref_model = None"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 453,
          "function_name": "run_policy_gradient",
          "code": "old_log_probs = None"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 455,
          "function_name": "run_policy_gradient",
          "code": "if use_cache:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 458,
          "function_name": "run_policy_gradient",
          "code": "out = sys.stdout"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 460,
          "function_name": "run_policy_gradient",
          "code": "for epoch in tqdm(range(num_epochs), desc=\"epoch\"):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 462,
          "function_name": "run_policy_gradient",
          "code": "if kl_penalty != 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 310,
          "function_name": "generate_responses",
          "code": "def generate_responses(prompts: torch.Tensor, model: Model, num_responses: int) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 288,
          "function_name": "forward",
          "code": "def forward(self, prompts: torch.Tensor) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 296,
          "function_name": "forward",
          "code": "embeddings = self.embedding(prompts)   # [batch pos dim]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 299,
          "function_name": "forward",
          "code": "encoded = einsum(embeddings, self.encode_weights, \"batch pos dim1, pos dim1 dim2 -> batch dim2\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 302,
          "function_name": "forward",
          "code": "decoded = einsum(encoded, self.decode_weights, \"batch dim2, pos dim2 dim1 -> batch pos dim1\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 305,
          "function_name": "forward",
          "code": "logits = einsum(decoded, self.embedding.weight, \"batch pos dim1, vocab dim1 -> batch pos vocab\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 307,
          "function_name": "forward",
          "code": "return logits"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 325,
          "function_name": "generate_responses",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 326,
          "function_name": "generate_responses",
          "code": "batch_size = prompts.shape[0]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 329,
          "function_name": "generate_responses",
          "code": "flattened_logits = rearrange(logits, \"batch pos vocab -> (batch pos) vocab\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 330,
          "function_name": "generate_responses",
          "code": "flattened_responses = torch.multinomial(softmax(flattened_logits, dim=-1), num_samples=num_responses, replacement=True)  # [batch pos trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 331,
          "function_name": "generate_responses",
          "code": "responses = rearrange(flattened_responses, \"(batch pos) trial -> batch trial pos\", batch=batch_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 332,
          "function_name": "generate_responses",
          "code": "return responses"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 467,
          "function_name": "run_policy_gradient",
          "code": "responses = generate_responses(prompts=prompts, model=model, num_responses=num_responses)  # [batch trial pos]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 238,
          "function_name": "compute_reward",
          "code": "def compute_reward(prompts: torch.Tensor, responses: torch.Tensor, reward_fn: Callable[[list[int], list[int]], float]) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 246,
          "function_name": "compute_reward",
          "code": "batch_size, num_responses, _ = responses.shape"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 247,
          "function_name": "compute_reward",
          "code": "rewards = torch.empty(batch_size, num_responses, dtype=torch.float32)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 248,
          "function_name": "compute_reward",
          "code": "for i in range(batch_size):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          4,
          2,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          4,
          0,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          2,
          0,
          4
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          2,
          0,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          4,
          0,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          2,
          0,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          4,
          2,
          4
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          4,
          0,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          4,
          2,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          0,
          2
        ],
        "response": [
          4,
          0,
          3
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 248,
          "function_name": "compute_reward",
          "code": "for i in range(batch_size):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          0,
          0,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          1,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          0,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          0,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          0,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          1,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          1,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          0,
          0,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          1,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          3,
          2,
          4
        ],
        "response": [
          3,
          0,
          0
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 248,
          "function_name": "compute_reward",
          "code": "for i in range(batch_size):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          1,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          1,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 264,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "def sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -> float:  # @inspect prompt, @inspect response"
        }
      ],
      "env": {
        "prompt": [
          1,
          2,
          3
        ],
        "response": [
          2,
          4,
          1
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 268,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "assert len(prompt) == len(response)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "<genexpr>",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 271,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward"
        }
      ],
      "env": {
        "inclusion_reward": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "<genexpr>",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 274,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x <= y)  # @inspect ordering_reward"
        }
      ],
      "env": {
        "ordering_reward": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        },
        {
          "path": "lecture_17.py",
          "line_number": 276,
          "function_name": "sort_inclusion_ordering_reward",
          "code": "return inclusion_reward + ordering_reward"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 250,
          "function_name": "compute_reward",
          "code": "rewards[i, j] = reward_fn(prompts[i, :], responses[i, j, :])"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 249,
          "function_name": "compute_reward",
          "code": "for j in range(num_responses):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 248,
          "function_name": "compute_reward",
          "code": "for i in range(batch_size):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 251,
          "function_name": "compute_reward",
          "code": "return rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 468,
          "function_name": "run_policy_gradient",
          "code": "rewards = compute_reward(prompts=prompts, responses=responses, reward_fn=reward_fn)  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 469,
          "function_name": "run_policy_gradient",
          "code": "deltas = compute_deltas(rewards=rewards, mode=deltas_mode)  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 469,
          "function_name": "run_policy_gradient",
          "code": "deltas = compute_deltas(rewards=rewards, mode=deltas_mode)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 357,
          "function_name": "compute_deltas",
          "code": "def compute_deltas(rewards: torch.Tensor, mode: str) -> torch.Tensor:  # @inspect rewards"
        }
      ],
      "env": {
        "rewards": [
          [
            2.0,
            2.0,
            3.0,
            3.0,
            2.0,
            3.0,
            2.0,
            2.0,
            2.0,
            2.0
          ],
          [
            2.0,
            1.0,
            2.0,
            2.0,
            2.0,
            1.0,
            1.0,
            2.0,
            2.0,
            2.0
          ],
          [
            3.0,
            3.0,
            3.0,
            3.0,
            3.0,
            3.0,
            3.0,
            3.0,
            3.0,
            3.0
          ]
        ]
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 469,
          "function_name": "run_policy_gradient",
          "code": "deltas = compute_deltas(rewards=rewards, mode=deltas_mode)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 364,
          "function_name": "compute_deltas",
          "code": "if mode == \"rewards\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 469,
          "function_name": "run_policy_gradient",
          "code": "deltas = compute_deltas(rewards=rewards, mode=deltas_mode)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 365,
          "function_name": "compute_deltas",
          "code": "return rewards"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 469,
          "function_name": "run_policy_gradient",
          "code": "deltas = compute_deltas(rewards=rewards, mode=deltas_mode)  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 471,
          "function_name": "run_policy_gradient",
          "code": "if kl_penalty != 0:  # Compute under the reference model"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 475,
          "function_name": "run_policy_gradient",
          "code": "if loss_mode != \"naive\":  # Compute under the current model (but freeze while we do the inner steps)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 480,
          "function_name": "run_policy_gradient",
          "code": "for step in range(num_steps_per_epoch):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 335,
          "function_name": "compute_log_probs",
          "code": "def compute_log_probs(prompts: torch.Tensor, responses: torch.Tensor, model: Model) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 288,
          "function_name": "forward",
          "code": "def forward(self, prompts: torch.Tensor) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 296,
          "function_name": "forward",
          "code": "embeddings = self.embedding(prompts)   # [batch pos dim]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 299,
          "function_name": "forward",
          "code": "encoded = einsum(embeddings, self.encode_weights, \"batch pos dim1, pos dim1 dim2 -> batch dim2\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 302,
          "function_name": "forward",
          "code": "decoded = einsum(encoded, self.decode_weights, \"batch dim2, pos dim2 dim1 -> batch pos dim1\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 305,
          "function_name": "forward",
          "code": "logits = einsum(decoded, self.embedding.weight, \"batch pos dim1, vocab dim1 -> batch pos vocab\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1739,
          "function_name": "_wrapped_call_impl",
          "code": "return self._call_impl(*args, **kwargs)"
        },
        {
          "path": "venv/lib/python3.12/site-packages/torch/nn/modules/module.py",
          "line_number": 1750,
          "function_name": "_call_impl",
          "code": "return forward_call(*args, **kwargs)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 307,
          "function_name": "forward",
          "code": "return logits"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 344,
          "function_name": "compute_log_probs",
          "code": "logits = model(prompts)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 345,
          "function_name": "compute_log_probs",
          "code": "log_probs = F.log_softmax(logits, dim=-1)  # [batch pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 348,
          "function_name": "compute_log_probs",
          "code": "num_responses = responses.shape[1]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 349,
          "function_name": "compute_log_probs",
          "code": "log_probs = repeat(log_probs, \"batch pos vocab -> batch trial pos vocab\", trial=num_responses)  # [batch trial pos vocab]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 352,
          "function_name": "compute_log_probs",
          "code": "log_probs = log_probs.gather(dim=-1, index=responses.unsqueeze(-1)).squeeze(-1)  # [batch trial pos]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        },
        {
          "path": "lecture_17.py",
          "line_number": 354,
          "function_name": "compute_log_probs",
          "code": "return log_probs"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 481,
          "function_name": "run_policy_gradient",
          "code": "log_probs = compute_log_probs(prompts=prompts, responses=responses, model=model)  # [batch trial]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 482,
          "function_name": "run_policy_gradient",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=loss_mode, old_log_probs=old_log_probs)  # @inspect loss"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 482,
          "function_name": "run_policy_gradient",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=loss_mode, old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 389,
          "function_name": "compute_loss",
          "code": "def compute_loss(log_probs: torch.Tensor, deltas: torch.Tensor, mode: str, old_log_probs: torch.Tensor | None = None) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 482,
          "function_name": "run_policy_gradient",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=loss_mode, old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 390,
          "function_name": "compute_loss",
          "code": "if mode == \"naive\":"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 482,
          "function_name": "run_policy_gradient",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=loss_mode, old_log_probs=old_log_probs)  # @inspect loss"
        },
        {
          "path": "lecture_17.py",
          "line_number": 391,
          "function_name": "compute_loss",
          "code": "return -einsum(log_probs, deltas, \"batch trial pos, batch trial -> batch trial pos\").mean()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 482,
          "function_name": "run_policy_gradient",
          "code": "loss = compute_loss(log_probs=log_probs, deltas=deltas, mode=loss_mode, old_log_probs=old_log_probs)  # @inspect loss"
        }
      ],
      "env": {
        "loss": 1.1641370058059692
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 483,
          "function_name": "run_policy_gradient",
          "code": "if kl_penalty != 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 525,
          "function_name": "print_information",
          "code": "def print_information(epoch: int, step: int, loss: torch.Tensor, prompts: torch.Tensor, rewards: torch.Tensor, responses: torch.Tensor, log_probs: torch.Tensor, deltas: torch.Tensor, out):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 526,
          "function_name": "print_information",
          "code": "print(f\"epoch = {epoch}, step = {step}, loss = {loss:.3f}, reward = {rewards.mean():.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 527,
          "function_name": "print_information",
          "code": "if epoch % 1 == 0 and step % 5 == 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 528,
          "function_name": "print_information",
          "code": "for batch in range(prompts.shape[0]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 529,
          "function_name": "print_information",
          "code": "print(f\"  prompt = {prompts[batch, :]}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 528,
          "function_name": "print_information",
          "code": "for batch in range(prompts.shape[0]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 529,
          "function_name": "print_information",
          "code": "print(f\"  prompt = {prompts[batch, :]}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 528,
          "function_name": "print_information",
          "code": "for batch in range(prompts.shape[0]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 529,
          "function_name": "print_information",
          "code": "print(f\"  prompt = {prompts[batch, :]}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 534,
          "function_name": "tstr",
          "code": "def tstr(x: torch.Tensor) -> str:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "<genexpr>",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 535,
          "function_name": "tstr",
          "code": "return \"[\" + \", \".join(f\"{x[i]:.3f}\" for i in range(x.shape[0])) + \"]\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 531,
          "function_name": "print_information",
          "code": "print(f\"    response = {responses[batch, trial, :]}, log_probs = {tstr(log_probs[batch, trial])}, reward = {rewards[batch, trial]}, delta = {deltas[batch, trial]:.3f}\", file=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 530,
          "function_name": "print_information",
          "code": "for trial in range(responses.shape[1]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 528,
          "function_name": "print_information",
          "code": "for batch in range(prompts.shape[0]):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 487,
          "function_name": "run_policy_gradient",
          "code": "print_information(epoch=epoch, step=step, loss=loss, prompts=prompts, rewards=rewards, responses=responses, log_probs=log_probs, deltas=deltas, out=out)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 488,
          "function_name": "run_policy_gradient",
          "code": "global_step = epoch * num_steps_per_epoch + step"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 489,
          "function_name": "run_policy_gradient",
          "code": "records.append({\"epoch\": epoch, \"step\": global_step, \"loss\": loss.item(), \"mean_reward\": rewards.mean().item()})"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 492,
          "function_name": "run_policy_gradient",
          "code": "optimizer.zero_grad()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 493,
          "function_name": "run_policy_gradient",
          "code": "loss.backward()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 494,
          "function_name": "run_policy_gradient",
          "code": "optimizer.step()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 480,
          "function_name": "run_policy_gradient",
          "code": "for step in range(num_steps_per_epoch):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 460,
          "function_name": "run_policy_gradient",
          "code": "for epoch in tqdm(range(num_epochs), desc=\"epoch\"):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 495,
          "function_name": "run_policy_gradient",
          "code": "if use_cache:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 498,
          "function_name": "run_policy_gradient",
          "code": "if use_cache:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        },
        {
          "path": "lecture_17.py",
          "line_number": 522,
          "function_name": "run_policy_gradient",
          "code": "return image_path, log_path"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 138,
          "function_name": "training_walkthrough",
          "code": "run_policy_gradient(num_epochs=1, num_steps_per_epoch=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 140,
          "function_name": "training_walkthrough",
          "code": "text(\"Let's actually train some models.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's actually train some models.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 538,
          "function_name": "experiments",
          "code": "def experiments():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 539,
          "function_name": "experiments",
          "code": "text(\"Let's start with updating based on raw rewards.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's start with updating based on raw rewards.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 540,
          "function_name": "experiments",
          "code": "image_path, log_path = run_policy_gradient(num_epochs=100, num_steps_per_epoch=10, num_responses=10, deltas_mode=\"rewards\", loss_mode=\"naive\", reward_fn=sort_inclusion_ordering_reward, use_cache=True)  # @stepover"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 541,
          "function_name": "experiments",
          "code": "image(image_path, width=600), link(log_path)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/policy_gradient_rewards_naive.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "var/policy_gradient_rewards_naive.txt",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 542,
          "function_name": "experiments",
          "code": "text(\"Looking through the output, you'll see that by the end, we haven't really learned sorting very well (and this is still the training set).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Looking through the output, you'll see that by the end, we haven't really learned sorting very well (and this is still the training set).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 544,
          "function_name": "experiments",
          "code": "text(\"Let's try using centered rewards.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's try using centered rewards.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 545,
          "function_name": "experiments",
          "code": "image_path, log_path = run_policy_gradient(num_epochs=100, num_steps_per_epoch=10, num_responses=10, deltas_mode=\"centered_rewards\", loss_mode=\"naive\", reward_fn=sort_inclusion_ordering_reward, use_cache=True)  # @stepover"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 546,
          "function_name": "experiments",
          "code": "image(image_path, width=600), link(log_path)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/policy_gradient_centered_rewards_naive.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "var/policy_gradient_centered_rewards_naive.txt",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 547,
          "function_name": "experiments",
          "code": "text(\"This seems to help, as:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This seems to help, as:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 548,
          "function_name": "experiments",
          "code": "text(\"- Suboptimal rewards get a negative gradient update, and\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Suboptimal rewards get a negative gradient update, and",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 549,
          "function_name": "experiments",
          "code": "text(\"- If all the responses for a given prompt have the same reward, then we don't update.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- If all the responses for a given prompt have the same reward, then we don't update.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 550,
          "function_name": "experiments",
          "code": "text(\"Overall, this is better, but we're still getting stuck in local optima.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Overall, this is better, but we're still getting stuck in local optima.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 552,
          "function_name": "experiments",
          "code": "text(\"Finally, let's try normalizing by the standard deviation.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Finally, let's try normalizing by the standard deviation.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 553,
          "function_name": "experiments",
          "code": "image_path, log_path = run_policy_gradient(num_epochs=100, num_steps_per_epoch=10, num_responses=10, deltas_mode=\"normalized_rewards\", loss_mode=\"naive\", reward_fn=sort_inclusion_ordering_reward, use_cache=True)  # @stepover"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 554,
          "function_name": "experiments",
          "code": "image(image_path, width=600), link(log_path)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/policy_gradient_normalized_rewards_naive.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "var/policy_gradient_normalized_rewards_naive.txt",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 555,
          "function_name": "experiments",
          "code": "text(\"There is not much difference here, and indeed, variants like Dr. GRPO do not perform this normalization to avoid length bias (not an issue here since all responses have the same length. \"), link(\"https://arxiv.org/abs/2503.20783\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "There is not much difference here, and indeed, variants like Dr. GRPO do not perform this normalization to avoid length bias (not an issue here since all responses have the same length. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
            "authors": [
              "Zichen Liu",
              "Changyu Chen",
              "Wenjun Li",
              "Penghui Qi",
              "Tianyu Pang",
              "Chao Du",
              "Wee Sun Lee",
              "Min Lin"
            ],
            "organization": null,
            "date": "2025-03-26T17:59:14Z",
            "url": "https://arxiv.org/abs/2503.20783",
            "description": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 557,
          "function_name": "experiments",
          "code": "text(\"Overall, as you can see, reinforcement learning is not trivial, and you can easily get stuck in suboptimal states.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Overall, as you can see, reinforcement learning is not trivial, and you can easily get stuck in suboptimal states.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 558,
          "function_name": "experiments",
          "code": "text(\"The hyperparameters could probably be tuned better...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The hyperparameters could probably be tuned better...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        },
        {
          "path": "lecture_17.py",
          "line_number": 141,
          "function_name": "training_walkthrough",
          "code": "experiments()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 23,
          "function_name": "main",
          "code": "training_walkthrough()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 25,
          "function_name": "main",
          "code": "text(\"Summary\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 26,
          "function_name": "main",
          "code": "text(\"- Reinforcement learning is the key to surpassing human abilities\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Reinforcement learning is the key to surpassing human abilities",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 27,
          "function_name": "main",
          "code": "text(\"- **If** you can measure it, you can optimize it\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **If** you can measure it, you can optimize it",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 28,
          "function_name": "main",
          "code": "text(\"- Policy gradient framework is conceptually clear, just need baselines to reduce variance\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Policy gradient framework is conceptually clear, just need baselines to reduce variance",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 29,
          "function_name": "main",
          "code": "text(\"- RL systems is much more complex than pretraining (inference workloads, manage multiple models)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- RL systems is much more complex than pretraining (inference workloads, manage multiple models)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 31,
          "function_name": "main",
          "code": "text(\"Final two lectures:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Final two lectures:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 32,
          "function_name": "main",
          "code": "text(\"- Junyang Lin (Qwen) \"), link(qwen3)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Junyang Lin (Qwen) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Qwen3 Technical Report",
            "authors": [
              "An Yang",
              "Anfeng Li",
              "Baosong Yang",
              "Beichen Zhang",
              "Binyuan Hui",
              "Bo Zheng",
              "Bowen Yu",
              "Chang Gao",
              "Chengen Huang",
              "Chenxu Lv",
              "Chujie Zheng",
              "Dayiheng Liu",
              "Fan Zhou",
              "Fei Huang",
              "Feng Hu",
              "Hao Ge",
              "Haoran Wei",
              "Huan Lin",
              "Jialong Tang",
              "Jian Yang",
              "Jianhong Tu",
              "Jianwei Zhang",
              "Jianxin Yang",
              "Jiaxi Yang",
              "Jing Zhou",
              "Jingren Zhou",
              "Junyang Lin",
              "Kai Dang",
              "Keqin Bao",
              "Kexin Yang",
              "Le Yu",
              "Lianghao Deng",
              "Mei Li",
              "Mingfeng Xue",
              "Mingze Li",
              "Pei Zhang",
              "Peng Wang",
              "Qin Zhu",
              "Rui Men",
              "Ruize Gao",
              "Shixuan Liu",
              "Shuang Luo",
              "Tianhao Li",
              "Tianyi Tang",
              "Wenbiao Yin",
              "Xingzhang Ren",
              "Xinyu Wang",
              "Xinyu Zhang",
              "Xuancheng Ren",
              "Yang Fan",
              "Yang Su",
              "Yichang Zhang",
              "Yinger Zhang",
              "Yu Wan",
              "Yuqiong Liu",
              "Zekun Wang",
              "Zeyu Cui",
              "Zhenru Zhang",
              "Zhipeng Zhou",
              "Zihan Qiu"
            ],
            "organization": null,
            "date": "2025-05-14T13:41:34Z",
            "url": "https://arxiv.org/abs/2505.09388",
            "description": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_17.py",
          "line_number": 33,
          "function_name": "main",
          "code": "text(\"- Mike Lewis (Llama) \"), link(llama3)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Mike Lewis (Llama) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Llama 3 Herd of Models",
            "authors": [
              "Aaron Grattafiori",
              "Abhimanyu Dubey",
              "Abhinav Jauhri",
              "Abhinav Pandey",
              "Abhishek Kadian",
              "Ahmad Al-Dahle",
              "Aiesha Letman",
              "Akhil Mathur",
              "Alan Schelten",
              "Alex Vaughan",
              "Amy Yang",
              "Angela Fan",
              "Anirudh Goyal",
              "Anthony Hartshorn",
              "Aobo Yang",
              "Archi Mitra",
              "Archie Sravankumar",
              "Artem Korenev",
              "Arthur Hinsvark",
              "Arun Rao",
              "Aston Zhang",
              "Aurelien Rodriguez",
              "Austen Gregerson",
              "Ava Spataru",
              "Baptiste Roziere",
              "Bethany Biron",
              "Binh Tang",
              "Bobbie Chern",
              "Charlotte Caucheteux",
              "Chaya Nayak",
              "Chloe Bi",
              "Chris Marra",
              "Chris McConnell",
              "Christian Keller",
              "Christophe Touret",
              "Chunyang Wu",
              "Corinne Wong",
              "Cristian Canton Ferrer",
              "Cyrus Nikolaidis",
              "Damien Allonsius",
              "Daniel Song",
              "Danielle Pintz",
              "Danny Livshits",
              "Danny Wyatt",
              "David Esiobu",
              "Dhruv Choudhary",
              "Dhruv Mahajan",
              "Diego Garcia-Olano",
              "Diego Perino",
              "Dieuwke Hupkes",
              "Egor Lakomkin",
              "Ehab AlBadawy",
              "Elina Lobanova",
              "Emily Dinan",
              "Eric Michael Smith",
              "Filip Radenovic",
              "Francisco Guzm\u00e1n",
              "Frank Zhang",
              "Gabriel Synnaeve",
              "Gabrielle Lee",
              "Georgia Lewis Anderson",
              "Govind Thattai",
              "Graeme Nail",
              "Gregoire Mialon",
              "Guan Pang",
              "Guillem Cucurell",
              "Hailey Nguyen",
              "Hannah Korevaar",
              "Hu Xu",
              "Hugo Touvron",
              "Iliyan Zarov",
              "Imanol Arrieta Ibarra",
              "Isabel Kloumann",
              "Ishan Misra",
              "Ivan Evtimov",
              "Jack Zhang",
              "Jade Copet",
              "Jaewon Lee",
              "Jan Geffert",
              "Jana Vranes",
              "Jason Park",
              "Jay Mahadeokar",
              "Jeet Shah",
              "Jelmer van der Linde",
              "Jennifer Billock",
              "Jenny Hong",
              "Jenya Lee",
              "Jeremy Fu",
              "Jianfeng Chi",
              "Jianyu Huang",
              "Jiawen Liu",
              "Jie Wang",
              "Jiecao Yu",
              "Joanna Bitton",
              "Joe Spisak",
              "Jongsoo Park",
              "Joseph Rocca",
              "Joshua Johnstun",
              "Joshua Saxe",
              "Junteng Jia",
              "Kalyan Vasuden Alwala",
              "Karthik Prasad",
              "Kartikeya Upasani",
              "Kate Plawiak",
              "Ke Li",
              "Kenneth Heafield",
              "Kevin Stone",
              "Khalid El-Arini",
              "Krithika Iyer",
              "Kshitiz Malik",
              "Kuenley Chiu",
              "Kunal Bhalla",
              "Kushal Lakhotia",
              "Lauren Rantala-Yeary",
              "Laurens van der Maaten",
              "Lawrence Chen",
              "Liang Tan",
              "Liz Jenkins",
              "Louis Martin",
              "Lovish Madaan",
              "Lubo Malo",
              "Lukas Blecher",
              "Lukas Landzaat",
              "Luke de Oliveira",
              "Madeline Muzzi",
              "Mahesh Pasupuleti",
              "Mannat Singh",
              "Manohar Paluri",
              "Marcin Kardas",
              "Maria Tsimpoukelli",
              "Mathew Oldham",
              "Mathieu Rita",
              "Maya Pavlova",
              "Melanie Kambadur",
              "Mike Lewis",
              "Min Si",
              "Mitesh Kumar Singh",
              "Mona Hassan",
              "Naman Goyal",
              "Narjes Torabi",
              "Nikolay Bashlykov",
              "Nikolay Bogoychev",
              "Niladri Chatterji",
              "Ning Zhang",
              "Olivier Duchenne",
              "Onur \u00c7elebi",
              "Patrick Alrassy",
              "Pengchuan Zhang",
              "Pengwei Li",
              "Petar Vasic",
              "Peter Weng",
              "Prajjwal Bhargava",
              "Pratik Dubal",
              "Praveen Krishnan",
              "Punit Singh Koura",
              "Puxin Xu",
              "Qing He",
              "Qingxiao Dong",
              "Ragavan Srinivasan",
              "Raj Ganapathy",
              "Ramon Calderer",
              "Ricardo Silveira Cabral",
              "Robert Stojnic",
              "Roberta Raileanu",
              "Rohan Maheswari",
              "Rohit Girdhar",
              "Rohit Patel",
              "Romain Sauvestre",
              "Ronnie Polidoro",
              "Roshan Sumbaly",
              "Ross Taylor",
              "Ruan Silva",
              "Rui Hou",
              "Rui Wang",
              "Saghar Hosseini",
              "Sahana Chennabasappa",
              "Sanjay Singh",
              "Sean Bell",
              "Seohyun Sonia Kim",
              "Sergey Edunov",
              "Shaoliang Nie",
              "Sharan Narang",
              "Sharath Raparthy",
              "Sheng Shen",
              "Shengye Wan",
              "Shruti Bhosale",
              "Shun Zhang",
              "Simon Vandenhende",
              "Soumya Batra",
              "Spencer Whitman",
              "Sten Sootla",
              "Stephane Collot",
              "Suchin Gururangan",
              "Sydney Borodinsky",
              "Tamar Herman",
              "Tara Fowler",
              "Tarek Sheasha",
              "Thomas Georgiou",
              "Thomas Scialom",
              "Tobias Speckbacher",
              "Todor Mihaylov",
              "Tong Xiao",
              "Ujjwal Karn",
              "Vedanuj Goswami",
              "Vibhor Gupta",
              "Vignesh Ramanathan",
              "Viktor Kerkez",
              "Vincent Gonguet",
              "Virginie Do",
              "Vish Vogeti",
              "V\u00edtor Albiero",
              "Vladan Petrovic",
              "Weiwei Chu",
              "Wenhan Xiong",
              "Wenyin Fu",
              "Whitney Meers",
              "Xavier Martinet",
              "Xiaodong Wang",
              "Xiaofang Wang",
              "Xiaoqing Ellen Tan",
              "Xide Xia",
              "Xinfeng Xie",
              "Xuchao Jia",
              "Xuewei Wang",
              "Yaelle Goldschlag",
              "Yashesh Gaur",
              "Yasmine Babaei",
              "Yi Wen",
              "Yiwen Song",
              "Yuchen Zhang",
              "Yue Li",
              "Yuning Mao",
              "Zacharie Delpierre Coudert",
              "Zheng Yan",
              "Zhengxing Chen",
              "Zoe Papakipos",
              "Aaditya Singh",
              "Aayushi Srivastava",
              "Abha Jain",
              "Adam Kelsey",
              "Adam Shajnfeld",
              "Adithya Gangidi",
              "Adolfo Victoria",
              "Ahuva Goldstand",
              "Ajay Menon",
              "Ajay Sharma",
              "Alex Boesenberg",
              "Alexei Baevski",
              "Allie Feinstein",
              "Amanda Kallet",
              "Amit Sangani",
              "Amos Teo",
              "Anam Yunus",
              "Andrei Lupu",
              "Andres Alvarado",
              "Andrew Caples",
              "Andrew Gu",
              "Andrew Ho",
              "Andrew Poulton",
              "Andrew Ryan",
              "Ankit Ramchandani",
              "Annie Dong",
              "Annie Franco",
              "Anuj Goyal",
              "Aparajita Saraf",
              "Arkabandhu Chowdhury",
              "Ashley Gabriel",
              "Ashwin Bharambe",
              "Assaf Eisenman",
              "Azadeh Yazdan",
              "Beau James",
              "Ben Maurer",
              "Benjamin Leonhardi",
              "Bernie Huang",
              "Beth Loyd",
              "Beto De Paola",
              "Bhargavi Paranjape",
              "Bing Liu",
              "Bo Wu",
              "Boyu Ni",
              "Braden Hancock",
              "Bram Wasti",
              "Brandon Spence",
              "Brani Stojkovic",
              "Brian Gamido",
              "Britt Montalvo",
              "Carl Parker",
              "Carly Burton",
              "Catalina Mejia",
              "Ce Liu",
              "Changhan Wang",
              "Changkyu Kim",
              "Chao Zhou",
              "Chester Hu",
              "Ching-Hsiang Chu",
              "Chris Cai",
              "Chris Tindal",
              "Christoph Feichtenhofer",
              "Cynthia Gao",
              "Damon Civin",
              "Dana Beaty",
              "Daniel Kreymer",
              "Daniel Li",
              "David Adkins",
              "David Xu",
              "Davide Testuggine",
              "Delia David",
              "Devi Parikh",
              "Diana Liskovich",
              "Didem Foss",
              "Dingkang Wang",
              "Duc Le",
              "Dustin Holland",
              "Edward Dowling",
              "Eissa Jamil",
              "Elaine Montgomery",
              "Eleonora Presani",
              "Emily Hahn",
              "Emily Wood",
              "Eric-Tuan Le",
              "Erik Brinkman",
              "Esteban Arcaute",
              "Evan Dunbar",
              "Evan Smothers",
              "Fei Sun",
              "Felix Kreuk",
              "Feng Tian",
              "Filippos Kokkinos",
              "Firat Ozgenel",
              "Francesco Caggioni",
              "Frank Kanayet",
              "Frank Seide",
              "Gabriela Medina Florez",
              "Gabriella Schwarz",
              "Gada Badeer",
              "Georgia Swee",
              "Gil Halpern",
              "Grant Herman",
              "Grigory Sizov",
              "Guangyi",
              "Zhang",
              "Guna Lakshminarayanan",
              "Hakan Inan",
              "Hamid Shojanazeri",
              "Han Zou",
              "Hannah Wang",
              "Hanwen Zha",
              "Haroun Habeeb",
              "Harrison Rudolph",
              "Helen Suk",
              "Henry Aspegren",
              "Hunter Goldman",
              "Hongyuan Zhan",
              "Ibrahim Damlaj",
              "Igor Molybog",
              "Igor Tufanov",
              "Ilias Leontiadis",
              "Irina-Elena Veliche",
              "Itai Gat",
              "Jake Weissman",
              "James Geboski",
              "James Kohli",
              "Janice Lam",
              "Japhet Asher",
              "Jean-Baptiste Gaya",
              "Jeff Marcus",
              "Jeff Tang",
              "Jennifer Chan",
              "Jenny Zhen",
              "Jeremy Reizenstein",
              "Jeremy Teboul",
              "Jessica Zhong",
              "Jian Jin",
              "Jingyi Yang",
              "Joe Cummings",
              "Jon Carvill",
              "Jon Shepard",
              "Jonathan McPhie",
              "Jonathan Torres",
              "Josh Ginsburg",
              "Junjie Wang",
              "Kai Wu",
              "Kam Hou U",
              "Karan Saxena",
              "Kartikay Khandelwal",
              "Katayoun Zand",
              "Kathy Matosich",
              "Kaushik Veeraraghavan",
              "Kelly Michelena",
              "Keqian Li",
              "Kiran Jagadeesh",
              "Kun Huang",
              "Kunal Chawla",
              "Kyle Huang",
              "Lailin Chen",
              "Lakshya Garg",
              "Lavender A",
              "Leandro Silva",
              "Lee Bell",
              "Lei Zhang",
              "Liangpeng Guo",
              "Licheng Yu",
              "Liron Moshkovich",
              "Luca Wehrstedt",
              "Madian Khabsa",
              "Manav Avalani",
              "Manish Bhatt",
              "Martynas Mankus",
              "Matan Hasson",
              "Matthew Lennie",
              "Matthias Reso",
              "Maxim Groshev",
              "Maxim Naumov",
              "Maya Lathi",
              "Meghan Keneally",
              "Miao Liu",
              "Michael L. Seltzer",
              "Michal Valko",
              "Michelle Restrepo",
              "Mihir Patel",
              "Mik Vyatskov",
              "Mikayel Samvelyan",
              "Mike Clark",
              "Mike Macey",
              "Mike Wang",
              "Miquel Jubert Hermoso",
              "Mo Metanat",
              "Mohammad Rastegari",
              "Munish Bansal",
              "Nandhini Santhanam",
              "Natascha Parks",
              "Natasha White",
              "Navyata Bawa",
              "Nayan Singhal",
              "Nick Egebo",
              "Nicolas Usunier",
              "Nikhil Mehta",
              "Nikolay Pavlovich Laptev",
              "Ning Dong",
              "Norman Cheng",
              "Oleg Chernoguz",
              "Olivia Hart",
              "Omkar Salpekar",
              "Ozlem Kalinli",
              "Parkin Kent",
              "Parth Parekh",
              "Paul Saab",
              "Pavan Balaji",
              "Pedro Rittner",
              "Philip Bontrager",
              "Pierre Roux",
              "Piotr Dollar",
              "Polina Zvyagina",
              "Prashant Ratanchandani",
              "Pritish Yuvraj",
              "Qian Liang",
              "Rachad Alao",
              "Rachel Rodriguez",
              "Rafi Ayub",
              "Raghotham Murthy",
              "Raghu Nayani",
              "Rahul Mitra",
              "Rangaprabhu Parthasarathy",
              "Raymond Li",
              "Rebekkah Hogan",
              "Robin Battey",
              "Rocky Wang",
              "Russ Howes",
              "Ruty Rinott",
              "Sachin Mehta",
              "Sachin Siby",
              "Sai Jayesh Bondu",
              "Samyak Datta",
              "Sara Chugh",
              "Sara Hunt",
              "Sargun Dhillon",
              "Sasha Sidorov",
              "Satadru Pan",
              "Saurabh Mahajan",
              "Saurabh Verma",
              "Seiji Yamamoto",
              "Sharadh Ramaswamy",
              "Shaun Lindsay",
              "Shaun Lindsay",
              "Sheng Feng",
              "Shenghao Lin",
              "Shengxin Cindy Zha",
              "Shishir Patil",
              "Shiva Shankar",
              "Shuqiang Zhang",
              "Shuqiang Zhang",
              "Sinong Wang",
              "Sneha Agarwal",
              "Soji Sajuyigbe",
              "Soumith Chintala",
              "Stephanie Max",
              "Stephen Chen",
              "Steve Kehoe",
              "Steve Satterfield",
              "Sudarshan Govindaprasad",
              "Sumit Gupta",
              "Summer Deng",
              "Sungmin Cho",
              "Sunny Virk",
              "Suraj Subramanian",
              "Sy Choudhury",
              "Sydney Goldman",
              "Tal Remez",
              "Tamar Glaser",
              "Tamara Best",
              "Thilo Koehler",
              "Thomas Robinson",
              "Tianhe Li",
              "Tianjun Zhang",
              "Tim Matthews",
              "Timothy Chou",
              "Tzook Shaked",
              "Varun Vontimitta",
              "Victoria Ajayi",
              "Victoria Montanez",
              "Vijai Mohan",
              "Vinay Satish Kumar",
              "Vishal Mangla",
              "Vlad Ionescu",
              "Vlad Poenaru",
              "Vlad Tiberiu Mihailescu",
              "Vladimir Ivanov",
              "Wei Li",
              "Wenchen Wang",
              "Wenwen Jiang",
              "Wes Bouaziz",
              "Will Constable",
              "Xiaocheng Tang",
              "Xiaojian Wu",
              "Xiaolan Wang",
              "Xilun Wu",
              "Xinbo Gao",
              "Yaniv Kleinman",
              "Yanjun Chen",
              "Ye Hu",
              "Ye Jia",
              "Ye Qi",
              "Yenda Li",
              "Yilin Zhang",
              "Ying Zhang",
              "Yossi Adi",
              "Youngjin Nam",
              "Yu",
              "Wang",
              "Yu Zhao",
              "Yuchen Hao",
              "Yundi Qian",
              "Yunlu Li",
              "Yuzi He",
              "Zach Rait",
              "Zachary DeVito",
              "Zef Rosnbrick",
              "Zhaoduo Wen",
              "Zhenyu Yang",
              "Zhiwei Zhao",
              "Zhiyu Ma"
            ],
            "organization": "Meta",
            "date": "2024-07-31T17:54:27Z",
            "url": "https://arxiv.org/abs/2407.21783",
            "description": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
            "notes": "15T tokens\n405B parameters"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    }
  ]
}