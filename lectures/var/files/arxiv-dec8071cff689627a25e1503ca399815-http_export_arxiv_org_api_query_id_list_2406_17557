<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2406.17557%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2406.17557&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/XsupieABGoKi5WUvbCH0ea7fjHg</id>
  <updated>2025-05-15T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2406.17557v2</id>
    <updated>2024-10-31T11:37:49Z</updated>
    <published>2024-06-25T13:50:56Z</published>
    <title>The FineWeb Datasets: Decanting the Web for the Finest Text Data at
  Scale</title>
    <summary>  The performance of a large language model (LLM) depends heavily on the
quality and size of its pretraining dataset. However, the pretraining datasets
for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly
available and very little is known about how they were created. In this work,
we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl
snapshots that produces better-performing LLMs than other open pretraining
datasets. To advance the understanding of how best to curate high-quality
pretraining datasets, we carefully document and ablate all of the design
choices used in FineWeb, including in-depth investigations of deduplication and
filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion
token collection of educational text filtered from FineWeb. LLMs pretrained on
FineWeb-Edu exhibit dramatically better performance on knowledge- and
reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we
publicly release our data curation codebase and all of the models trained
during our ablation experiments.
</summary>
    <author>
      <name>Guilherme Penedo</name>
    </author>
    <author>
      <name>Hynek Kydlíček</name>
    </author>
    <author>
      <name>Loubna Ben allal</name>
    </author>
    <author>
      <name>Anton Lozhkov</name>
    </author>
    <author>
      <name>Margaret Mitchell</name>
    </author>
    <author>
      <name>Colin Raffel</name>
    </author>
    <author>
      <name>Leandro Von Werra</name>
    </author>
    <author>
      <name>Thomas Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2406.17557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.17557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
