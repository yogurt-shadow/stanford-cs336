<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2502.03461%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2502.03461&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/ah5lHkN+FwHK9ddSVzoKI47IeGM</id>
  <updated>2025-05-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.03461v1</id>
    <updated>2025-02-05T18:58:19Z</updated>
    <published>2025-02-05T18:58:19Z</published>
    <title>Do Large Language Model Benchmarks Test Reliability?</title>
    <summary>  When deploying large language models (LLMs), it is important to ensure that
these models are not only capable, but also reliable. Many benchmarks have been
created to track LLMs' growing capabilities, however there has been no similar
focus on measuring their reliability. To understand the potential ramifications
of this gap, we investigate how well current benchmarks quantify model
reliability. We find that pervasive label errors can compromise these
evaluations, obscuring lingering model failures and hiding unreliable behavior.
  Motivated by this gap in the evaluation of reliability, we then propose the
concept of so-called platinum benchmarks, i.e., benchmarks carefully curated to
minimize label errors and ambiguity. As a first attempt at constructing such
benchmarks, we revise examples from fifteen existing popular benchmarks. We
evaluate a wide range of models on these platinum benchmarks and find that,
indeed, frontier LLMs still exhibit failures on simple tasks such as
elementary-level math word problems. Analyzing these failures further reveals
previously unidentified patterns of problems on which frontier models
consistently struggle. We provide code at
https://github.com/MadryLab/platinum-benchmarks
</summary>
    <author>
      <name>Joshua Vendrow</name>
    </author>
    <author>
      <name>Edward Vendrow</name>
    </author>
    <author>
      <name>Sara Beery</name>
    </author>
    <author>
      <name>Aleksander Madry</name>
    </author>
    <link href="http://arxiv.org/abs/2502.03461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.03461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
