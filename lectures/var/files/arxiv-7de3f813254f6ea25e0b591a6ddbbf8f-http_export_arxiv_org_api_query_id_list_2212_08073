<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2212.08073%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2212.08073&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/JzJM5DSXO3FsVccKW1Q6sixNqvs</id>
  <updated>2025-05-13T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2212.08073v1</id>
    <updated>2022-12-15T06:19:23Z</updated>
    <published>2022-12-15T06:19:23Z</published>
    <title>Constitutional AI: Harmlessness from AI Feedback</title>
    <summary>  As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as 'Constitutional AI'. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels.
</summary>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Sandipan Kundu</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Anna Goldie</name>
    </author>
    <author>
      <name>Azalia Mirhoseini</name>
    </author>
    <author>
      <name>Cameron McKinnon</name>
    </author>
    <author>
      <name>Carol Chen</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Christopher Olah</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Dustin Li</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Jamie Kerr</name>
    </author>
    <author>
      <name>Jared Mueller</name>
    </author>
    <author>
      <name>Jeffrey Ladish</name>
    </author>
    <author>
      <name>Joshua Landau</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Kamile Lukosuite</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Michael Sellitto</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Noemi Mercado</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Robert Lasenby</name>
    </author>
    <author>
      <name>Robin Larson</name>
    </author>
    <author>
      <name>Sam Ringer</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Sheer El Showk</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Tamera Lanham</name>
    </author>
    <author>
      <name>Timothy Telleen-Lawton</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <link href="http://arxiv.org/abs/2212.08073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.08073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
