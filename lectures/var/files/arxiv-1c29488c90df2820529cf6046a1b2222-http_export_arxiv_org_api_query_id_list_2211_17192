<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2211.17192%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2211.17192&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/D1aaykgxinqTRbJQlrWaXQEZNb0</id>
  <updated>2025-04-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2211.17192v2</id>
    <updated>2023-05-18T20:28:20Z</updated>
    <published>2022-11-30T17:33:28Z</published>
    <title>Fast Inference from Transformers via Speculative Decoding</title>
    <summary>  Inference from large autoregressive models like Transformers is slow -
decoding K tokens takes K serial runs of the model. In this work we introduce
speculative decoding - an algorithm to sample from autoregressive models faster
without any changes to the outputs, by computing several tokens in parallel. At
the heart of our approach lie the observations that (1) hard language-modeling
tasks often include easier subtasks that can be approximated well by more
efficient models, and (2) using speculative execution and a novel sampling
method, we can make exact decoding from the large models faster, by running
them in parallel on the outputs of the approximation models, potentially
generating several tokens concurrently, and without changing the distribution.
Our method can accelerate existing off-the-shelf models without retraining or
architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration
compared to the standard T5X implementation, with identical outputs.
</summary>
    <author>
      <name>Yaniv Leviathan</name>
    </author>
    <author>
      <name>Matan Kalman</name>
    </author>
    <author>
      <name>Yossi Matias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2023 Oral</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.17192v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.17192v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
