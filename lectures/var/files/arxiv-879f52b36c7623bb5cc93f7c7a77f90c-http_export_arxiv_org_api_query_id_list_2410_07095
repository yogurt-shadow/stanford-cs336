<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2410.07095%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2410.07095&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/s54FKqFVwelnrIYBeDZaLtAcemE</id>
  <updated>2025-05-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2410.07095v6</id>
    <updated>2025-02-26T11:57:30Z</updated>
    <published>2024-10-09T17:34:27Z</published>
    <title>MLE-bench: Evaluating Machine Learning Agents on Machine Learning
  Engineering</title>
    <summary>  We introduce MLE-bench, a benchmark for measuring how well AI agents perform
at machine learning engineering. To this end, we curate 75 ML
engineering-related competitions from Kaggle, creating a diverse set of
challenging tasks that test real-world ML engineering skills such as training
models, preparing datasets, and running experiments. We establish human
baselines for each competition using Kaggle's publicly available leaderboards.
We use open-source agent scaffolds to evaluate several frontier language models
on our benchmark, finding that the best-performing setup--OpenAI's o1-preview
with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in
16.9% of competitions. In addition to our main results, we investigate various
forms of resource scaling for AI agents and the impact of contamination from
pre-training. We open-source our benchmark code (github.com/openai/mle-bench/)
to facilitate future research in understanding the ML engineering capabilities
of AI agents.
</summary>
    <author>
      <name>Jun Shern Chan</name>
    </author>
    <author>
      <name>Neil Chowdhury</name>
    </author>
    <author>
      <name>Oliver Jaffe</name>
    </author>
    <author>
      <name>James Aung</name>
    </author>
    <author>
      <name>Dane Sherburn</name>
    </author>
    <author>
      <name>Evan Mays</name>
    </author>
    <author>
      <name>Giulio Starace</name>
    </author>
    <author>
      <name>Kevin Liu</name>
    </author>
    <author>
      <name>Leon Maksin</name>
    </author>
    <author>
      <name>Tejal Patwardhan</name>
    </author>
    <author>
      <name>Lilian Weng</name>
    </author>
    <author>
      <name>Aleksander MÄ…dry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 17 pages appendix. Equal contribution by first seven
  authors, authors randomized. ICLR version</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.07095v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.07095v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
