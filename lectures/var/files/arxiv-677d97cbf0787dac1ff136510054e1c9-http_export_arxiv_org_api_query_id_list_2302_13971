<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2302.13971%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2302.13971&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api//VuEHzzVKTPMbBrYD8FSsM3j4jc</id>
  <updated>2025-03-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2302.13971v1</id>
    <updated>2023-02-27T17:11:15Z</updated>
    <published>2023-02-27T17:11:15Z</published>
    <title>LLaMA: Open and Efficient Foundation Language Models</title>
    <summary>  We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.
</summary>
    <author>
      <name>Hugo Touvron</name>
    </author>
    <author>
      <name>Thibaut Lavril</name>
    </author>
    <author>
      <name>Gautier Izacard</name>
    </author>
    <author>
      <name>Xavier Martinet</name>
    </author>
    <author>
      <name>Marie-Anne Lachaux</name>
    </author>
    <author>
      <name>Timothée Lacroix</name>
    </author>
    <author>
      <name>Baptiste Rozière</name>
    </author>
    <author>
      <name>Naman Goyal</name>
    </author>
    <author>
      <name>Eric Hambro</name>
    </author>
    <author>
      <name>Faisal Azhar</name>
    </author>
    <author>
      <name>Aurelien Rodriguez</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Guillaume Lample</name>
    </author>
    <link href="http://arxiv.org/abs/2302.13971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
