<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2302.10149%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2302.10149&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/pFQ2ZI2pIT6aY4VGwx1uBf4ZGvE</id>
  <updated>2025-05-13T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2302.10149v2</id>
    <updated>2024-05-06T06:47:30Z</updated>
    <published>2023-02-20T18:30:54Z</published>
    <title>Poisoning Web-Scale Training Datasets is Practical</title>
    <summary>  Deep learning models are often trained on distributed, web-scale datasets
crawled from the internet. In this paper, we introduce two new dataset
poisoning attacks that intentionally introduce malicious examples to a model's
performance. Our attacks are immediately practical and could, today, poison 10
popular datasets. Our first attack, split-view poisoning, exploits the mutable
nature of internet content to ensure a dataset annotator's initial view of the
dataset differs from the view downloaded by subsequent clients. By exploiting
specific invalid trust assumptions, we show how we could have poisoned 0.01% of
the LAION-400M or COYO-700M datasets for just $60 USD. Our second attack,
frontrunning poisoning, targets web-scale datasets that periodically snapshot
crowd-sourced content -- such as Wikipedia -- where an attacker only needs a
time-limited window to inject malicious examples. In light of both attacks, we
notify the maintainers of each affected dataset and recommended several
low-overhead defenses.
</summary>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <author>
      <name>Matthew Jagielski</name>
    </author>
    <author>
      <name>Christopher A. Choquette-Choo</name>
    </author>
    <author>
      <name>Daniel Paleka</name>
    </author>
    <author>
      <name>Will Pearce</name>
    </author>
    <author>
      <name>Hyrum Anderson</name>
    </author>
    <author>
      <name>Andreas Terzis</name>
    </author>
    <author>
      <name>Kurt Thomas</name>
    </author>
    <author>
      <name>Florian Tram√®r</name>
    </author>
    <link href="http://arxiv.org/abs/2302.10149v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.10149v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
