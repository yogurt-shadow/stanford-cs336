<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2001.08361%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2001.08361&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/hlZAyOC9dBQhvv2bnjx6tYkxP4g</id>
  <updated>2025-03-26T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2001.08361v1</id>
    <updated>2020-01-23T03:59:20Z</updated>
    <published>2020-01-23T03:59:20Z</published>
    <title>Scaling Laws for Neural Language Models</title>
    <summary>  We study empirical scaling laws for language model performance on the
cross-entropy loss. The loss scales as a power-law with model size, dataset
size, and the amount of compute used for training, with some trends spanning
more than seven orders of magnitude. Other architectural details such as
network width or depth have minimal effects within a wide range. Simple
equations govern the dependence of overfitting on model/dataset size and the
dependence of training speed on model size. These relationships allow us to
determine the optimal allocation of a fixed compute budget. Larger models are
significantly more sample-efficient, such that optimally compute-efficient
training involves training very large models on a relatively modest amount of
data and stopping significantly before convergence.
</summary>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Benjamin Chess</name>
    </author>
    <author>
      <name>Rewon Child</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Jeffrey Wu</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.08361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
