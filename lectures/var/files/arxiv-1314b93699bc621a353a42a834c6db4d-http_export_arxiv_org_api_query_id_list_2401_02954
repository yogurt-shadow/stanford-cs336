<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2401.02954%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2401.02954&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/Ct1tDYKM9KuB4Y8m3IM56p9X5Tg</id>
  <updated>2025-03-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2401.02954v1</id>
    <updated>2024-01-05T18:59:13Z</updated>
    <published>2024-01-05T18:59:13Z</published>
    <title>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</title>
    <summary>  The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.
</summary>
    <author>
      <name> DeepSeek-AI</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Xiao Bi</name>
    </author>
    <author>
      <name>Deli Chen</name>
    </author>
    <author>
      <name>Guanting Chen</name>
    </author>
    <author>
      <name>Shanhuang Chen</name>
    </author>
    <author>
      <name>Damai Dai</name>
    </author>
    <author>
      <name>Chengqi Deng</name>
    </author>
    <author>
      <name>Honghui Ding</name>
    </author>
    <author>
      <name>Kai Dong</name>
    </author>
    <author>
      <name>Qiushi Du</name>
    </author>
    <author>
      <name>Zhe Fu</name>
    </author>
    <author>
      <name>Huazuo Gao</name>
    </author>
    <author>
      <name>Kaige Gao</name>
    </author>
    <author>
      <name>Wenjun Gao</name>
    </author>
    <author>
      <name>Ruiqi Ge</name>
    </author>
    <author>
      <name>Kang Guan</name>
    </author>
    <author>
      <name>Daya Guo</name>
    </author>
    <author>
      <name>Jianzhong Guo</name>
    </author>
    <author>
      <name>Guangbo Hao</name>
    </author>
    <author>
      <name>Zhewen Hao</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
    <author>
      <name>Wenjie Hu</name>
    </author>
    <author>
      <name>Panpan Huang</name>
    </author>
    <author>
      <name>Erhang Li</name>
    </author>
    <author>
      <name>Guowei Li</name>
    </author>
    <author>
      <name>Jiashi Li</name>
    </author>
    <author>
      <name>Yao Li</name>
    </author>
    <author>
      <name>Y. K. Li</name>
    </author>
    <author>
      <name>Wenfeng Liang</name>
    </author>
    <author>
      <name>Fangyun Lin</name>
    </author>
    <author>
      <name>A. X. Liu</name>
    </author>
    <author>
      <name>Bo Liu</name>
    </author>
    <author>
      <name>Wen Liu</name>
    </author>
    <author>
      <name>Xiaodong Liu</name>
    </author>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Yiyuan Liu</name>
    </author>
    <author>
      <name>Haoyu Lu</name>
    </author>
    <author>
      <name>Shanghao Lu</name>
    </author>
    <author>
      <name>Fuli Luo</name>
    </author>
    <author>
      <name>Shirong Ma</name>
    </author>
    <author>
      <name>Xiaotao Nie</name>
    </author>
    <author>
      <name>Tian Pei</name>
    </author>
    <author>
      <name>Yishi Piao</name>
    </author>
    <author>
      <name>Junjie Qiu</name>
    </author>
    <author>
      <name>Hui Qu</name>
    </author>
    <author>
      <name>Tongzheng Ren</name>
    </author>
    <author>
      <name>Zehui Ren</name>
    </author>
    <author>
      <name>Chong Ruan</name>
    </author>
    <author>
      <name>Zhangli Sha</name>
    </author>
    <author>
      <name>Zhihong Shao</name>
    </author>
    <author>
      <name>Junxiao Song</name>
    </author>
    <author>
      <name>Xuecheng Su</name>
    </author>
    <author>
      <name>Jingxiang Sun</name>
    </author>
    <author>
      <name>Yaofeng Sun</name>
    </author>
    <author>
      <name>Minghui Tang</name>
    </author>
    <author>
      <name>Bingxuan Wang</name>
    </author>
    <author>
      <name>Peiyi Wang</name>
    </author>
    <author>
      <name>Shiyu Wang</name>
    </author>
    <author>
      <name>Yaohui Wang</name>
    </author>
    <author>
      <name>Yongji Wang</name>
    </author>
    <author>
      <name>Tong Wu</name>
    </author>
    <author>
      <name>Y. Wu</name>
    </author>
    <author>
      <name>Xin Xie</name>
    </author>
    <author>
      <name>Zhenda Xie</name>
    </author>
    <author>
      <name>Ziwei Xie</name>
    </author>
    <author>
      <name>Yiliang Xiong</name>
    </author>
    <author>
      <name>Hanwei Xu</name>
    </author>
    <author>
      <name>R. X. Xu</name>
    </author>
    <author>
      <name>Yanhong Xu</name>
    </author>
    <author>
      <name>Dejian Yang</name>
    </author>
    <author>
      <name>Yuxiang You</name>
    </author>
    <author>
      <name>Shuiping Yu</name>
    </author>
    <author>
      <name>Xingkai Yu</name>
    </author>
    <author>
      <name>B. Zhang</name>
    </author>
    <author>
      <name>Haowei Zhang</name>
    </author>
    <author>
      <name>Lecong Zhang</name>
    </author>
    <author>
      <name>Liyue Zhang</name>
    </author>
    <author>
      <name>Mingchuan Zhang</name>
    </author>
    <author>
      <name>Minghua Zhang</name>
    </author>
    <author>
      <name>Wentao Zhang</name>
    </author>
    <author>
      <name>Yichao Zhang</name>
    </author>
    <author>
      <name>Chenggang Zhao</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <author>
      <name>Shangyan Zhou</name>
    </author>
    <author>
      <name>Shunfeng Zhou</name>
    </author>
    <author>
      <name>Qihao Zhu</name>
    </author>
    <author>
      <name>Yuheng Zou</name>
    </author>
    <link href="http://arxiv.org/abs/2401.02954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.02954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
