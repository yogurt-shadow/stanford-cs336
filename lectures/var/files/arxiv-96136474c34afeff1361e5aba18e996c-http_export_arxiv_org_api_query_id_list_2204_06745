<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2204.06745%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2204.06745&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/rihGPXb8HXhhxhaZyWdHoGkriak</id>
  <updated>2025-03-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2204.06745v1</id>
    <updated>2022-04-14T04:00:27Z</updated>
    <published>2022-04-14T04:00:27Z</published>
    <title>GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
    <summary>  We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language
model trained on the Pile, whose weights will be made freely and openly
available to the public through a permissive license. It is, to the best of our
knowledge, the largest dense autoregressive model that has publicly available
weights at the time of submission. In this work, we describe \model{}'s
architecture and training and evaluate its performance on a range of
language-understanding, mathematics, and knowledge-based tasks. We find that
GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in
performance when evaluated five-shot than similarly sized GPT-3 and FairSeq
models. We open-source the training and evaluation code, as well as the model
weights, at https://github.com/EleutherAI/gpt-neox.
</summary>
    <author>
      <name>Sid Black</name>
    </author>
    <author>
      <name>Stella Biderman</name>
    </author>
    <author>
      <name>Eric Hallahan</name>
    </author>
    <author>
      <name>Quentin Anthony</name>
    </author>
    <author>
      <name>Leo Gao</name>
    </author>
    <author>
      <name>Laurence Golding</name>
    </author>
    <author>
      <name>Horace He</name>
    </author>
    <author>
      <name>Connor Leahy</name>
    </author>
    <author>
      <name>Kyle McDonell</name>
    </author>
    <author>
      <name>Jason Phang</name>
    </author>
    <author>
      <name>Michael Pieler</name>
    </author>
    <author>
      <name>USVSN Sai Prashanth</name>
    </author>
    <author>
      <name>Shivanshu Purohit</name>
    </author>
    <author>
      <name>Laria Reynolds</name>
    </author>
    <author>
      <name>Jonathan Tow</name>
    </author>
    <author>
      <name>Ben Wang</name>
    </author>
    <author>
      <name>Samuel Weinbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the ACL Workshop on Challenges &amp;
  Perspectives in Creating Large Language Models</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
